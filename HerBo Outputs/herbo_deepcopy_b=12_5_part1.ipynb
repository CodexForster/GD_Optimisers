{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HerBo_deepcopy_ver_B12.5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbaF0qYB9jl2"
      },
      "source": [
        "# HerBo + Minibatch Gradient Descent (Size of 128)\n",
        "# 5 fold Cross-validation\n",
        "## Testing, training dataset size: 2560, 320 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHuJH1MG-sFC"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "class convnet_herbo_mbgd():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.inputImg = None  # The input data\n",
        "        self.strides = []  # The stride length of each layer for convolution\n",
        "        self.recfield = []  # The receptive field in each layer for convolution\n",
        "        self.lengths = []\n",
        "        self.widths = []\n",
        "        self.depths = []\n",
        "        self.weights = []  # The weights for convolution filters\n",
        "        #self.weights2 = [] # The array to keep track of previous weights so we can implement HerBo\n",
        "        self.node = 10  # The number of nodes in the output layer\n",
        "        self.track = []  # Keeps track of layer order, i.e Conv./Pooling/FC\n",
        "        self.learning_rate = 0.005\n",
        "        self.fc_weights = []\n",
        "        #self.fc_weights2 = [] # The array to keep track of previous fc_weights so we can implement HerBo\n",
        "        self.output_fc = []\n",
        "        self.n = 3 # The n'th moment calculations\n",
        "        self.beta = 12.5 # The hyperparameter involved in addition of higher moment terms\n",
        "        self.count = 1 # Keeps track of iteration number needed for optimisation\n",
        "\n",
        "    def addInput(self, inpImage):  # Assign the input image\n",
        "        inpImage = np.array(inpImage)\n",
        "        self.inputImg = inpImage\n",
        "        if(len(inpImage.shape) < 3):\n",
        "            num3 = 1\n",
        "            numrows = inpImage.shape[0]\n",
        "            numcols = inpImage.shape[1]\n",
        "        else:\n",
        "            num3 = inpImage.shape[0]\n",
        "            numrows = inpImage.shape[1]\n",
        "            numcols = inpImage.shape[2]\n",
        "\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "\n",
        "    def cvolume(self, s, r, f):\n",
        "        \"\"\"\n",
        "        Creates a new Conv. volume.\n",
        "        s - stride length for convolving the previous layer to create this new volume\n",
        "        r - receptive field for convolving the previous layer to create this new volume\n",
        "        f - number of filters, or in other words, the depth of this new volume\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Initializing the weights\n",
        "        W = []\n",
        "\n",
        "        #b = np.zeros((1, stre))\n",
        "        for i in range(f):\n",
        "            W.append(np.random.randn(prevd, r, r) / (r*r))\n",
        "\n",
        "        W = np.array(W)\n",
        "\n",
        "        # The dimensions of the layer after convolution with the above weight array\n",
        "        numrows = (prevw - r)/s + 1\n",
        "        numcols = (prevl - r)/s + 1\n",
        "        num3 = f\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(W)\n",
        "        self.strides.append(s)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "        self.track.append('c')\n",
        "\n",
        "\n",
        "    def pmaxvolume(self, r):\n",
        "        \"\"\"\n",
        "        Creates a new max pooling layer.\n",
        "        r - the receptive field around which the max value has to be taken.\n",
        "            E.g - If r = 2, max pooling is done withing 2x2 sub matrices.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(None)\n",
        "        self.strides.append(r)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(prevl/r)\n",
        "        self.widths.append(prevw/r)\n",
        "        self.depths.append(prevd)\n",
        "        self.track.append('p')\n",
        "\n",
        "\n",
        "    def FCLayer(self, n_nodes):\n",
        "        \"\"\"\n",
        "        Creates a fully connected layer\n",
        "        n - the no.of nodes in the output layer.\n",
        "        input_fc - the input to the fully connected layer.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = int(self.depths[-1])\n",
        "        prevw = int(self.widths[-1])\n",
        "        prevl = int(self.lengths[-1])\n",
        "\n",
        "        # flatten the input\n",
        "        input_fc = np.zeros((prevd, prevw, prevl))\n",
        "        input_fc = input_fc.flatten()\n",
        "        len_input_fc = len(input_fc)\n",
        "\n",
        "        # Initialise the weights and biases for the FC layer\n",
        "        self.fc_weights = np.random.randn(len_input_fc, n_nodes) / (len_input_fc)\n",
        "        #self.fc_bias = np.zeros(n_nodes)\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(self.fc_weights)\n",
        "        self.strides.append(0)\n",
        "        self.recfield.append(0)\n",
        "        self.lengths.append(1)\n",
        "        self.widths.append(len_input_fc)\n",
        "        self.depths.append(1)\n",
        "        self.track.append('f')\n",
        "        self.node = n_nodes\n",
        "\n",
        "\n",
        "    def activFunc(self, inputArray):\n",
        "        \"\"\"\n",
        "        The activation function for the neurons in the network.\n",
        "        \"\"\"\n",
        "        # ReLU activation\n",
        "        return np.maximum(0, inputArray)\n",
        "\n",
        "    def dataLoss(self, predResults, trueResults):\n",
        "        \"\"\"\n",
        "        Returns the data loss. Cross-Entropy loss function (Softmax Classifier).\n",
        "        \"\"\"\n",
        "        # Softmax\n",
        "        loss = 0\n",
        "        sum = 0\n",
        "        for i in range(len(predResults)):\n",
        "            sum += math.exp(predResults[i])\n",
        "        correct = np.argmax(trueResults)\n",
        "        loss = (-1)*(math.log((math.exp(predResults[correct]))/sum))\n",
        "        return loss\n",
        "\n",
        "    def ConvOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Convolutional Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        if(len(W.shape) < 4):\n",
        "            f = 1\n",
        "        else:\n",
        "            f = W.shape[0]\n",
        "\n",
        "        for i in range(f):  # Run loop to create f-filters\n",
        "            for k in range(w):  # Convolve around width\n",
        "                for m in range(l):  # Convolve around length\n",
        "                    # for j in range(d):   #Run over entire depth of prevOut volume\n",
        "                    volOutput[i][k][m] += np.sum(np.multiply(W[i][:][:][:], prevOut[:, k*s: k*s + r, m*s: m*s + r])[:, :, :])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def PoolOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Pooling Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer, since there is no Weight matrix for MaxPooling, it is None\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    volOutput[j][k][m] = np.amax(prevOut[j, k*r: (k + 1)*r, m*r: (m + 1)*r])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def FCOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Implements forward pass for the FC layer. Uses a softmax Classifier.\n",
        "        n_nodes - the no.of nodes in the fully connected layer. \n",
        "        \"\"\"\n",
        "        # flatten the input\n",
        "        prevOut = prevOut.flatten()\n",
        "        #len_input_fc = len(prevOut)\n",
        "\n",
        "        totals = np.dot(prevOut, W)  # + self.fc_bias\n",
        "        # Softmax\n",
        "        exp_totals = np.exp(totals)\n",
        "\n",
        "        # Output from the FC layer\n",
        "        self.output_fc = exp_totals / (np.sum(exp_totals, axis=0))\n",
        "        return self.output_fc\n",
        "\n",
        "    def getVolumeOutput(self, n):\n",
        "        \"\"\"\n",
        "        Returns the output of the nth volume of the ConvNet.\n",
        "        \"\"\"\n",
        "        penLayer = len(self.weights) - 1  # The penultimate volume\n",
        "\n",
        "        # h stores the output of the current layer\n",
        "        h = np.array(self.inputImg)\n",
        "\n",
        "        # Loop through the hidden layers\n",
        "        for i in range(min(n, penLayer)):\n",
        "            W = self.weights[i]\n",
        "            s = self.strides[i]\n",
        "            r = self.recfield[i]\n",
        "            d = self.depths[i+1]\n",
        "            w = self.widths[i+1]\n",
        "            l = self.lengths[i+1]\n",
        "            if (self.track[i] == 'c'):\n",
        "                h = self.activFunc(self.ConvOutput(h, W, s, r, d, w, l))\n",
        "            elif (self.track[i] == 'p'):\n",
        "                h = self.PoolOutput(h, W, s, r, d, w, l)\n",
        "            else:\n",
        "                h = self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "        # Return the output\n",
        "        if n <= penLayer:\n",
        "            return h\n",
        "        else:\n",
        "            W = self.weights[n-1]\n",
        "            s = self.strides[n-1]\n",
        "            r = self.recfield[n-1]\n",
        "            d = self.depths[n]\n",
        "            w = self.widths[n]\n",
        "            l = self.lengths[n]\n",
        "            return self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "    def FCGD(self, index, trueResults):  # FC layer Gradient Descent\n",
        "        input_fc = self.getVolumeOutput(index - 1)\n",
        "\n",
        "        # Store the shape of input before flattening (to be used for backpropagation)\n",
        "        input_fc_shape = input_fc.shape\n",
        "\n",
        "        # Flatten the input\n",
        "        input_fc = input_fc.flatten()\n",
        "\n",
        "        # Get the weights and the totals of the fixed layer\n",
        "        W = self.weights[index - 1]\n",
        "        totals = np.dot(input_fc, W)\n",
        "\n",
        "        # Calculating d_L_d_out\n",
        "        correct = np.argmax(trueResults)\n",
        "        d_L_d_out = np.zeros_like(self.output_fc)\n",
        "        d_L_d_out[correct] = -1 / (self.output_fc[correct])\n",
        "\n",
        "        # Calculating d_out_d_t\n",
        "        exp_totals = np.exp(totals)\n",
        "        sum_exp_totals = np.sum(exp_totals)\n",
        "\n",
        "        d_out_d_t = np.zeros_like(self.output_fc)\n",
        "        d_out_d_t = -exp_totals[correct] * (exp_totals / (sum_exp_totals ** 2))\n",
        "        d_out_d_t[correct] = exp_totals[correct] * ((sum_exp_totals - exp_totals[correct])/(sum_exp_totals ** 2))\n",
        "\n",
        "        # Other necessary gradients\n",
        "        d_t_d_w = input_fc\n",
        "        d_t_d_inputs = W\n",
        "        d_L_d_t = d_L_d_out * d_out_d_t\n",
        "\n",
        "        # Gradients of loss wrt Weights of FC layer and Input of FC layers\n",
        "        # d_L_d_t.shape = (n_nodes,1)\n",
        "        # Adding appropriate axes to d_L_d_t and d_t_d_w(same as input_fc) for . product\n",
        "        d_L_d_w = np.dot(d_t_d_w[np.newaxis].T, d_L_d_t[np.newaxis])\n",
        "\n",
        "        # d_L_d_inputs should have the dimensions of input_fc\n",
        "        d_L_d_inputs = np.dot(d_t_d_inputs, d_L_d_t)\n",
        "\n",
        "        # The dimension of d_L_d_inputs is (len_input_fc,), so, changing the shape so it can be given to maxpool's backprop.\n",
        "        d_L_d_inputs_final = d_L_d_inputs.reshape(input_fc_shape)\n",
        "\n",
        "        #W -= self.learning_rate * d_L_d_w\n",
        "        #self.weights[index - 1] = W\n",
        "        dW = -self.learning_rate * d_L_d_w\n",
        "        return (d_L_d_inputs_final, dW)\n",
        "\n",
        "    def PoolGD(self, dLdOut, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates gradients through the MaxPooling layer\n",
        "        dLdOut is the differential of Loss wrt the Output where Output here refers to the output of the MaxPooling layer\n",
        "        This function thus finds dLdI which is the differential of Loss wrt the Input where Input here refers to input to MaxPool layer.\n",
        "        \"\"\"\n",
        "        input_vol = self.getVolumeOutput(index - 1)\n",
        "        s = self.strides[index - 1]\n",
        "        r = self.recfield[index - 1]\n",
        "        d = dLdOut.shape[0]\n",
        "        w = dLdOut.shape[1]\n",
        "        l = dLdOut.shape[2]\n",
        "\n",
        "        # Convert the numbers to int, as the for loops below will report errors if this is not done\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "\n",
        "        # Keep track of the depth and spatial indices of where the maximum element is, in the sub arrays taken for pooling\n",
        "        d_ind = []\n",
        "        spatial_ind = []\n",
        "        # Keep track of which sub array is being taken for max pooling\n",
        "        track_w = []\n",
        "        track_l = []\n",
        "\n",
        "        dLdI = np.zeros((int(self.depths[index - 1]), int(self.lengths[index - 1]), int(self.widths[index - 1])))\n",
        "        replace = dLdOut.flatten()\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    spatial_ind.append(np.where(input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r] == input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r].max()))\n",
        "                    track_l.append(m)\n",
        "                    track_w.append(k)\n",
        "                    d_ind.append(j)\n",
        "\n",
        "        # Initialise correct values in dLdI array\n",
        "        for i in range(len(replace)):\n",
        "            width = spatial_ind[i][0][0]  # Note the (width) spatial index of the maximum element of the sub array\n",
        "            width += track_w[i]*r  # Add the (width) location depending on which sub array was taken for max pooling\n",
        "            length = spatial_ind[i][1][0]  # Note the (length) spatial index of the maximum element of the sub array\n",
        "            length += track_l[i]*r  # Add the (length) location depending on which sub array was taken for max pooling\n",
        "            depth = d_ind[i]  # Note the depth index of the maximum element of the sub array\n",
        "            dLdI[depth][width][length] = replace[i]\n",
        "\n",
        "        return dLdI\n",
        "\n",
        "    # Helper functions for convBackProp()\n",
        "    def convolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the convoluted output convFilter on inputLayer.\n",
        "        Both are two dimensional matrices square matrices.\n",
        "        inputLayer - (n, n)\n",
        "        convFilter - (f, f)\n",
        "        \"\"\"\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Defining the shape of the output matrix\n",
        "        l = (n-f) + 1\n",
        "        output_matrix = np.zeros((l, l))\n",
        "        s = 1\n",
        "\n",
        "        # Convolving\n",
        "        for row in range(l):\n",
        "            for col in range(l):\n",
        "                output_matrix[row][col] = np.sum(np.multiply(inputLayer[row:row+f, col:col+f], convFilter))\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def fullConvolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the full convoluted output of convFilter on inputLayer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Creating padding for the inputLayer matrix\n",
        "        padding = f - 1\n",
        "        new_dim = n + 2*padding\n",
        "\n",
        "        padded_input = np.zeros([new_dim, new_dim])\n",
        "        padded_input[padding:new_dim - padding,padding:new_dim - padding] = inputLayer\n",
        "\n",
        "        # Now convolve padded_input with convFilter\n",
        "        output_matrix = self.convolve(padded_input, convFilter)\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def rotate180(self, matrix):\n",
        "        \"\"\"\n",
        "        Rotates matrix by 180 degrees in the plane.\n",
        "        Takes only two dimensional matrices.\n",
        "        \"\"\"\n",
        "        return np.rot90(matrix, 2)\n",
        "\n",
        "    def ConvGD(self, dLdoutput, index, t):\n",
        "        \"\"\"\n",
        "        Function that backpropagates through a convolutional layer.\n",
        "        index = index of the current layer\n",
        "        dLdoutput = Gradient of the loss function wrt the output of the current layer (channel, row, col)\n",
        "        Returns dLdinput.\n",
        "        t = iterative index\n",
        "        \"\"\"\n",
        "        X = self.getVolumeOutput(index-1)  # Input to the current layer (channel, row, col)\n",
        "        # Weights of the current layer (numFilter, channel, row, col)\n",
        "        W = self.weights[index - 1]\n",
        "\n",
        "        dLdX = np.empty(X.shape)\n",
        "        dLdW = np.empty(W.shape)\n",
        "\n",
        "        dLdout = np.copy(dLdoutput)\n",
        "        dLdout[dLdout < 0] = 0\n",
        "\n",
        "        # Loop over the filters\n",
        "        numFilters = W.shape[0]\n",
        "\n",
        "        for fil_ter in range(numFilters):\n",
        "            filter_output = dLdout[fil_ter]\n",
        "\n",
        "            # Loop over the channels\n",
        "            for channel in range(W.shape[1]):\n",
        "                filter_layer = W[fil_ter][channel]\n",
        "                dWLayer = self.convolve(X[channel], filter_output)\n",
        "                dXLayer = self.rotate180(self.fullConvolve(self.rotate180(filter_layer), filter_output))\n",
        "\n",
        "                # Combine these and return in arrays\n",
        "                dLdW[fil_ter][channel] = dWLayer\n",
        "                dLdX[channel] = dXLayer\n",
        "\n",
        "        return (dLdX,-self.learning_rate*dLdW)\n",
        "\n",
        "    def backPropagation(self, input, trueResults, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Updates weights by carrying out backpropagation using mini-batches.\n",
        "        trueResults = the expected output from the neural network.\n",
        "        \"\"\"\n",
        "        # This keeps track of all weight parameter changes, and make average update at end of mini-batch\n",
        "        conv_weights = [] \n",
        "        for i in range(len(self.weights) - 1):\n",
        "            if(self.track[i]=='p'):\n",
        "                conv_weights.append('p')\n",
        "            elif(self.track[i]=='c'):\n",
        "                conv_weights.append(np.zeros(self.weights[i].shape))\n",
        "        fc_weights = np.zeros(self.weights[len(self.weights) - 1].shape)\n",
        "        size = int(len(input)/mini_batch_size)\n",
        "        weights2 = copy.deepcopy(self.weights)\n",
        "        temp = copy.deepcopy(self.weights)\n",
        "        fc_weights2 = copy.deepcopy(self.weights[len(self.weights) - 1])\n",
        "        temp2 = copy.deepcopy(self.weights[len(self.weights) - 1])\n",
        "        for batch in range(size):\n",
        "            mini_inp = np.array(input[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            mini_res = np.array(trueResults[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            weights2 = copy.deepcopy(temp)\n",
        "            fc_weights2 = copy.deepcopy(temp2)\n",
        "            if (batch > 0):\n",
        "                temp = copy.deepcopy(self.weights)\n",
        "                temp2 = copy.deepcopy(self.weights[len(self.weights) - 1])\n",
        "            for i in range(len(mini_inp)):\n",
        "                self.inputImg = np.array(mini_inp[i])\n",
        "                if(len(self.inputImg.shape) < 3):\n",
        "                    a = self.inputImg\n",
        "                    self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                # Index keeping track of the previous layer\n",
        "                nPrev = len(self.weights)\n",
        "                (doutput,dLdW) = self.FCGD(nPrev, mini_res[i]) # The dLdW has the negative sign and is multiplied by the learning rate before getting passed here\n",
        "                fc_weights += dLdW/len(mini_inp)\n",
        "                nPrev -= 1\n",
        "                # Loop over the layers\n",
        "                while nPrev - 1 >= 0:\n",
        "                    if(self.track[nPrev - 1] == 'p'):\n",
        "                        dhidden = self.PoolGD(doutput, nPrev)\n",
        "                    else:\n",
        "                        (dhidden, dLdW) = self.ConvGD(doutput, nPrev, self.count)\n",
        "                        conv_weights[nPrev - 1] += dLdW/len(mini_inp)\n",
        "                        #self.weights[nPrev - 1] -= dLdW / len(mini_inp)\n",
        "                    doutput = dhidden  # Move to the previous layer\n",
        "                    nPrev -= 1\n",
        "            #print('w = ',self.weights[0],',\\t w2 = ',weights2[0])\n",
        "            for i in range(len(self.weights) - 1):\n",
        "                if (self.track[i]=='c'):\n",
        "                    self.weights[i] = self.weights[i] + conv_weights[i] + self.beta*((self.weights[i])**self.n - (weights2[i])**self.n)\n",
        "                    conv_weights[i] = np.zeros(conv_weights[i].shape)\n",
        "            self.weights[len(self.weights) - 1] = self.weights[len(self.weights) - 1] + fc_weights + self.beta*((self.weights[len(self.weights) - 1])**self.n - (fc_weights2)**self.n)\n",
        "            fc_weights = np.zeros(fc_weights.shape)\n",
        "            #print('Training over mini-batch number: ',self.count, 'done.')\n",
        "            self.count = self.count + 1\n",
        "\n",
        "    def evaluate(self, x_train, y_train, x_test, y_test, epochs, mini_batch_size, test_freq):\n",
        "        \"\"\"\n",
        "        Train the neural network. And run test using test data to see progress of the model accuracy.\n",
        "        x_train, x_test = the inout training and testing data respectively.\n",
        "        y_train, y_test = the expected results from the CNN for the training and testing data respectively.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size = the size of mini-batches.\n",
        "        test_freq = Testing results should be printed after 'test_freq' number of epochs.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(x_train, y_train, mini_batch_size)\n",
        "            print('\\t Epoch Number: ', i + 1, ' done.')\n",
        "            if(i%test_freq==0):\n",
        "                print('\\t Testing score after ',i + 1,' epochs:')\n",
        "                self.accuracy(x_test, y_test)\n",
        "        print(\"\\t Training Complete.\")\n",
        "\n",
        "\n",
        "    def train(self, input, Y, epochs, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "        Y = the expected results from the neural network.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size is the size of mini-batches.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(input, Y, mini_batch_size)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "    def accuracy(self, X, Y):\n",
        "        \"\"\"\n",
        "        Function that takes in test data and results and calculates the accuracy of the Network.\n",
        "        \"\"\"\n",
        "        y = []\n",
        "        cor = 0\n",
        "        correct = 0\n",
        "        for i in range(len(X)):\n",
        "            self.inputImg = np.array(X[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                y.append(self.getVolumeOutput(len(self.weights)))\n",
        "        Y = np.array(Y)\n",
        "        y = np.array(y)\n",
        "        if (np.max(y) == 0):\n",
        "            y /= 1.0\n",
        "        else:\n",
        "            y /= np.max(y)\n",
        "        for i in range(len(Y)):\n",
        "            correct = np.argmax(Y[i])\n",
        "            if (np.argmax(y[i]) == correct):\n",
        "                cor += 1\n",
        "        cor /= len(Y)\n",
        "        print('\\t \\t Accuracy = ', cor*100, '%')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnDdydRz9L8U"
      },
      "source": [
        "# Code Run for HerBO (MBGD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXxxgdqhqng8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484b8fa0-0dfa-4ce7-c723-b9b72e0ada6e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd    \n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "(x_original, y_original), (x2_original, y2_original) = datasets.mnist.load_data()\n",
        "\n",
        "# See distribution of data for each class in the original testing data of MNIST database\n",
        "count = np.zeros(10)\n",
        "for i in range(len(x_original)):\n",
        "    count[y_original[i]] += 1\n",
        "count /= len(x_original)\n",
        "\n",
        "# How many examples of each do we need to maintain the same distribution of original MNIST database.\n",
        "count *= 3200 # We want 2560 training images and 640 testing images\n",
        "print('Reshaped class-wise number of examples to maintain same distribution original MNIST data: ',count)\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "df=pd.DataFrame(y_original, columns=['y_original']) \n",
        "index = [i for i in range(0, len(df.index))]\n",
        "df['index'] = index\n",
        "df.columns=['y_original', 'Index']\n",
        "df.set_index('Index',inplace=True)\n",
        "\n",
        "# Append first n examples of each class\n",
        "# NOTE: Yes it is an easier way if we could use a double for-loop, but we purposely write it in 10 explicit for-loops so that it is easier for the reader to understand.\n",
        "for i in range(316):\n",
        "    index = (df.index[df['y_original'] == 0]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(0)\n",
        "\n",
        "for i in range(360):\n",
        "    index = (df.index[df['y_original'] == 1]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(1)\n",
        "\n",
        "for i in range(318):\n",
        "    index = (df.index[df['y_original'] == 2]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(2)\n",
        "\n",
        "for i in range(327):\n",
        "    index = (df.index[df['y_original'] == 3]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(3)\n",
        "\n",
        "for i in range(311):\n",
        "    index = (df.index[df['y_original'] == 4]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(4)\n",
        "\n",
        "for i in range(289):\n",
        "    index = (df.index[df['y_original'] == 5]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(5)\n",
        "\n",
        "for i in range(316):\n",
        "    index = (df.index[df['y_original'] == 6]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(6)\n",
        "\n",
        "for i in range(334):\n",
        "    index = (df.index[df['y_original'] == 7]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(7)\n",
        "\n",
        "for i in range(312):\n",
        "    index = (df.index[df['y_original'] == 8]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(8)\n",
        "\n",
        "for i in range(317):\n",
        "    index = (df.index[df['y_original'] == 9]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(9)\n",
        "\n",
        "cross_x = x\n",
        "cross_y = y\n",
        "\n",
        "# Shuffle data since we are going to do cross validation (as it was grouped by each class)\n",
        "from sklearn.utils import shuffle\n",
        "cross_x, cross_y = shuffle(cross_x, cross_y, random_state=0)\n",
        "\n",
        "cross_x= np.array(cross_x)\n",
        "cross_y= np.array(cross_y)\n",
        "\n",
        "# Normalise data\n",
        "cross_x = ((cross_x/255) - 0.5)\n",
        "\n",
        "print('Training and testing data pre-processed.')\n",
        "print('Cross-validation dataset size: ',len(cross_x))\n",
        "\n",
        "#Since we test the CNN with MNIST data, we write the target output in the required format before sent to training/testing.\n",
        "results = np.zeros((len(cross_y),10))\n",
        "\n",
        "for i in range(len(cross_y)):\n",
        "    results[i,cross_y[i]] = 1\n",
        "\n",
        "size = 640\n",
        "# 5-fold cross-validation\n",
        "for i in range(5):\n",
        "    # Ensuring reproducibility\n",
        "    np.random.seed(0)\n",
        "    # Create instance of NeuralNetwork\n",
        "    model = convnet_herbo_mbgd()\n",
        "    #model = convnet_adam_mbgd()\n",
        "    #model = convnet_sgd()\n",
        "    #model = convnet_mbgd()\n",
        "\n",
        "    X = np.random.randn(1,28,28)\n",
        "    model.addInput(X) # Input layer\n",
        "    model.cvolume(1,5,10) # Add Convolutional volume (stride length, receptive field, filters)\n",
        "    model.pmaxvolume(2) # Add Pooling layer (receptive field)\n",
        "    model.FCLayer(10)  # Add FC Layer (number of classifiers)\n",
        "    # Get final output layer. \n",
        "    # ========================================================================================\n",
        "    # IMPORTANT: One HAS to run it once before training, so that all variables are initialised.\n",
        "    # ========================================================================================\n",
        "    print('Test Run Output: ',model.getVolumeOutput(3))\n",
        "\n",
        "    arr1x = cross_x[0: i*size]\n",
        "    test_x = cross_x[i*size: (i+1)*size]\n",
        "    arr2x = cross_x[(i+1)*size:]\n",
        "    arr1y = results[0: i*size]\n",
        "    test_y = results[i*size: (i+1)*size]\n",
        "    arr2y = results[(i+1)*size:]\n",
        "    train_x = np.concatenate((arr1x, arr2x))\n",
        "    train_y = np.concatenate((arr1y, arr2y))\n",
        "    print('Training and testing data generated, and pre-processed.')\n",
        "    # Train model using cross-validation with 2560 train images for 10 epochs with a mini-batch size; test size is 260 images\n",
        "    print('Cross-validation fold number: ',i+1)\n",
        "    model.evaluate(train_x, train_y, test_x, test_y, 10, 128, 1)\n",
        "\n",
        "#model.accuracy(x[2000:4000], results[2000:4000])  # Predict accuracy using test data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaped class-wise number of examples to maintain same distribution original MNIST data:  [315.89333333 359.57333333 317.76       326.98666667 311.57333333\n",
            " 289.12       315.62666667 334.13333333 312.05333333 317.28      ]\n",
            "Training and testing data pre-processed.\n",
            "Cross-validation dataset size:  3200\n",
            "Test Run Output:  [0.09997204 0.09983971 0.10041394 0.10040842 0.10016026 0.09975728\n",
            " 0.10072183 0.10047065 0.09918427 0.09907158]\n",
            "Training and testing data generated, and pre-processed.\n",
            "Cross-validation fold number:  1\n",
            "\t Epoch Number:  1  done.\n",
            "\t Testing score after  1  epochs:\n",
            "\t \t Accuracy =  61.40624999999999 %\n",
            "\t Epoch Number:  2  done.\n",
            "\t Testing score after  2  epochs:\n",
            "\t \t Accuracy =  62.96874999999999 %\n",
            "\t Epoch Number:  3  done.\n",
            "\t Testing score after  3  epochs:\n",
            "\t \t Accuracy =  63.28125 %\n",
            "\t Epoch Number:  4  done.\n",
            "\t Testing score after  4  epochs:\n",
            "\t \t Accuracy =  62.96874999999999 %\n",
            "\t Epoch Number:  5  done.\n",
            "\t Testing score after  5  epochs:\n",
            "\t \t Accuracy =  62.5 %\n",
            "\t Epoch Number:  6  done.\n",
            "\t Testing score after  6  epochs:\n",
            "\t \t Accuracy =  61.09375 %\n",
            "\t Epoch Number:  7  done.\n",
            "\t Testing score after  7  epochs:\n",
            "\t \t Accuracy =  60.46875000000001 %\n",
            "\t Epoch Number:  8  done.\n",
            "\t Testing score after  8  epochs:\n",
            "\t \t Accuracy =  59.06249999999999 %\n",
            "\t Epoch Number:  9  done.\n",
            "\t Testing score after  9  epochs:\n",
            "\t \t Accuracy =  58.28124999999999 %\n",
            "\t Epoch Number:  10  done.\n",
            "\t Testing score after  10  epochs:\n",
            "\t \t Accuracy =  56.71874999999999 %\n",
            "\t Training Complete.\n",
            "Test Run Output:  [0.09997204 0.09983971 0.10041394 0.10040842 0.10016026 0.09975728\n",
            " 0.10072183 0.10047065 0.09918427 0.09907158]\n",
            "Training and testing data generated, and pre-processed.\n",
            "Cross-validation fold number:  2\n",
            "\t Epoch Number:  1  done.\n",
            "\t Testing score after  1  epochs:\n",
            "\t \t Accuracy =  50.78125 %\n",
            "\t Epoch Number:  2  done.\n",
            "\t Testing score after  2  epochs:\n",
            "\t \t Accuracy =  56.25 %\n",
            "\t Epoch Number:  3  done.\n",
            "\t Testing score after  3  epochs:\n",
            "\t \t Accuracy =  58.75 %\n",
            "\t Epoch Number:  4  done.\n",
            "\t Testing score after  4  epochs:\n",
            "\t \t Accuracy =  59.68750000000001 %\n",
            "\t Epoch Number:  5  done.\n",
            "\t Testing score after  5  epochs:\n",
            "\t \t Accuracy =  59.375 %\n",
            "\t Epoch Number:  6  done.\n",
            "\t Testing score after  6  epochs:\n",
            "\t \t Accuracy =  58.90625000000001 %\n",
            "\t Epoch Number:  7  done.\n",
            "\t Testing score after  7  epochs:\n",
            "\t \t Accuracy =  58.4375 %\n",
            "\t Epoch Number:  8  done.\n",
            "\t Testing score after  8  epochs:\n",
            "\t \t Accuracy =  58.75 %\n",
            "\t Epoch Number:  9  done.\n",
            "\t Testing score after  9  epochs:\n",
            "\t \t Accuracy =  58.28124999999999 %\n",
            "\t Epoch Number:  10  done.\n",
            "\t Testing score after  10  epochs:\n",
            "\t \t Accuracy =  57.96875 %\n",
            "\t Training Complete.\n",
            "Test Run Output:  [0.09997204 0.09983971 0.10041394 0.10040842 0.10016026 0.09975728\n",
            " 0.10072183 0.10047065 0.09918427 0.09907158]\n",
            "Training and testing data generated, and pre-processed.\n",
            "Cross-validation fold number:  3\n",
            "\t Epoch Number:  1  done.\n",
            "\t Testing score after  1  epochs:\n",
            "\t \t Accuracy =  53.125 %\n",
            "\t Epoch Number:  2  done.\n",
            "\t Testing score after  2  epochs:\n",
            "\t \t Accuracy =  58.12500000000001 %\n",
            "\t Epoch Number:  3  done.\n",
            "\t Testing score after  3  epochs:\n",
            "\t \t Accuracy =  59.68750000000001 %\n",
            "\t Epoch Number:  4  done.\n",
            "\t Testing score after  4  epochs:\n",
            "\t \t Accuracy =  59.21875 %\n",
            "\t Epoch Number:  5  done.\n",
            "\t Testing score after  5  epochs:\n",
            "\t \t Accuracy =  60.0 %\n",
            "\t Epoch Number:  6  done.\n",
            "\t Testing score after  6  epochs:\n",
            "\t \t Accuracy =  60.0 %\n",
            "\t Epoch Number:  7  done.\n",
            "\t Testing score after  7  epochs:\n",
            "\t \t Accuracy =  59.375 %\n",
            "\t Epoch Number:  8  done.\n",
            "\t Testing score after  8  epochs:\n",
            "\t \t Accuracy =  59.21875 %\n",
            "\t Epoch Number:  9  done.\n",
            "\t Testing score after  9  epochs:\n",
            "\t \t Accuracy =  58.75 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}