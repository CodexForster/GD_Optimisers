{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_via_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yynqRNV41w0"
      },
      "source": [
        "# Adam + SGD (minibatch size of 128)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyEgKS9Q41hn"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "class ConvNet_Adam():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.inputImg = None  # The input data\n",
        "        self.strides = []  # The stride length of each layer for convolution\n",
        "        self.recfield = []  # The receptive field in each layer for convolution\n",
        "        self.lengths = []\n",
        "        self.widths = []\n",
        "        self.depths = []\n",
        "        self.weights = []  # The weights for convolution filters\n",
        "        self.node = 10  # The number of nodes in the output layer\n",
        "        self.track = []  # Keeps track of layer order, i.e Conv./Pooling/FC\n",
        "        self.learning_rate = 0.005\n",
        "        self.fc_weights = []\n",
        "        self.output_fc = []\n",
        "\n",
        "        # Adam optimization\n",
        "        self.adam_m = []\n",
        "        self.adam_v = []\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.eps = 1e-4\n",
        "        self.count = 1 # Keeps track of iteration number needed for optimisation\n",
        "\n",
        "    def addInput(self, inpImage):  # Assign the input image\n",
        "        inpImage = np.array(inpImage)\n",
        "        self.inputImg = inpImage\n",
        "        if(len(inpImage.shape) < 3):\n",
        "            num3 = 1\n",
        "            numrows = inpImage.shape[0]\n",
        "            numcols = inpImage.shape[1]\n",
        "        else:\n",
        "            num3 = inpImage.shape[0]\n",
        "            numrows = inpImage.shape[1]\n",
        "            numcols = inpImage.shape[2]\n",
        "\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "\n",
        "    def cvolume(self, s, r, f):\n",
        "        \"\"\"\n",
        "        Creates a new Conv. volume.\n",
        "        s - stride length for convolving the previous layer to create this new volume\n",
        "        r - receptive field for convolving the previous layer to create this new volume\n",
        "        f - number of filters, or in other words, the depth of this new volume\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Initializing the weights\n",
        "        W = []\n",
        "\n",
        "        # For Adam optimization\n",
        "        M = []\n",
        "        V = []\n",
        "\n",
        "        #b = np.zeros((1, stre))\n",
        "        for i in range(f):\n",
        "            W.append(np.random.randn(prevd, r, r) / (r*r))\n",
        "            M.append(np.zeros((prevd, r, r)))\n",
        "            V.append(np.zeros((prevd, r, r)))\n",
        "\n",
        "        W = np.array(W)\n",
        "        M = np.array(M)\n",
        "        V = np.array(V)\n",
        "\n",
        "        # The dimensions of the layer after convolution with the above weight array\n",
        "        numrows = (prevw - r)/s + 1\n",
        "        numcols = (prevl - r)/s + 1\n",
        "        num3 = f\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(W)\n",
        "        self.strides.append(s)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "        self.track.append('c')\n",
        "\n",
        "        self.adam_m.append(M)\n",
        "        self.adam_v.append(V)\n",
        "\n",
        "    def pmaxvolume(self, r):\n",
        "        \"\"\"\n",
        "        Creates a new max pooling layer.\n",
        "        r - the receptive field around which the max value has to be taken.\n",
        "            E.g - If r = 2, max pooling is done withing 2x2 sub matrices.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(None)\n",
        "        self.strides.append(r)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(prevl/r)\n",
        "        self.widths.append(prevw/r)\n",
        "        self.depths.append(prevd)\n",
        "        self.track.append('p')\n",
        "\n",
        "        self.adam_m.append(None)\n",
        "        self.adam_v.append(None)\n",
        "\n",
        "    def FCLayer(self, n_nodes):\n",
        "        \"\"\"\n",
        "        Creates a fully connected layer\n",
        "        n - the no.of nodes in the output layer.\n",
        "        input_fc - the input to the fully connected layer.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = int(self.depths[-1])\n",
        "        prevw = int(self.widths[-1])\n",
        "        prevl = int(self.lengths[-1])\n",
        "\n",
        "        # flatten the input\n",
        "        input_fc = np.zeros((prevd, prevw, prevl))\n",
        "        input_fc = input_fc.flatten()\n",
        "        len_input_fc = len(input_fc)\n",
        "\n",
        "        # Initialise the weights and biases for the FC layer\n",
        "        self.fc_weights = np.random.randn(len_input_fc, n_nodes) / (len_input_fc)\n",
        "        #self.fc_bias = np.zeros(n_nodes)\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(self.fc_weights)\n",
        "        self.strides.append(0)\n",
        "        self.recfield.append(0)\n",
        "        self.lengths.append(1)\n",
        "        self.widths.append(len_input_fc)\n",
        "        self.depths.append(1)\n",
        "        self.track.append('f')\n",
        "        self.node = n_nodes\n",
        "\n",
        "        self.adam_m.append(None)\n",
        "        self.adam_v.append(None)\n",
        "\n",
        "    def activFunc(self, inputArray):\n",
        "        \"\"\"\n",
        "        The activation function for the neurons in the network.\n",
        "        \"\"\"\n",
        "        # ReLU activation\n",
        "        return np.maximum(0, inputArray)\n",
        "\n",
        "    def dataLoss(self, predResults, trueResults):\n",
        "        \"\"\"\n",
        "        Returns the data loss. Cross-Entropy loss function (Softmax Classifier).\n",
        "        \"\"\"\n",
        "        # Softmax\n",
        "        loss = 0\n",
        "        sum = 0\n",
        "        for i in range(len(predResults)):\n",
        "            sum += math.exp(predResults[i])\n",
        "        correct = np.argmax(trueResults)\n",
        "        loss = (-1)*(math.log((math.exp(predResults[correct]))/sum))\n",
        "        return loss\n",
        "\n",
        "    def ConvOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Convolutional Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        if(len(W.shape) < 4):\n",
        "            f = 1\n",
        "        else:\n",
        "            f = W.shape[0]\n",
        "\n",
        "        for i in range(f):  # Run loop to create f-filters\n",
        "            for k in range(w):  # Convolve around width\n",
        "                for m in range(l):  # Convolve around length\n",
        "                    # for j in range(d):   #Run over entire depth of prevOut volume\n",
        "                    volOutput[i][k][m] += np.sum(np.multiply(W[i][:][:][:], prevOut[:, k*s: k*s + r, m*s: m*s + r])[:, :, :])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def PoolOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Pooling Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer, since there is no Weight matrix for MaxPooling, it is None\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    volOutput[j][k][m] = np.amax(prevOut[j, k*r: (k + 1)*r, m*r: (m + 1)*r])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def FCOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Implements forward pass for the FC layer. Uses a softmax Classifier.\n",
        "        n_nodes - the no.of nodes in the fully connected layer. \n",
        "        \"\"\"\n",
        "        # flatten the input\n",
        "        prevOut = prevOut.flatten()\n",
        "        #len_input_fc = len(prevOut)\n",
        "\n",
        "        totals = np.dot(prevOut, W)  # + self.fc_bias\n",
        "        # Softmax\n",
        "        exp_totals = np.exp(totals)\n",
        "\n",
        "        # Output from the FC layer\n",
        "        self.output_fc = exp_totals / (np.sum(exp_totals, axis=0))\n",
        "        return self.output_fc\n",
        "\n",
        "    def getVolumeOutput(self, n):\n",
        "        \"\"\"\n",
        "        Returns the output of the nth volume of the ConvNet.\n",
        "        \"\"\"\n",
        "        penLayer = len(self.weights) - 1  # The penultimate volume\n",
        "\n",
        "        # h stores the output of the current layer\n",
        "        h = np.array(self.inputImg)\n",
        "\n",
        "        # Loop through the hidden layers\n",
        "        for i in range(min(n, penLayer)):\n",
        "            W = self.weights[i]\n",
        "            s = self.strides[i]\n",
        "            r = self.recfield[i]\n",
        "            d = self.depths[i+1]\n",
        "            w = self.widths[i+1]\n",
        "            l = self.lengths[i+1]\n",
        "            if (self.track[i] == 'c'):\n",
        "                h = self.activFunc(self.ConvOutput(h, W, s, r, d, w, l))\n",
        "            elif (self.track[i] == 'p'):\n",
        "                h = self.PoolOutput(h, W, s, r, d, w, l)\n",
        "            else:\n",
        "                h = self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "        # Return the output\n",
        "        if n <= penLayer:\n",
        "            return h\n",
        "        else:\n",
        "            W = self.weights[n-1]\n",
        "            s = self.strides[n-1]\n",
        "            r = self.recfield[n-1]\n",
        "            d = self.depths[n]\n",
        "            w = self.widths[n]\n",
        "            l = self.lengths[n]\n",
        "            return self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "    def FCGD(self, index, trueResults):  # FC layer Gradient Descent\n",
        "        input_fc = self.getVolumeOutput(index - 1)\n",
        "\n",
        "        # Store the shape of input before flattening (to be used for backpropagation)\n",
        "        input_fc_shape = input_fc.shape\n",
        "\n",
        "        # Flatten the input\n",
        "        input_fc = input_fc.flatten()\n",
        "\n",
        "        # Get the weights and the totals of the fixed layer\n",
        "        W = self.weights[index - 1]\n",
        "        totals = np.dot(input_fc, W)\n",
        "\n",
        "        # Calculating d_L_d_out\n",
        "        correct = np.argmax(trueResults)\n",
        "        d_L_d_out = np.zeros_like(self.output_fc)\n",
        "        d_L_d_out[correct] = -1 / (self.output_fc[correct])\n",
        "\n",
        "        # Calculating d_out_d_t\n",
        "        exp_totals = np.exp(totals)\n",
        "        sum_exp_totals = np.sum(exp_totals)\n",
        "\n",
        "        d_out_d_t = np.zeros_like(self.output_fc)\n",
        "        d_out_d_t = -exp_totals[correct] * (exp_totals / (sum_exp_totals ** 2))\n",
        "        d_out_d_t[correct] = exp_totals[correct] * ((sum_exp_totals - exp_totals[correct])/(sum_exp_totals ** 2))\n",
        "\n",
        "        # Other necessary gradients\n",
        "        d_t_d_w = input_fc\n",
        "        d_t_d_inputs = W\n",
        "        d_L_d_t = d_L_d_out * d_out_d_t\n",
        "\n",
        "        # Gradients of loss wrt Weights of FC layer and Input of FC layers\n",
        "        # d_L_d_t.shape = (n_nodes,1)\n",
        "        # Adding appropriate axes to d_L_d_t and d_t_d_w(same as input_fc) for . product\n",
        "        d_L_d_w = np.dot(d_t_d_w[np.newaxis].T, d_L_d_t[np.newaxis])\n",
        "\n",
        "        # d_L_d_inputs should have the dimensions of input_fc\n",
        "        d_L_d_inputs = np.dot(d_t_d_inputs, d_L_d_t)\n",
        "\n",
        "        # The dimension of d_L_d_inputs is (len_input_fc,), so, changing the shape so it can be given to maxpool's backprop.\n",
        "        d_L_d_inputs_final = d_L_d_inputs.reshape(input_fc_shape)\n",
        "\n",
        "        W -= self.learning_rate * d_L_d_w\n",
        "        self.weights[index - 1] = W\n",
        "\n",
        "        return d_L_d_inputs_final\n",
        "\n",
        "    def PoolGD(self, dLdOut, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates gradients through the MaxPooling layer\n",
        "        dLdOut is the differential of Loss wrt the Output where Output here refers to the output of the MaxPooling layer\n",
        "        This function thus finds dLdI which is the differential of Loss wrt the Input where Input here refers to input to MaxPool layer.\n",
        "        \"\"\"\n",
        "        input_vol = self.getVolumeOutput(index - 1)\n",
        "        s = self.strides[index - 1]\n",
        "        r = self.recfield[index - 1]\n",
        "        d = dLdOut.shape[0]\n",
        "        w = dLdOut.shape[1]\n",
        "        l = dLdOut.shape[2]\n",
        "\n",
        "        # Convert the numbers to int, as the for loops below will report errors if this is not done\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "\n",
        "        # Keep track of the depth and spatial indices of where the maximum element is, in the sub arrays taken for pooling\n",
        "        d_ind = []\n",
        "        spatial_ind = []\n",
        "        # Keep track of which sub array is being taken for max pooling\n",
        "        track_w = []\n",
        "        track_l = []\n",
        "\n",
        "        dLdI = np.zeros((int(self.depths[index - 1]), int(self.lengths[index - 1]), int(self.widths[index - 1])))\n",
        "        replace = dLdOut.flatten()\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    spatial_ind.append(np.where(input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r] == input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r].max()))\n",
        "                    track_l.append(m)\n",
        "                    track_w.append(k)\n",
        "                    d_ind.append(j)\n",
        "\n",
        "        # Initialise correct values in dLdI array\n",
        "        for i in range(len(replace)):\n",
        "            width = spatial_ind[i][0][0]  # Note the (width) spatial index of the maximum element of the sub array\n",
        "            width += track_w[i]*r  # Add the (width) location depending on which sub array was taken for max pooling\n",
        "            length = spatial_ind[i][1][0]  # Note the (length) spatial index of the maximum element of the sub array\n",
        "            length += track_l[i]*r  # Add the (length) location depending on which sub array was taken for max pooling\n",
        "            depth = d_ind[i]  # Note the depth index of the maximum element of the sub array\n",
        "            dLdI[depth][width][length] = replace[i]\n",
        "\n",
        "        return dLdI\n",
        "\n",
        "    # Helper functions for convBackProp()\n",
        "    def convolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the convoluted output convFilter on inputLayer.\n",
        "        Both are two dimensional matrices square matrices.\n",
        "        inputLayer - (n, n)\n",
        "        convFilter - (f, f)\n",
        "        \"\"\"\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Defining the shape of the output matrix\n",
        "        l = (n-f) + 1\n",
        "        output_matrix = np.zeros((l, l))\n",
        "        s = 1\n",
        "\n",
        "        # Convolving\n",
        "        for row in range(l):\n",
        "            for col in range(l):\n",
        "                output_matrix[row][col] = np.sum(np.multiply(inputLayer[row:row+f, col:col+f], convFilter))\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def fullConvolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the full convoluted output of convFilter on inputLayer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Creating padding for the inputLayer matrix\n",
        "        padding = f - 1\n",
        "        new_dim = n + 2*padding\n",
        "\n",
        "        padded_input = np.zeros([new_dim, new_dim])\n",
        "        padded_input[padding:new_dim - padding,padding:new_dim - padding] = inputLayer\n",
        "\n",
        "        # Now convolve padded_input with convFilter\n",
        "        output_matrix = self.convolve(padded_input, convFilter)\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def rotate180(self, matrix):\n",
        "        \"\"\"\n",
        "        Rotates matrix by 180 degrees in the plane.\n",
        "        Takes only two dimensional matrices.\n",
        "        \"\"\"\n",
        "        return np.rot90(matrix, 2)\n",
        "\n",
        "    def ConvGD(self, dLdoutput, index, t):\n",
        "        \"\"\"\n",
        "        Function that backpropagates through a convolutional layer.\n",
        "        index = index of the current layer\n",
        "        dLdoutput = Gradient of the loss function wrt the output of the current layer (channel, row, col)\n",
        "        Returns dLdinput.\n",
        "        t = iterative index\n",
        "        \"\"\"\n",
        "        X = self.getVolumeOutput(index-1)  # Input to the current layer (channel, row, col)\n",
        "        # Weights of the current layer (numFilter, channel, row, col)\n",
        "        W = self.weights[index - 1]\n",
        "        M = self.adam_m[index - 1]\n",
        "        V = self.adam_v[index - 1]\n",
        "\n",
        "        dLdX = np.empty(X.shape)\n",
        "        dLdW = np.empty(W.shape)\n",
        "\n",
        "        dLdout = np.copy(dLdoutput)\n",
        "        dLdout[dLdout < 0] = 0\n",
        "\n",
        "        # Loop over the filters\n",
        "        numFilters = W.shape[0]\n",
        "\n",
        "        for fil_ter in range(numFilters):\n",
        "            filter_output = dLdout[fil_ter]\n",
        "\n",
        "            # Loop over the channels\n",
        "            for channel in range(W.shape[1]):\n",
        "                filter_layer = W[fil_ter][channel]\n",
        "                dWLayer = self.convolve(X[channel], filter_output)\n",
        "                dXLayer = self.rotate180(self.fullConvolve(self.rotate180(filter_layer), filter_output))\n",
        "\n",
        "                # Combine these and return in arrays\n",
        "                dLdW[fil_ter][channel] = dWLayer\n",
        "                dLdX[channel] = dXLayer\n",
        "\n",
        "        # Adam optimization\n",
        "        M = self.beta1 * M + (1 - self.beta1) * dLdW\n",
        "        Mt = M / (1 - self.beta1**t)\n",
        "        V = self.beta2 * V + (1 - self.beta2) * np.square(dLdW)\n",
        "        Vt = V / (1 - self.beta2**t)\n",
        "\n",
        "        dLdW -= self.learning_rate * Mt / (np.sqrt(Vt) + self.eps)  # Note that the weights update is being multiplied with learning rate\n",
        "        #self.weights[index - 1] = W\n",
        "\n",
        "        self.adam_m[index - 1] = M\n",
        "        self.adam_v[index - 1] = V\n",
        "\n",
        "        return (dLdX,dLdW)\n",
        "\n",
        "    def backPropagation(self, input, trueResults, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Updates weights by carrying out backpropagation using mini-batches.\n",
        "        trueResults = the expected output from the neural network.\n",
        "        \"\"\"\n",
        "        size = int(len(input)/mini_batch_size)\n",
        "        for batch in range(size):\n",
        "            mini_inp = np.array(input[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            mini_res = np.array(trueResults[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            for i in range(len(mini_inp)):\n",
        "                self.inputImg = np.array(mini_inp[i])\n",
        "                if(len(self.inputImg.shape) < 3):\n",
        "                    a = self.inputImg\n",
        "                    self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                # Called once so that all weights are initialised, just in case if not done before\n",
        "                out = self.getVolumeOutput(len(self.weights))\n",
        "                # Index keeping track of the previous layer\n",
        "                nPrev = len(self.weights)\n",
        "                doutput = self.FCGD(nPrev, mini_res[i])\n",
        "                nPrev -= 1\n",
        "                \n",
        "                # Loop over the layers\n",
        "                while nPrev - 1 >= 0:\n",
        "                    if(self.track[nPrev - 1] == 'p'):\n",
        "                        dhidden = self.PoolGD(doutput, nPrev)\n",
        "                    else:\n",
        "                        (dhidden, dLdW) = self.ConvGD(doutput, nPrev, self.count)\n",
        "                        self.weights[nPrev - 1] -= dLdW / len(mini_inp)\n",
        "                    doutput = dhidden  # Move to the previous layer\n",
        "                    nPrev -= 1\n",
        "            print('count for adam: ',self.count)\n",
        "            self.count = self.count + 1\n",
        "\n",
        "    def evaluate(self, x_train, y_train, x_test, y_test, epochs, mini_batch_size, test_freq):\n",
        "        \"\"\"\n",
        "        Train the neural network. And run test using test data to see progress of the model accuracy.\n",
        "        x_train, x_test = the inout training and testing data respectively.\n",
        "        y_train, y_test = the expected results from the CNN for the training and testing data respectively.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size = the size of mini-batches.\n",
        "        test_freq = Testing results should be printed after 'test_freq' number of epochs.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(input, Y, mini_batch_size)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "            if(i%test_freq==0):\n",
        "                print('Testing score after ',i + 1,' epochs:')\n",
        "                self.accuracy(x_test, y_test)\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "\n",
        "    def train(self, input, Y, epochs, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "        Y = the expected results from the neural network.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size is the size of mini-batches.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(input, Y, mini_batch_size)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "    def accuracy(self, X, Y):\n",
        "        \"\"\"\n",
        "        Function that takes in test data and results and calculates the accuracy of the Network.\n",
        "        \"\"\"\n",
        "        y = []\n",
        "        cor = 0\n",
        "        correct = 0\n",
        "        for i in range(len(X)):\n",
        "            self.inputImg = np.array(X[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                y.append(self.getVolumeOutput(len(self.weights)))\n",
        "        Y = np.array(Y)\n",
        "        y = np.array(y)\n",
        "        if (np.max(y) == 0):\n",
        "            y /= 1.0\n",
        "        else:\n",
        "            y /= np.max(y)\n",
        "        for i in range(len(Y)):\n",
        "            correct = np.argmax(Y[i])\n",
        "            if (np.argmax(y[i]) == correct):\n",
        "                cor += 1\n",
        "        cor /= len(Y)\n",
        "        print('Accuracy = ', cor*100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yHEkrzy41QG"
      },
      "source": [
        "# Code run for Adam + SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0454MfAM40nA"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(x, y), (x2, y2) = datasets.mnist.load_data()\n",
        "# Normalise data\n",
        "x = ((x/255) - 0.5)\n",
        "x2 = ((x2/255) - 0.5)\n",
        "\n",
        "# Create instance of NeuralNetwork\n",
        "model = ConvNet_SGD()\n",
        "X = np.random.randn(1,28,28)\n",
        "model.addInput(X) # Input layer\n",
        "model.cvolume(1,5,10) # Add Convolutional volume (stride length, receptive field, filters)\n",
        "model.pmaxvolume(2) # Add Pooling layer (receptive field)\n",
        "model.FCLayer(10)  # Add FC Layer (number of classifiers)\n",
        "# Get final output layer. It is advised to run it once before training, so that all variables are initialised.\n",
        "print('Test Run Output: ',model.getVolumeOutput(3))\n",
        "\n",
        "#Since we test the CNN with MNIST data, we write the target output in the required format before sent to training/testing.\n",
        "results = np.zeros((len(y),10))  \n",
        "for i in range(len(y)):\n",
        "    results[i,y[i]] = 1\n",
        "\n",
        "model.evaluate(x[0:2560], results[0:2560], x[2560:5120], results[2560:5120], 5, 128, 1)\n",
        "#model.train(x[0:2000], results[0:2000], 5) # Train model with 2000 images for 5 epochs\n",
        "#model.accuracy(x[2000:4000], results[2000:4000])  # Predict accuracy using test data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cdZEcyqisAO"
      },
      "source": [
        "# SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf9wLuEnirl_"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "class ConvNet_SGD():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.inputImg = None  # The input data\n",
        "        self.strides = []  # The stride length of each layer for convolution\n",
        "        self.recfield = []  # The receptive field in each layer for convolution\n",
        "        self.lengths = []\n",
        "        self.widths = []\n",
        "        self.depths = []\n",
        "        self.weights = []  # The weights for convolution filters\n",
        "        self.node = 10  # The number of nodes in the output layer\n",
        "        self.track = []  # Keeps track of layer order, i.e Conv./Pooling/FC\n",
        "        self.learning_rate = 0.005\n",
        "        self.fc_weights = []\n",
        "        self.output_fc = []\n",
        "\n",
        "    def addInput(self, inpImage):  # Assign the input image\n",
        "        inpImage = np.array(inpImage)\n",
        "        self.inputImg = inpImage\n",
        "        if(len(inpImage.shape) < 3):\n",
        "            num3 = 1\n",
        "            numrows = inpImage.shape[0]\n",
        "            numcols = inpImage.shape[1]\n",
        "        else:\n",
        "            num3 = inpImage.shape[0]\n",
        "            numrows = inpImage.shape[1]\n",
        "            numcols = inpImage.shape[2]\n",
        "\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "\n",
        "    def cvolume(self, s, r, f):\n",
        "        \"\"\"\n",
        "        Creates a new Conv. volume.\n",
        "        s - stride length for convolving the previous layer to create this new volume\n",
        "        r - receptive field for convolving the previous layer to create this new volume\n",
        "        f - number of filters, or in other words, the depth of this new volume\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Initializing the weights\n",
        "        W = []\n",
        "        #b = np.zeros((1, stre))\n",
        "        for i in range(f):\n",
        "            W.append(np.random.randn(prevd, r, r) / (r*r))\n",
        "\n",
        "        W = np.array(W)\n",
        "\n",
        "        # The dimensions of the layer after convolution with the above weight array\n",
        "        numrows = (prevw - r)/s + 1\n",
        "        numcols = (prevl - r)/s + 1\n",
        "        num3 = f\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(W)\n",
        "        self.strides.append(s)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "        self.track.append('c')\n",
        "\n",
        "    def pmaxvolume(self, r):\n",
        "        \"\"\"\n",
        "        Creates a new max pooling layer.\n",
        "        r - the receptive field around which the max value has to be taken.\n",
        "            E.g - If r = 2, max pooling is done withing 2x2 sub matrices.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(None)\n",
        "        self.strides.append(r)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(prevl/r)\n",
        "        self.widths.append(prevw/r)\n",
        "        self.depths.append(prevd)\n",
        "        self.track.append('p')\n",
        "\n",
        "    def FCLayer(self, n_nodes):\n",
        "        \"\"\"\n",
        "        Creates a fully connected layer\n",
        "        n - the no.of nodes in the output layer.\n",
        "        input_fc - the input to the fully connected layer.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = int(self.depths[-1])\n",
        "        prevw = int(self.widths[-1])\n",
        "        prevl = int(self.lengths[-1])\n",
        "\n",
        "        # flatten the input\n",
        "        input_fc = np.zeros((prevd, prevw, prevl))\n",
        "        input_fc = input_fc.flatten()\n",
        "        len_input_fc = len(input_fc)\n",
        "\n",
        "        # Initialise the weights and biases for the FC layer\n",
        "        self.fc_weights = np.random.randn(len_input_fc, n_nodes) / (len_input_fc)\n",
        "        #self.fc_bias = np.zeros(n_nodes)\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(self.fc_weights)\n",
        "        self.strides.append(0)\n",
        "        self.recfield.append(0)\n",
        "        self.lengths.append(1)\n",
        "        self.widths.append(len_input_fc)\n",
        "        self.depths.append(1)\n",
        "        self.track.append('f')\n",
        "        self.node = n_nodes\n",
        "\n",
        "    def activFunc(self, inputArray):\n",
        "        \"\"\"\n",
        "        The activation function for the neurons in the network.\n",
        "        \"\"\"\n",
        "        # ReLU activation\n",
        "        \n",
        "        return np.maximum(0, inputArray)\n",
        "\n",
        "    def dataLoss(self, predResults, trueResults):\n",
        "        \"\"\"\n",
        "        Returns the data loss. Cross-Entropy loss function (Softmax Classifier).\n",
        "        \"\"\"\n",
        "        # L2 loss\n",
        "        loss = 0\n",
        "        sum = 0\n",
        "        for i in range(len(predResults)):\n",
        "            sum += math.exp(predResults[i])\n",
        "        correct = np.argmax(trueResults)\n",
        "        loss = (-1)*(math.log((math.exp(predResults[correct]))/sum))\n",
        "        return loss\n",
        "\n",
        "    def ConvOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Convolutional Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        if(len(W.shape) < 4):\n",
        "            f = 1\n",
        "        else:\n",
        "            f = W.shape[0]\n",
        "\n",
        "        for i in range(f):  # Run loop to create f-filters\n",
        "            for k in range(w):  # Convolve around width\n",
        "                for m in range(l):  # Convolve around length\n",
        "                    # for j in range(d):   #Run over entire depth of prevOut volume\n",
        "                    volOutput[i][k][m] += np.sum(np.multiply(W[i][:][:][:], prevOut[:, k*s: k*s + r, m*s: m*s + r])[:, :, :])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def PoolOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Pooling Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer, since there is no Weight matrix for MaxPooling, it is None\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    volOutput[j][k][m] = np.amax(prevOut[j, k*r: (k + 1)*r, m*r: (m + 1)*r])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def FCOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Implements forward pass for the FC layer. Uses a softmax Classifier.\n",
        "        n_nodes - the no.of nodes in the fully connected layer. \n",
        "        \"\"\"\n",
        "        # flatten the input\n",
        "        prevOut = prevOut.flatten()\n",
        "        #len_input_fc = len(prevOut)\n",
        "\n",
        "        totals = np.dot(prevOut, W)  # + self.fc_bias\n",
        "        # Softmax\n",
        "        exp_totals = np.exp(totals)\n",
        "\n",
        "        # Output from the FC layer\n",
        "        self.output_fc = exp_totals / (np.sum(exp_totals, axis=0))\n",
        "        \n",
        "        return self.output_fc\n",
        "\n",
        "    def getVolumeOutput(self, n):\n",
        "        \"\"\"\n",
        "        Returns the output of the nth volume of the ConvNet.\n",
        "        \"\"\"\n",
        "        penLayer = len(self.weights) - 1  # The penultimate volume\n",
        "\n",
        "        # h stores the output of the current layer\n",
        "        h = np.array(self.inputImg)\n",
        "\n",
        "        # Loop through the hidden layers\n",
        "        for i in range(min(n, penLayer)):\n",
        "            W = self.weights[i]\n",
        "            s = self.strides[i]\n",
        "            r = self.recfield[i]\n",
        "            d = self.depths[i+1]\n",
        "            w = self.widths[i+1]\n",
        "            l = self.lengths[i+1]\n",
        "            if (self.track[i] == 'c'):\n",
        "                h = self.activFunc(self.ConvOutput(h, W, s, r, d, w, l))\n",
        "            elif (self.track[i] == 'p'):\n",
        "                h = self.PoolOutput(h, W, s, r, d, w, l)\n",
        "            else:\n",
        "                h = self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "        # Return the output\n",
        "        if n <= penLayer:\n",
        "            return h\n",
        "        else:\n",
        "            W = self.weights[n-1]\n",
        "            s = self.strides[n-1]\n",
        "            r = self.recfield[n-1]\n",
        "            d = self.depths[n]\n",
        "            w = self.widths[n]\n",
        "            l = self.lengths[n]\n",
        "            return self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "    def FCGD(self, index, trueResults):  # FC layer Gradient Descent\n",
        "        input_fc = self.getVolumeOutput(index - 1)\n",
        "\n",
        "        # Store the shape of input before flattening (to be used for backpropagation)\n",
        "        input_fc_shape = input_fc.shape\n",
        "\n",
        "        # Flatten the input\n",
        "        input_fc = input_fc.flatten()\n",
        "        \n",
        "        # Get the weights and the totals of the fixed layer\n",
        "        W = self.weights[index - 1]\n",
        "        totals = np.dot(input_fc, W)\n",
        "        \n",
        "        # Calculating d_L_d_out\n",
        "        correct = np.argmax(trueResults)\n",
        "        d_L_d_out = np.zeros_like(self.output_fc)\n",
        "        d_L_d_out[correct] = -1 / (self.output_fc[correct])\n",
        "        \n",
        "        # Calculating d_out_d_t\n",
        "        exp_totals = np.exp(totals)\n",
        "        sum_exp_totals = np.sum(exp_totals)\n",
        "        \n",
        "        d_out_d_t = np.zeros_like(self.output_fc)\n",
        "        d_out_d_t = -exp_totals[correct] * (exp_totals / (sum_exp_totals ** 2))\n",
        "        d_out_d_t[correct] = exp_totals[correct] * ((sum_exp_totals - exp_totals[correct])/(sum_exp_totals ** 2))\n",
        "                \n",
        "        # Other necessary gradients\n",
        "        d_t_d_w = input_fc\n",
        "        d_t_d_inputs = W\n",
        "        d_L_d_t = d_L_d_out * d_out_d_t\n",
        "\n",
        "        # Gradients of loss wrt Weights of FC layer and Input of FC layers\n",
        "        # d_L_d_t.shape = (n_nodes,1)\n",
        "        # Adding appropriate axes to d_L_d_t and d_t_d_w(same as input_fc) for . product\n",
        "        d_L_d_w = np.dot(d_t_d_w[np.newaxis].T, d_L_d_t[np.newaxis])\n",
        "\n",
        "        # d_L_d_inputs should have the dimensions of input_fc\n",
        "        d_L_d_inputs = np.dot(d_t_d_inputs, d_L_d_t)\n",
        "\n",
        "        # The dimension of d_L_d_inputs is (len_input_fc,), so, changing the shape so it can be given to maxpool's backprop.\n",
        "        d_L_d_inputs_final = d_L_d_inputs.reshape(input_fc_shape)\n",
        "\n",
        "        W -= self.learning_rate * d_L_d_w\n",
        "        self.weights[index - 1] = W\n",
        "        \n",
        "        return d_L_d_inputs_final\n",
        "\n",
        "    def PoolGD(self, dLdOut, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates gradients through the MaxPooling layer\n",
        "        dLdOut is the differential of Loss wrt the Output where Output here refers to the output of the MaxPooling layer\n",
        "        This function thus finds dLdI which is the differential of Loss wrt the Input where Input here refers to input to MaxPool layer.\n",
        "        \"\"\"\n",
        "        input_vol = self.getVolumeOutput(index - 1)\n",
        "        s = self.strides[index - 1]\n",
        "        r = self.recfield[index - 1]\n",
        "        d = dLdOut.shape[0]\n",
        "        w = dLdOut.shape[1]\n",
        "        l = dLdOut.shape[2]\n",
        "\n",
        "        # Convert the numbers to int, as the for loops below will report errors if this is not done\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "\n",
        "        # Keep track of the depth and spatial indices of where the maximum element is, in the sub arrays taken for pooling\n",
        "        d_ind = []\n",
        "        spatial_ind = []\n",
        "        # Keep track of which sub array is being taken for max pooling\n",
        "        track_w = []\n",
        "        track_l = []\n",
        "\n",
        "        dLdI = np.zeros((int(self.depths[index - 1]), int(self.lengths[index - 1]), int(self.widths[index - 1])))\n",
        "        replace = dLdOut.flatten()\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    spatial_ind.append(np.where(input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r] == input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r].max()))\n",
        "                    track_l.append(m)\n",
        "                    track_w.append(k)\n",
        "                    d_ind.append(j)\n",
        "\n",
        "        # Initialise correct values in dLdI array\n",
        "        for i in range(len(replace)):\n",
        "            width = spatial_ind[i][0][0]  # Note the (width) spatial index of the maximum element of the sub array\n",
        "            width += track_w[i]*r  # Add the (width) location depending on which sub array was taken for max pooling\n",
        "            length = spatial_ind[i][1][0]  # Note the (length) spatial index of the maximum element of the sub array\n",
        "            length += track_l[i]*r  # Add the (length) location depending on which sub array was taken for max pooling\n",
        "            depth = d_ind[i]  # Note the depth index of the maximum element of the sub array\n",
        "            dLdI[depth][width][length] = replace[i]\n",
        "        \n",
        "        return dLdI\n",
        "\n",
        "    # Helper functions for convBackProp()\n",
        "    def convolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the convoluted output convFilter on inputLayer.\n",
        "        Both are two dimensional matrices square matrices.\n",
        "        inputLayer - (n, n)\n",
        "        convFilter - (f, f)\n",
        "        \"\"\"\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Defining the shape of the output matrix\n",
        "        l = (n-f) + 1\n",
        "        output_matrix = np.zeros((l, l))\n",
        "        s = 1\n",
        "\n",
        "        # Convolving\n",
        "        for row in range(l):\n",
        "            for col in range(l):\n",
        "                output_matrix[row][col] = np.sum(np.multiply(inputLayer[row:row+f, col:col+f], convFilter))\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def fullConvolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the full convoluted output of convFilter on inputLayer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Creating padding for the inputLayer matrix\n",
        "        padding = f - 1\n",
        "        new_dim = n + 2*padding\n",
        "\n",
        "        padded_input = np.zeros([new_dim, new_dim])\n",
        "        padded_input[padding:new_dim - padding,padding:new_dim - padding] = inputLayer\n",
        "\n",
        "        # Now convolve padded_input with convFilter\n",
        "        output_matrix = self.convolve(padded_input, convFilter)\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def rotate180(self, matrix):\n",
        "        \"\"\"\n",
        "        Rotates matrix by 180 degrees in the plane.\n",
        "        Takes only two dimensional matrices.\n",
        "        \"\"\"\n",
        "        return np.rot90(matrix, 2)\n",
        "\n",
        "    def ConvGD(self, dLdoutput, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates through a convolutional layer.\n",
        "        index = index of the current layer\n",
        "        dLdoutput = Gradient of the loss function wrt the output of the current layer (channel, row, col)\n",
        "        Returns dLdinput.\n",
        "        \"\"\"\n",
        "        X = self.getVolumeOutput(index-1)  # Input to the current layer (channel, row, col)\n",
        "        # Weights of the current layer (numFilter, channel, row, col)\n",
        "        W = self.weights[index - 1]\n",
        "\n",
        "        dLdX = np.empty(X.shape)\n",
        "        dLdW = np.empty(W.shape)\n",
        "\n",
        "        dLdout = np.copy(dLdoutput)\n",
        "        dLdout[dLdout < 0] = 0\n",
        "\n",
        "        # Loop over the filters\n",
        "        numFilters = W.shape[0]\n",
        "\n",
        "        for fil_ter in range(numFilters):\n",
        "            filter_output = dLdout[fil_ter]\n",
        "\n",
        "            # Loop over the channels\n",
        "            for channel in range(W.shape[1]):\n",
        "                filter_layer = W[fil_ter][channel]\n",
        "                dWLayer = self.convolve(X[channel], filter_output)\n",
        "                dXLayer = self.rotate180(self.fullConvolve(self.rotate180(filter_layer), filter_output))\n",
        "\n",
        "                # Combine these and return in arrays\n",
        "                dLdW[fil_ter][channel] = dWLayer\n",
        "                dLdX[channel] = dXLayer\n",
        "\n",
        "        W -= self.learning_rate * dLdW\n",
        "        self.weights[index - 1] = W\n",
        "        return dLdX\n",
        "\n",
        "    def backPropagation(self, input, trueResults):\n",
        "        \"\"\"\n",
        "        Updates weights by carrying out backpropagation.\n",
        "        trueResults = the expected output from the neural network.\n",
        "        \"\"\"\n",
        "        for i in range(len(input)):\n",
        "            self.inputImg = np.array(input[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "            # Called once so that all weights are initialised, just in case if not done before\n",
        "            out = self.getVolumeOutput(len(self.weights))\n",
        "            # Index keeping track of the previous layer\n",
        "            nPrev = len(self.weights)\n",
        "            doutput = self.FCGD(nPrev, trueResults[i])\n",
        "            nPrev -= 1\n",
        "\n",
        "            # Loop over the layers\n",
        "            while nPrev - 1 >= 0:\n",
        "                if(self.track[nPrev - 1] == 'p'):\n",
        "                    dhidden = self.PoolGD(doutput, nPrev)\n",
        "                else:\n",
        "                    dhidden = self.ConvGD(doutput, nPrev)\n",
        "                doutput = dhidden  # Move to the previous layer\n",
        "                nPrev -= 1\n",
        "\n",
        "    def train(self, input, Y, epochs):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "        Y = the expected results from the neural network.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(input, Y)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "    def evaluate(self, x_train, y_train, x_test, y_test, epochs, test_freq):\n",
        "        \"\"\"\n",
        "        Train the neural network. And run test using test data to see progress of the model accuracy.\n",
        "        x_train, x_test = the inout training and testing data respectively.\n",
        "        y_train, y_test = the expected results from the CNN for the training and testing data respectively.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        test_freq = Testing results should be printed after 'test_freq' number of epochs.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(x_train, y_train)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "            if(i%test_freq==0):\n",
        "                print('Testing score after ',i + 1,' epochs:')\n",
        "                self.accuracy(x_test, y_test)\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "\n",
        "    def accuracy(self, X, Y):\n",
        "        \"\"\"\n",
        "        Function that takes in test data and results and calculates the accuracy of the Network.\n",
        "        \"\"\"\n",
        "        y = []\n",
        "        cor = 0\n",
        "        correct = 0\n",
        "        for i in range(len(X)):\n",
        "            self.inputImg = np.array(X[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                y.append(self.getVolumeOutput(len(self.weights)))\n",
        "        Y = np.array(Y)\n",
        "        y = np.array(y)\n",
        "        if (np.max(y) == 0):\n",
        "            y /= 1.0\n",
        "        else:\n",
        "            y /= np.max(y)\n",
        "        for i in range(len(Y)):\n",
        "            correct = np.argmax(Y[i])\n",
        "            if (np.argmax(y[i]) == correct):\n",
        "                cor += 1\n",
        "        cor /= len(Y)\n",
        "        print('Accuracy = ', cor*100, '%')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDMgYlVrixod"
      },
      "source": [
        "# Code Run for SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LQi7bZjiznD",
        "outputId": "014ef96f-a923-47d8-ed81-5fc0b2b856c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(x, y), (x2, y2) = datasets.mnist.load_data()\n",
        "# Normalise data\n",
        "x = ((x/255) - 0.5)\n",
        "x2 = ((x2/255) - 0.5)\n",
        "\n",
        "# Create instance of NeuralNetwork\n",
        "model = ConvNet_SGD()\n",
        "X = np.random.randn(1,28,28)\n",
        "model.addInput(X) # Input layer\n",
        "model.cvolume(1,5,10) # Add Convolutional volume (stride length, receptive field, filters)\n",
        "model.pmaxvolume(2) # Add Pooling layer (receptive field)\n",
        "model.FCLayer(10)  # Add FC Layer (number of classifiers)\n",
        "# Get final output layer. It is advised to run it once before training, so that all variables are initialised.\n",
        "print('Test Run Output: ',model.getVolumeOutput(3))\n",
        "\n",
        "#Since we test the CNN with MNIST data, we write the target output in the required format before sent to training/testing.\n",
        "results = np.zeros((len(y),10))  \n",
        "for i in range(len(y)):\n",
        "    results[i,y[i]] = 1\n",
        "\n",
        "model.evaluate(x[0:2000], results[0:2000], x[2000:4000], results[2000:4000], 5, 1)\n",
        "#model.train(x[0:2000], results[0:2000], 5) # Train model with 2000 images for 5 epochs\n",
        "#model.accuracy(x[2000:4000], results[2000:4000])  # Predict accuracy using test data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Run Output:  [0.09997204 0.09983971 0.10041394 0.10040842 0.10016026 0.09975728\n",
            " 0.10072183 0.10047065 0.09918427 0.09907158]\n",
            "Epoch Number:  1  done.\n",
            "Testing score after  0  epochs:\n",
            "Accuracy =  70.8 %\n",
            "Epoch Number:  2  done.\n",
            "Testing score after  1  epochs:\n",
            "Accuracy =  71.15 %\n",
            "Epoch Number:  3  done.\n",
            "Testing score after  2  epochs:\n",
            "Accuracy =  71.25 %\n",
            "Epoch Number:  4  done.\n",
            "Testing score after  3  epochs:\n",
            "Accuracy =  72.0 %\n",
            "Epoch Number:  5  done.\n",
            "Testing score after  4  epochs:\n",
            "Accuracy =  71.65 %\n",
            "Training Complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbeAg2WdN4NJ"
      },
      "source": [
        "Change accuracy function\n",
        "\n",
        "Check weights (plot them)\n",
        "\n",
        "Set control as Keras\n",
        "\n",
        "testing ~ 3 mins\n",
        "training ~ 12 mins"
      ]
    }
  ]
}