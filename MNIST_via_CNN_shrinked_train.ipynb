{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_via_CNN_shrinked_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbaF0qYB9jl2"
      },
      "source": [
        "# HerBo + Minibatch Gradient Descent (Size of 128)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHuJH1MG-sFC"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "class convnet_herbo_mbgd():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.inputImg = None  # The input data\n",
        "        self.strides = []  # The stride length of each layer for convolution\n",
        "        self.recfield = []  # The receptive field in each layer for convolution\n",
        "        self.lengths = []\n",
        "        self.widths = []\n",
        "        self.depths = []\n",
        "        self.weights = []  # The weights for convolution filters\n",
        "        #self.weights2 = [] # The array to keep track of previous weights so we can implement HerBo\n",
        "        self.node = 10  # The number of nodes in the output layer\n",
        "        self.track = []  # Keeps track of layer order, i.e Conv./Pooling/FC\n",
        "        self.learning_rate = 0.005\n",
        "        self.fc_weights = []\n",
        "        #self.fc_weights2 = [] # The array to keep track of previous fc_weights so we can implement HerBo\n",
        "        self.output_fc = []\n",
        "        self.n = 3 # The n'th moment calculations\n",
        "        self.beta = 0.1 # The hyperparameter involved in addition of higher moment terms\n",
        "        self.count = 1 # Keeps track of iteration number needed for optimisation\n",
        "\n",
        "    def addInput(self, inpImage):  # Assign the input image\n",
        "        inpImage = np.array(inpImage)\n",
        "        self.inputImg = inpImage\n",
        "        if(len(inpImage.shape) < 3):\n",
        "            num3 = 1\n",
        "            numrows = inpImage.shape[0]\n",
        "            numcols = inpImage.shape[1]\n",
        "        else:\n",
        "            num3 = inpImage.shape[0]\n",
        "            numrows = inpImage.shape[1]\n",
        "            numcols = inpImage.shape[2]\n",
        "\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "\n",
        "    def cvolume(self, s, r, f):\n",
        "        \"\"\"\n",
        "        Creates a new Conv. volume.\n",
        "        s - stride length for convolving the previous layer to create this new volume\n",
        "        r - receptive field for convolving the previous layer to create this new volume\n",
        "        f - number of filters, or in other words, the depth of this new volume\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Initializing the weights\n",
        "        W = []\n",
        "\n",
        "        #b = np.zeros((1, stre))\n",
        "        for i in range(f):\n",
        "            W.append(np.random.randn(prevd, r, r) / (r*r))\n",
        "\n",
        "        W = np.array(W)\n",
        "\n",
        "        # The dimensions of the layer after convolution with the above weight array\n",
        "        numrows = (prevw - r)/s + 1\n",
        "        numcols = (prevl - r)/s + 1\n",
        "        num3 = f\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(W)\n",
        "        self.strides.append(s)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "        self.track.append('c')\n",
        "\n",
        "\n",
        "    def pmaxvolume(self, r):\n",
        "        \"\"\"\n",
        "        Creates a new max pooling layer.\n",
        "        r - the receptive field around which the max value has to be taken.\n",
        "            E.g - If r = 2, max pooling is done withing 2x2 sub matrices.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(None)\n",
        "        self.strides.append(r)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(prevl/r)\n",
        "        self.widths.append(prevw/r)\n",
        "        self.depths.append(prevd)\n",
        "        self.track.append('p')\n",
        "\n",
        "\n",
        "    def FCLayer(self, n_nodes):\n",
        "        \"\"\"\n",
        "        Creates a fully connected layer\n",
        "        n - the no.of nodes in the output layer.\n",
        "        input_fc - the input to the fully connected layer.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = int(self.depths[-1])\n",
        "        prevw = int(self.widths[-1])\n",
        "        prevl = int(self.lengths[-1])\n",
        "\n",
        "        # flatten the input\n",
        "        input_fc = np.zeros((prevd, prevw, prevl))\n",
        "        input_fc = input_fc.flatten()\n",
        "        len_input_fc = len(input_fc)\n",
        "\n",
        "        # Initialise the weights and biases for the FC layer\n",
        "        self.fc_weights = np.random.randn(len_input_fc, n_nodes) / (len_input_fc)\n",
        "        #self.fc_bias = np.zeros(n_nodes)\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(self.fc_weights)\n",
        "        self.strides.append(0)\n",
        "        self.recfield.append(0)\n",
        "        self.lengths.append(1)\n",
        "        self.widths.append(len_input_fc)\n",
        "        self.depths.append(1)\n",
        "        self.track.append('f')\n",
        "        self.node = n_nodes\n",
        "\n",
        "\n",
        "    def activFunc(self, inputArray):\n",
        "        \"\"\"\n",
        "        The activation function for the neurons in the network.\n",
        "        \"\"\"\n",
        "        # ReLU activation\n",
        "        return np.maximum(0, inputArray)\n",
        "\n",
        "    def dataLoss(self, predResults, trueResults):\n",
        "        \"\"\"\n",
        "        Returns the data loss. Cross-Entropy loss function (Softmax Classifier).\n",
        "        \"\"\"\n",
        "        # Softmax\n",
        "        loss = 0\n",
        "        sum = 0\n",
        "        for i in range(len(predResults)):\n",
        "            sum += math.exp(predResults[i])\n",
        "        correct = np.argmax(trueResults)\n",
        "        loss = (-1)*(math.log((math.exp(predResults[correct]))/sum))\n",
        "        return loss\n",
        "\n",
        "    def ConvOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Convolutional Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        if(len(W.shape) < 4):\n",
        "            f = 1\n",
        "        else:\n",
        "            f = W.shape[0]\n",
        "\n",
        "        for i in range(f):  # Run loop to create f-filters\n",
        "            for k in range(w):  # Convolve around width\n",
        "                for m in range(l):  # Convolve around length\n",
        "                    # for j in range(d):   #Run over entire depth of prevOut volume\n",
        "                    volOutput[i][k][m] += np.sum(np.multiply(W[i][:][:][:], prevOut[:, k*s: k*s + r, m*s: m*s + r])[:, :, :])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def PoolOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Pooling Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer, since there is no Weight matrix for MaxPooling, it is None\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    volOutput[j][k][m] = np.amax(prevOut[j, k*r: (k + 1)*r, m*r: (m + 1)*r])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def FCOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Implements forward pass for the FC layer. Uses a softmax Classifier.\n",
        "        n_nodes - the no.of nodes in the fully connected layer. \n",
        "        \"\"\"\n",
        "        # flatten the input\n",
        "        prevOut = prevOut.flatten()\n",
        "        #len_input_fc = len(prevOut)\n",
        "\n",
        "        totals = np.dot(prevOut, W)  # + self.fc_bias\n",
        "        # Softmax\n",
        "        exp_totals = np.exp(totals)\n",
        "\n",
        "        # Output from the FC layer\n",
        "        self.output_fc = exp_totals / (np.sum(exp_totals, axis=0))\n",
        "        return self.output_fc\n",
        "\n",
        "    def getVolumeOutput(self, n):\n",
        "        \"\"\"\n",
        "        Returns the output of the nth volume of the ConvNet.\n",
        "        \"\"\"\n",
        "        penLayer = len(self.weights) - 1  # The penultimate volume\n",
        "\n",
        "        # h stores the output of the current layer\n",
        "        h = np.array(self.inputImg)\n",
        "\n",
        "        # Loop through the hidden layers\n",
        "        for i in range(min(n, penLayer)):\n",
        "            W = self.weights[i]\n",
        "            s = self.strides[i]\n",
        "            r = self.recfield[i]\n",
        "            d = self.depths[i+1]\n",
        "            w = self.widths[i+1]\n",
        "            l = self.lengths[i+1]\n",
        "            if (self.track[i] == 'c'):\n",
        "                h = self.activFunc(self.ConvOutput(h, W, s, r, d, w, l))\n",
        "            elif (self.track[i] == 'p'):\n",
        "                h = self.PoolOutput(h, W, s, r, d, w, l)\n",
        "            else:\n",
        "                h = self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "        # Return the output\n",
        "        if n <= penLayer:\n",
        "            return h\n",
        "        else:\n",
        "            W = self.weights[n-1]\n",
        "            s = self.strides[n-1]\n",
        "            r = self.recfield[n-1]\n",
        "            d = self.depths[n]\n",
        "            w = self.widths[n]\n",
        "            l = self.lengths[n]\n",
        "            return self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "    def FCGD(self, index, trueResults):  # FC layer Gradient Descent\n",
        "        input_fc = self.getVolumeOutput(index - 1)\n",
        "\n",
        "        # Store the shape of input before flattening (to be used for backpropagation)\n",
        "        input_fc_shape = input_fc.shape\n",
        "\n",
        "        # Flatten the input\n",
        "        input_fc = input_fc.flatten()\n",
        "\n",
        "        # Get the weights and the totals of the fixed layer\n",
        "        W = self.weights[index - 1]\n",
        "        totals = np.dot(input_fc, W)\n",
        "\n",
        "        # Calculating d_L_d_out\n",
        "        correct = np.argmax(trueResults)\n",
        "        d_L_d_out = np.zeros_like(self.output_fc)\n",
        "        d_L_d_out[correct] = -1 / (self.output_fc[correct])\n",
        "\n",
        "        # Calculating d_out_d_t\n",
        "        exp_totals = np.exp(totals)\n",
        "        sum_exp_totals = np.sum(exp_totals)\n",
        "\n",
        "        d_out_d_t = np.zeros_like(self.output_fc)\n",
        "        d_out_d_t = -exp_totals[correct] * (exp_totals / (sum_exp_totals ** 2))\n",
        "        d_out_d_t[correct] = exp_totals[correct] * ((sum_exp_totals - exp_totals[correct])/(sum_exp_totals ** 2))\n",
        "\n",
        "        # Other necessary gradients\n",
        "        d_t_d_w = input_fc\n",
        "        d_t_d_inputs = W\n",
        "        d_L_d_t = d_L_d_out * d_out_d_t\n",
        "\n",
        "        # Gradients of loss wrt Weights of FC layer and Input of FC layers\n",
        "        # d_L_d_t.shape = (n_nodes,1)\n",
        "        # Adding appropriate axes to d_L_d_t and d_t_d_w(same as input_fc) for . product\n",
        "        d_L_d_w = np.dot(d_t_d_w[np.newaxis].T, d_L_d_t[np.newaxis])\n",
        "\n",
        "        # d_L_d_inputs should have the dimensions of input_fc\n",
        "        d_L_d_inputs = np.dot(d_t_d_inputs, d_L_d_t)\n",
        "\n",
        "        # The dimension of d_L_d_inputs is (len_input_fc,), so, changing the shape so it can be given to maxpool's backprop.\n",
        "        d_L_d_inputs_final = d_L_d_inputs.reshape(input_fc_shape)\n",
        "\n",
        "        #W -= self.learning_rate * d_L_d_w\n",
        "        #self.weights[index - 1] = W\n",
        "        dW = -self.learning_rate * d_L_d_w\n",
        "        return (d_L_d_inputs_final, dW)\n",
        "\n",
        "    def PoolGD(self, dLdOut, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates gradients through the MaxPooling layer\n",
        "        dLdOut is the differential of Loss wrt the Output where Output here refers to the output of the MaxPooling layer\n",
        "        This function thus finds dLdI which is the differential of Loss wrt the Input where Input here refers to input to MaxPool layer.\n",
        "        \"\"\"\n",
        "        input_vol = self.getVolumeOutput(index - 1)\n",
        "        s = self.strides[index - 1]\n",
        "        r = self.recfield[index - 1]\n",
        "        d = dLdOut.shape[0]\n",
        "        w = dLdOut.shape[1]\n",
        "        l = dLdOut.shape[2]\n",
        "\n",
        "        # Convert the numbers to int, as the for loops below will report errors if this is not done\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "\n",
        "        # Keep track of the depth and spatial indices of where the maximum element is, in the sub arrays taken for pooling\n",
        "        d_ind = []\n",
        "        spatial_ind = []\n",
        "        # Keep track of which sub array is being taken for max pooling\n",
        "        track_w = []\n",
        "        track_l = []\n",
        "\n",
        "        dLdI = np.zeros((int(self.depths[index - 1]), int(self.lengths[index - 1]), int(self.widths[index - 1])))\n",
        "        replace = dLdOut.flatten()\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    spatial_ind.append(np.where(input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r] == input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r].max()))\n",
        "                    track_l.append(m)\n",
        "                    track_w.append(k)\n",
        "                    d_ind.append(j)\n",
        "\n",
        "        # Initialise correct values in dLdI array\n",
        "        for i in range(len(replace)):\n",
        "            width = spatial_ind[i][0][0]  # Note the (width) spatial index of the maximum element of the sub array\n",
        "            width += track_w[i]*r  # Add the (width) location depending on which sub array was taken for max pooling\n",
        "            length = spatial_ind[i][1][0]  # Note the (length) spatial index of the maximum element of the sub array\n",
        "            length += track_l[i]*r  # Add the (length) location depending on which sub array was taken for max pooling\n",
        "            depth = d_ind[i]  # Note the depth index of the maximum element of the sub array\n",
        "            dLdI[depth][width][length] = replace[i]\n",
        "\n",
        "        return dLdI\n",
        "\n",
        "    # Helper functions for convBackProp()\n",
        "    def convolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the convoluted output convFilter on inputLayer.\n",
        "        Both are two dimensional matrices square matrices.\n",
        "        inputLayer - (n, n)\n",
        "        convFilter - (f, f)\n",
        "        \"\"\"\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Defining the shape of the output matrix\n",
        "        l = (n-f) + 1\n",
        "        output_matrix = np.zeros((l, l))\n",
        "        s = 1\n",
        "\n",
        "        # Convolving\n",
        "        for row in range(l):\n",
        "            for col in range(l):\n",
        "                output_matrix[row][col] = np.sum(np.multiply(inputLayer[row:row+f, col:col+f], convFilter))\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def fullConvolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the full convoluted output of convFilter on inputLayer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Creating padding for the inputLayer matrix\n",
        "        padding = f - 1\n",
        "        new_dim = n + 2*padding\n",
        "\n",
        "        padded_input = np.zeros([new_dim, new_dim])\n",
        "        padded_input[padding:new_dim - padding,padding:new_dim - padding] = inputLayer\n",
        "\n",
        "        # Now convolve padded_input with convFilter\n",
        "        output_matrix = self.convolve(padded_input, convFilter)\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def rotate180(self, matrix):\n",
        "        \"\"\"\n",
        "        Rotates matrix by 180 degrees in the plane.\n",
        "        Takes only two dimensional matrices.\n",
        "        \"\"\"\n",
        "        return np.rot90(matrix, 2)\n",
        "\n",
        "    def ConvGD(self, dLdoutput, index, t):\n",
        "        \"\"\"\n",
        "        Function that backpropagates through a convolutional layer.\n",
        "        index = index of the current layer\n",
        "        dLdoutput = Gradient of the loss function wrt the output of the current layer (channel, row, col)\n",
        "        Returns dLdinput.\n",
        "        t = iterative index\n",
        "        \"\"\"\n",
        "        X = self.getVolumeOutput(index-1)  # Input to the current layer (channel, row, col)\n",
        "        # Weights of the current layer (numFilter, channel, row, col)\n",
        "        W = self.weights[index - 1]\n",
        "\n",
        "        dLdX = np.empty(X.shape)\n",
        "        dLdW = np.empty(W.shape)\n",
        "\n",
        "        dLdout = np.copy(dLdoutput)\n",
        "        dLdout[dLdout < 0] = 0\n",
        "\n",
        "        # Loop over the filters\n",
        "        numFilters = W.shape[0]\n",
        "\n",
        "        for fil_ter in range(numFilters):\n",
        "            filter_output = dLdout[fil_ter]\n",
        "\n",
        "            # Loop over the channels\n",
        "            for channel in range(W.shape[1]):\n",
        "                filter_layer = W[fil_ter][channel]\n",
        "                dWLayer = self.convolve(X[channel], filter_output)\n",
        "                dXLayer = self.rotate180(self.fullConvolve(self.rotate180(filter_layer), filter_output))\n",
        "\n",
        "                # Combine these and return in arrays\n",
        "                dLdW[fil_ter][channel] = dWLayer\n",
        "                dLdX[channel] = dXLayer\n",
        "\n",
        "        return (dLdX,-self.learning_rate*dLdW)\n",
        "\n",
        "    def backPropagation(self, input, trueResults, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Updates weights by carrying out backpropagation using mini-batches.\n",
        "        trueResults = the expected output from the neural network.\n",
        "        \"\"\"\n",
        "        # This keeps track of all weight parameter changes, and make average update at end of mini-batch\n",
        "        conv_weights = [] \n",
        "        for i in range(len(self.weights) - 1):\n",
        "            if(self.track[i]=='p'):\n",
        "                conv_weights.append('p')\n",
        "            elif(self.track[i]=='c'):\n",
        "                conv_weights.append(np.zeros(self.weights[i].shape))\n",
        "        fc_weights = np.zeros(self.weights[len(self.weights) - 1].shape)\n",
        "        size = int(len(input)/mini_batch_size)\n",
        "        weights2 = copy.deepcopy(self.weights)\n",
        "        temp = copy.deepcopy(self.weights)\n",
        "        fc_weights2 = copy.deepcopy(self.weights[len(self.weights) - 1])\n",
        "        temp2 = copy.deepcopy(self.weights[len(self.weights) - 1])\n",
        "        for batch in range(size):\n",
        "            mini_inp = np.array(input[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            mini_res = np.array(trueResults[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            weights2 = copy.deepcopy(temp)\n",
        "            fc_weights2 = copy.deepcopy(temp2)\n",
        "            if (batch > 0):\n",
        "                temp = copy.deepcopy(self.weights)\n",
        "                temp2 = copy.deepcopy(self.weights[len(self.weights) - 1])\n",
        "            for i in range(len(mini_inp)):\n",
        "                self.inputImg = np.array(mini_inp[i])\n",
        "                if(len(self.inputImg.shape) < 3):\n",
        "                    a = self.inputImg\n",
        "                    self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                # Index keeping track of the previous layer\n",
        "                nPrev = len(self.weights)\n",
        "                (doutput,dLdW) = self.FCGD(nPrev, mini_res[i]) # The dLdW has the negative sign and is multiplied by the learning rate before getting passed here\n",
        "                fc_weights += dLdW/len(mini_inp)\n",
        "                nPrev -= 1\n",
        "                # Loop over the layers\n",
        "                while nPrev - 1 >= 0:\n",
        "                    if(self.track[nPrev - 1] == 'p'):\n",
        "                        dhidden = self.PoolGD(doutput, nPrev)\n",
        "                    else:\n",
        "                        (dhidden, dLdW) = self.ConvGD(doutput, nPrev, self.count)\n",
        "                        conv_weights[nPrev - 1] += dLdW/len(mini_inp)\n",
        "                        #self.weights[nPrev - 1] -= dLdW / len(mini_inp)\n",
        "                    doutput = dhidden  # Move to the previous layer\n",
        "                    nPrev -= 1\n",
        "            #print('w = ',self.weights[0],',\\t w2 = ',weights2[0])\n",
        "            for i in range(len(self.weights) - 1):\n",
        "                if (self.track[i]=='c'):\n",
        "                    self.weights[i] = self.weights[i] + conv_weights[i] + self.beta*((self.weights[i])**self.n - (weights2[i])**self.n)\n",
        "                    conv_weights[i] = np.zeros(conv_weights[i].shape)\n",
        "            self.weights[len(self.weights) - 1] = self.weights[len(self.weights) - 1] + fc_weights + self.beta*((self.weights[len(self.weights) - 1])**self.n - (fc_weights2)**self.n)\n",
        "            fc_weights = np.zeros(fc_weights.shape)\n",
        "            print('Training over mini-batch number: ',self.count, 'done.')\n",
        "            self.count = self.count + 1\n",
        "\n",
        "    def evaluate(self, x_train, y_train, x_test, y_test, epochs, mini_batch_size, test_freq):\n",
        "        \"\"\"\n",
        "        Train the neural network. And run test using test data to see progress of the model accuracy.\n",
        "        x_train, x_test = the inout training and testing data respectively.\n",
        "        y_train, y_test = the expected results from the CNN for the training and testing data respectively.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size = the size of mini-batches.\n",
        "        test_freq = Testing results should be printed after 'test_freq' number of epochs.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(x_train, y_train, mini_batch_size)\n",
        "            print('\\t Epoch Number: ', i + 1, ' done.')\n",
        "            if(i%test_freq==0):\n",
        "                print('\\t Testing score after ',i + 1,' epochs:')\n",
        "                self.accuracy(x_test, y_test)\n",
        "        print(\"\\t Training Complete.\")\n",
        "\n",
        "\n",
        "    def train(self, input, Y, epochs, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "        Y = the expected results from the neural network.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size is the size of mini-batches.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(input, Y, mini_batch_size)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "    def accuracy(self, X, Y):\n",
        "        \"\"\"\n",
        "        Function that takes in test data and results and calculates the accuracy of the Network.\n",
        "        \"\"\"\n",
        "        y = []\n",
        "        cor = 0\n",
        "        correct = 0\n",
        "        for i in range(len(X)):\n",
        "            self.inputImg = np.array(X[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                y.append(self.getVolumeOutput(len(self.weights)))\n",
        "        Y = np.array(Y)\n",
        "        y = np.array(y)\n",
        "        if (np.max(y) == 0):\n",
        "            y /= 1.0\n",
        "        else:\n",
        "            y /= np.max(y)\n",
        "        for i in range(len(Y)):\n",
        "            correct = np.argmax(Y[i])\n",
        "            if (np.argmax(y[i]) == correct):\n",
        "                cor += 1\n",
        "        cor /= len(Y)\n",
        "        print('\\t \\t Accuracy = ', cor*100, '%')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnDdydRz9L8U"
      },
      "source": [
        "# Code Run for HerBO (MBGD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXxxgdqhqng8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be897589-52ba-404b-bc6e-74d932fba2b3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd    \n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "(x_original, y_original), (x2_original, y2_original) = datasets.mnist.load_data()\n",
        "\n",
        "# See distribution of data for each class in the original testing data of MNIST database\n",
        "count = np.zeros(10)\n",
        "for i in range(len(x_original)):\n",
        "    count[y_original[i]] += 1\n",
        "count /= len(x_original)\n",
        "\n",
        "# How many examples of each do we need to maintain the same distribution of original MNIST database.\n",
        "count *= 2560\n",
        "print('Reshaped class-wise number of examples to maintain same distribution original MNIST data: ',count)\n",
        "\n",
        "# Make new training and testing data\n",
        "x = []\n",
        "y = []\n",
        "x2 = []\n",
        "y2 = []\n",
        "\n",
        "df=pd.DataFrame(y_original, columns=['y_original']) \n",
        "index = [i for i in range(0, len(df.index))]\n",
        "df['index'] = index\n",
        "df.columns=['y_original', 'Index']\n",
        "df.set_index('Index',inplace=True)\n",
        "\n",
        "# Append first n examples of each class\n",
        "# NOTE: Yes it is an easier way if we could use a double for-loop, but we purposely write it in 10 explicit for-loops so that it is easier for the reader to understand.\n",
        "for i in range(253):\n",
        "    index = (df.index[df['y_original'] == 0]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(0)\n",
        "\n",
        "for i in range(288):\n",
        "    index = (df.index[df['y_original'] == 1]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(1)\n",
        "\n",
        "for i in range(254):\n",
        "    index = (df.index[df['y_original'] == 2]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(2)\n",
        "\n",
        "for i in range(262):\n",
        "    index = (df.index[df['y_original'] == 3]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(3)\n",
        "\n",
        "for i in range(249):\n",
        "    index = (df.index[df['y_original'] == 4]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(4)\n",
        "\n",
        "for i in range(231):\n",
        "    index = (df.index[df['y_original'] == 5]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(5)\n",
        "\n",
        "for i in range(252):\n",
        "    index = (df.index[df['y_original'] == 6]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(6)\n",
        "\n",
        "for i in range(267):\n",
        "    index = (df.index[df['y_original'] == 7]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(7)\n",
        "\n",
        "for i in range(250):\n",
        "    index = (df.index[df['y_original'] == 8]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(8)\n",
        "\n",
        "for i in range(254):\n",
        "    index = (df.index[df['y_original'] == 9]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(9)\n",
        "\n",
        "\n",
        "for i in range(260):\n",
        "    x2.append(x2_original[i])\n",
        "    y2.append(y2_original[i])\n",
        "\n",
        "x= np.array(x)\n",
        "y= np.array(y)\n",
        "x2= np.array(x2)\n",
        "y2= np.array(y2)\n",
        "\n",
        "# Normalise data\n",
        "x = ((x/255) - 0.5)\n",
        "x2 = ((x2/255) - 0.5)\n",
        "\n",
        "print('Training and testing data generated, and pre-processed.')\n",
        "print('Training dataset size: 2560 images, testing dataset size: 260 images.')\n",
        "\n",
        "\n",
        "# Create instance of NeuralNetwork\n",
        "model = convnet_herbo_mbgd()\n",
        "#model = convnet_adam_mbgd()\n",
        "#model = convnet_sgd()\n",
        "#model = convnet_mbgd()\n",
        "\n",
        "X = np.random.randn(1,28,28)\n",
        "model.addInput(X) # Input layer\n",
        "model.cvolume(1,5,10) # Add Convolutional volume (stride length, receptive field, filters)\n",
        "model.pmaxvolume(2) # Add Pooling layer (receptive field)\n",
        "model.FCLayer(10)  # Add FC Layer (number of classifiers)\n",
        "# Get final output layer. \n",
        "# ========================================================================================\n",
        "# IMPORTANT: One HAS to run it once before training, so that all variables are initialised.\n",
        "# ========================================================================================\n",
        "print('Test Run Output: ',model.getVolumeOutput(3))\n",
        "\n",
        "#Since we test the CNN with MNIST data, we write the target output in the required format before sent to training/testing.\n",
        "results = np.zeros((len(y),10))\n",
        "results2 = np.zeros((len(y2),10))\n",
        "for i in range(len(y)):O\n",
        "    results[i,y[i]] = 1\n",
        "for i in range(len(y2)):\n",
        "    results2[i,y2[i]] = 1\n",
        "\n",
        "# Train model with 2560 images for 10 epochs with a mini-batch size; and then test for 260 images from the MNIST testing dataset\n",
        "model.evaluate(x, results, x2, results2, 10, 128, 1)\n",
        "\n",
        "#model.accuracy(x[2000:4000], results[2000:4000])  # Predict accuracy using test data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaped class-wise number of examples to maintain same distribution original MNIST data:  [252.71466667 287.65866667 254.208      261.58933333 249.25866667\n",
            " 231.296      252.50133333 267.30666667 249.64266667 253.824     ]\n",
            "Training and testing data generated, and pre-processed.\n",
            "Training dataset size: 2560 images, testing dataset size: 260 images.\n",
            "Test Run Output:  [0.09997204 0.09983971 0.10041394 0.10040842 0.10016026 0.09975728\n",
            " 0.10072183 0.10047065 0.09918427 0.09907158]\n",
            "Training over mini-batch number:  1 done.\n",
            "Training over mini-batch number:  2 done.\n",
            "Training over mini-batch number:  3 done.\n",
            "Training over mini-batch number:  4 done.\n",
            "Training over mini-batch number:  5 done.\n",
            "Training over mini-batch number:  6 done.\n",
            "Training over mini-batch number:  7 done.\n",
            "Training over mini-batch number:  8 done.\n",
            "Training over mini-batch number:  9 done.\n",
            "Training over mini-batch number:  10 done.\n",
            "Training over mini-batch number:  11 done.\n",
            "Training over mini-batch number:  12 done.\n",
            "Training over mini-batch number:  13 done.\n",
            "Training over mini-batch number:  14 done.\n",
            "Training over mini-batch number:  15 done.\n",
            "Training over mini-batch number:  16 done.\n",
            "Training over mini-batch number:  17 done.\n",
            "Training over mini-batch number:  18 done.\n",
            "Training over mini-batch number:  19 done.\n",
            "Training over mini-batch number:  20 done.\n",
            "\t Epoch Number:  1  done.\n",
            "\t Testing score after  1  epochs:\n",
            "\t \t Accuracy =  54.61538461538461 %\n",
            "Training over mini-batch number:  21 done.\n",
            "Training over mini-batch number:  22 done.\n",
            "Training over mini-batch number:  23 done.\n",
            "Training over mini-batch number:  24 done.\n",
            "Training over mini-batch number:  25 done.\n",
            "Training over mini-batch number:  26 done.\n",
            "Training over mini-batch number:  27 done.\n",
            "Training over mini-batch number:  28 done.\n",
            "Training over mini-batch number:  29 done.\n",
            "Training over mini-batch number:  30 done.\n",
            "Training over mini-batch number:  31 done.\n",
            "Training over mini-batch number:  32 done.\n",
            "Training over mini-batch number:  33 done.\n",
            "Training over mini-batch number:  34 done.\n",
            "Training over mini-batch number:  35 done.\n",
            "Training over mini-batch number:  36 done.\n",
            "Training over mini-batch number:  37 done.\n",
            "Training over mini-batch number:  38 done.\n",
            "Training over mini-batch number:  39 done.\n",
            "Training over mini-batch number:  40 done.\n",
            "\t Epoch Number:  2  done.\n",
            "\t Testing score after  2  epochs:\n",
            "\t \t Accuracy =  56.92307692307692 %\n",
            "Training over mini-batch number:  41 done.\n",
            "Training over mini-batch number:  42 done.\n",
            "Training over mini-batch number:  43 done.\n",
            "Training over mini-batch number:  44 done.\n",
            "Training over mini-batch number:  45 done.\n",
            "Training over mini-batch number:  46 done.\n",
            "Training over mini-batch number:  47 done.\n",
            "Training over mini-batch number:  48 done.\n",
            "Training over mini-batch number:  49 done.\n",
            "Training over mini-batch number:  50 done.\n",
            "Training over mini-batch number:  51 done.\n",
            "Training over mini-batch number:  52 done.\n",
            "Training over mini-batch number:  53 done.\n",
            "Training over mini-batch number:  54 done.\n",
            "Training over mini-batch number:  55 done.\n",
            "Training over mini-batch number:  56 done.\n",
            "Training over mini-batch number:  57 done.\n",
            "Training over mini-batch number:  58 done.\n",
            "Training over mini-batch number:  59 done.\n",
            "Training over mini-batch number:  60 done.\n",
            "\t Epoch Number:  3  done.\n",
            "\t Testing score after  3  epochs:\n",
            "\t \t Accuracy =  57.692307692307686 %\n",
            "Training over mini-batch number:  61 done.\n",
            "Training over mini-batch number:  62 done.\n",
            "Training over mini-batch number:  63 done.\n",
            "Training over mini-batch number:  64 done.\n",
            "Training over mini-batch number:  65 done.\n",
            "Training over mini-batch number:  66 done.\n",
            "Training over mini-batch number:  67 done.\n",
            "Training over mini-batch number:  68 done.\n",
            "Training over mini-batch number:  69 done.\n",
            "Training over mini-batch number:  70 done.\n",
            "Training over mini-batch number:  71 done.\n",
            "Training over mini-batch number:  72 done.\n",
            "Training over mini-batch number:  73 done.\n",
            "Training over mini-batch number:  74 done.\n",
            "Training over mini-batch number:  75 done.\n",
            "Training over mini-batch number:  76 done.\n",
            "Training over mini-batch number:  77 done.\n",
            "Training over mini-batch number:  78 done.\n",
            "Training over mini-batch number:  79 done.\n",
            "Training over mini-batch number:  80 done.\n",
            "\t Epoch Number:  4  done.\n",
            "\t Testing score after  4  epochs:\n",
            "\t \t Accuracy =  57.692307692307686 %\n",
            "Training over mini-batch number:  81 done.\n",
            "Training over mini-batch number:  82 done.\n",
            "Training over mini-batch number:  83 done.\n",
            "Training over mini-batch number:  84 done.\n",
            "Training over mini-batch number:  85 done.\n",
            "Training over mini-batch number:  86 done.\n",
            "Training over mini-batch number:  87 done.\n",
            "Training over mini-batch number:  88 done.\n",
            "Training over mini-batch number:  89 done.\n",
            "Training over mini-batch number:  90 done.\n",
            "Training over mini-batch number:  91 done.\n",
            "Training over mini-batch number:  92 done.\n",
            "Training over mini-batch number:  93 done.\n",
            "Training over mini-batch number:  94 done.\n",
            "Training over mini-batch number:  95 done.\n",
            "Training over mini-batch number:  96 done.\n",
            "Training over mini-batch number:  97 done.\n",
            "Training over mini-batch number:  98 done.\n",
            "Training over mini-batch number:  99 done.\n",
            "Training over mini-batch number:  100 done.\n",
            "\t Epoch Number:  5  done.\n",
            "\t Testing score after  5  epochs:\n",
            "\t \t Accuracy =  58.07692307692308 %\n",
            "Training over mini-batch number:  101 done.\n",
            "Training over mini-batch number:  102 done.\n",
            "Training over mini-batch number:  103 done.\n",
            "Training over mini-batch number:  104 done.\n",
            "Training over mini-batch number:  105 done.\n",
            "Training over mini-batch number:  106 done.\n",
            "Training over mini-batch number:  107 done.\n",
            "Training over mini-batch number:  108 done.\n",
            "Training over mini-batch number:  109 done.\n",
            "Training over mini-batch number:  110 done.\n",
            "Training over mini-batch number:  111 done.\n",
            "Training over mini-batch number:  112 done.\n",
            "Training over mini-batch number:  113 done.\n",
            "Training over mini-batch number:  114 done.\n",
            "Training over mini-batch number:  115 done.\n",
            "Training over mini-batch number:  116 done.\n",
            "Training over mini-batch number:  117 done.\n",
            "Training over mini-batch number:  118 done.\n",
            "Training over mini-batch number:  119 done.\n",
            "Training over mini-batch number:  120 done.\n",
            "\t Epoch Number:  6  done.\n",
            "\t Testing score after  6  epochs:\n",
            "\t \t Accuracy =  58.84615384615385 %\n",
            "Training over mini-batch number:  121 done.\n",
            "Training over mini-batch number:  122 done.\n",
            "Training over mini-batch number:  123 done.\n",
            "Training over mini-batch number:  124 done.\n",
            "Training over mini-batch number:  125 done.\n",
            "Training over mini-batch number:  126 done.\n",
            "Training over mini-batch number:  127 done.\n",
            "Training over mini-batch number:  128 done.\n",
            "Training over mini-batch number:  129 done.\n",
            "Training over mini-batch number:  130 done.\n",
            "Training over mini-batch number:  131 done.\n",
            "Training over mini-batch number:  132 done.\n",
            "Training over mini-batch number:  133 done.\n",
            "Training over mini-batch number:  134 done.\n",
            "Training over mini-batch number:  135 done.\n",
            "Training over mini-batch number:  136 done.\n",
            "Training over mini-batch number:  137 done.\n",
            "Training over mini-batch number:  138 done.\n",
            "Training over mini-batch number:  139 done.\n",
            "Training over mini-batch number:  140 done.\n",
            "\t Epoch Number:  7  done.\n",
            "\t Testing score after  7  epochs:\n",
            "\t \t Accuracy =  58.46153846153847 %\n",
            "Training over mini-batch number:  141 done.\n",
            "Training over mini-batch number:  142 done.\n",
            "Training over mini-batch number:  143 done.\n",
            "Training over mini-batch number:  144 done.\n",
            "Training over mini-batch number:  145 done.\n",
            "Training over mini-batch number:  146 done.\n",
            "Training over mini-batch number:  147 done.\n",
            "Training over mini-batch number:  148 done.\n",
            "Training over mini-batch number:  149 done.\n",
            "Training over mini-batch number:  150 done.\n",
            "Training over mini-batch number:  151 done.\n",
            "Training over mini-batch number:  152 done.\n",
            "Training over mini-batch number:  153 done.\n",
            "Training over mini-batch number:  154 done.\n",
            "Training over mini-batch number:  155 done.\n",
            "Training over mini-batch number:  156 done.\n",
            "Training over mini-batch number:  157 done.\n",
            "Training over mini-batch number:  158 done.\n",
            "Training over mini-batch number:  159 done.\n",
            "Training over mini-batch number:  160 done.\n",
            "\t Epoch Number:  8  done.\n",
            "\t Testing score after  8  epochs:\n",
            "\t \t Accuracy =  57.692307692307686 %\n",
            "Training over mini-batch number:  161 done.\n",
            "Training over mini-batch number:  162 done.\n",
            "Training over mini-batch number:  163 done.\n",
            "Training over mini-batch number:  164 done.\n",
            "Training over mini-batch number:  165 done.\n",
            "Training over mini-batch number:  166 done.\n",
            "Training over mini-batch number:  167 done.\n",
            "Training over mini-batch number:  168 done.\n",
            "Training over mini-batch number:  169 done.\n",
            "Training over mini-batch number:  170 done.\n",
            "Training over mini-batch number:  171 done.\n",
            "Training over mini-batch number:  172 done.\n",
            "Training over mini-batch number:  173 done.\n",
            "Training over mini-batch number:  174 done.\n",
            "Training over mini-batch number:  175 done.\n",
            "Training over mini-batch number:  176 done.\n",
            "Training over mini-batch number:  177 done.\n",
            "Training over mini-batch number:  178 done.\n",
            "Training over mini-batch number:  179 done.\n",
            "Training over mini-batch number:  180 done.\n",
            "\t Epoch Number:  9  done.\n",
            "\t Testing score after  9  epochs:\n",
            "\t \t Accuracy =  57.30769230769231 %\n",
            "Training over mini-batch number:  181 done.\n",
            "Training over mini-batch number:  182 done.\n",
            "Training over mini-batch number:  183 done.\n",
            "Training over mini-batch number:  184 done.\n",
            "Training over mini-batch number:  185 done.\n",
            "Training over mini-batch number:  186 done.\n",
            "Training over mini-batch number:  187 done.\n",
            "Training over mini-batch number:  188 done.\n",
            "Training over mini-batch number:  189 done.\n",
            "Training over mini-batch number:  190 done.\n",
            "Training over mini-batch number:  191 done.\n",
            "Training over mini-batch number:  192 done.\n",
            "Training over mini-batch number:  193 done.\n",
            "Training over mini-batch number:  194 done.\n",
            "Training over mini-batch number:  195 done.\n",
            "Training over mini-batch number:  196 done.\n",
            "Training over mini-batch number:  197 done.\n",
            "Training over mini-batch number:  198 done.\n",
            "Training over mini-batch number:  199 done.\n",
            "Training over mini-batch number:  200 done.\n",
            "\t Epoch Number:  10  done.\n",
            "\t Testing score after  10  epochs:\n",
            "\t \t Accuracy =  55.769230769230774 %\n",
            "\t Training Complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yynqRNV41w0"
      },
      "source": [
        "# Adam + Minibatch Gradient Descent (Size of 128)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyEgKS9Q41hn"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Ensuring reproducibsility\n",
        "np.random.seed(0)\n",
        "\n",
        "class convnet_adam_mbgd():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.inputImg = None  # The input data\n",
        "        self.strides = []  # The stride length of each layer for convolution\n",
        "        self.recfield = []  # The receptive field in each layer for convolution\n",
        "        self.lengths = []\n",
        "        self.widths = []\n",
        "        self.depths = []\n",
        "        self.weights = []  # The weights for convolution filters\n",
        "        self.node = 10  # The number of nodes in the output layer\n",
        "        self.track = []  # Keeps track of layer order, i.e Conv./Pooling/FC\n",
        "        self.learning_rate = 0.005\n",
        "        self.fc_weights = []\n",
        "        self.output_fc = []\n",
        "\n",
        "        # Adam optimization\n",
        "        self.adam_m = []\n",
        "        self.adam_v = []\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.eps = 1e-4\n",
        "        self.count = 1 # Keeps track of iteration number needed for optimisation\n",
        "\n",
        "    def addInput(self, inpImage):  # Assign the input image\n",
        "        inpImage = np.array(inpImage)\n",
        "        self.inputImg = inpImage\n",
        "        if(len(inpImage.shape) < 3):\n",
        "            num3 = 1\n",
        "            numrows = inpImage.shape[0]\n",
        "            numcols = inpImage.shape[1]\n",
        "        else:\n",
        "            num3 = inpImage.shape[0]\n",
        "            numrows = inpImage.shape[1]\n",
        "            numcols = inpImage.shape[2]\n",
        "\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "\n",
        "    def cvolume(self, s, r, f):\n",
        "        \"\"\"\n",
        "        Creates a new Conv. volume.\n",
        "        s - stride length for convolving the previous layer to create this new volume\n",
        "        r - receptive field for convolving the previous layer to create this new volume\n",
        "        f - number of filters, or in other words, the depth of this new volume\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Initializing the weights\n",
        "        W = []\n",
        "\n",
        "        # For Adam optimization\n",
        "        M = []\n",
        "        V = []\n",
        "\n",
        "        #b = np.zeros((1, stre))\n",
        "        for i in range(f):\n",
        "            W.append(np.random.randn(prevd, r, r) / (r*r))\n",
        "            M.append(np.zeros((prevd, r, r)))\n",
        "            V.append(np.zeros((prevd, r, r)))\n",
        "\n",
        "        W = np.array(W)\n",
        "        M = np.array(M)\n",
        "        V = np.array(V)\n",
        "\n",
        "        # The dimensions of the layer after convolution with the above weight array\n",
        "        numrows = (prevw - r)/s + 1\n",
        "        numcols = (prevl - r)/s + 1\n",
        "        num3 = f\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(W)\n",
        "        self.strides.append(s)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "        self.track.append('c')\n",
        "\n",
        "        self.adam_m.append(M)\n",
        "        self.adam_v.append(V)\n",
        "\n",
        "    def pmaxvolume(self, r):\n",
        "        \"\"\"\n",
        "        Creates a new max pooling layer.\n",
        "        r - the receptive field around which the max value has to be taken.\n",
        "            E.g - If r = 2, max pooling is done withing 2x2 sub matrices.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(None)\n",
        "        self.strides.append(r)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(prevl/r)\n",
        "        self.widths.append(prevw/r)\n",
        "        self.depths.append(prevd)\n",
        "        self.track.append('p')\n",
        "\n",
        "        self.adam_m.append(None)\n",
        "        self.adam_v.append(None)\n",
        "\n",
        "    def FCLayer(self, n_nodes):\n",
        "        \"\"\"\n",
        "        Creates a fully connected layer\n",
        "        n - the no.of nodes in the output layer.\n",
        "        input_fc - the input to the fully connected layer.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = int(self.depths[-1])\n",
        "        prevw = int(self.widths[-1])\n",
        "        prevl = int(self.lengths[-1])\n",
        "\n",
        "        # flatten the input\n",
        "        input_fc = np.zeros((prevd, prevw, prevl))\n",
        "        input_fc = input_fc.flatten()\n",
        "        len_input_fc = len(input_fc)\n",
        "\n",
        "        # Initialise the weights and biases for the FC layer\n",
        "        self.fc_weights = np.random.randn(len_input_fc, n_nodes) / (len_input_fc)\n",
        "        #self.fc_bias = np.zeros(n_nodes)\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(self.fc_weights)\n",
        "        self.strides.append(0)\n",
        "        self.recfield.append(0)\n",
        "        self.lengths.append(1)\n",
        "        self.widths.append(len_input_fc)\n",
        "        self.depths.append(1)\n",
        "        self.track.append('f')\n",
        "        self.node = n_nodes\n",
        "\n",
        "        self.adam_m.append(None)\n",
        "        self.adam_v.append(None)\n",
        "\n",
        "    def activFunc(self, inputArray):\n",
        "        \"\"\"\n",
        "        The activation function for the neurons in the network.\n",
        "        \"\"\"\n",
        "        # ReLU activation\n",
        "        return np.maximum(0, inputArray)\n",
        "\n",
        "    def dataLoss(self, predResults, trueResults):\n",
        "        \"\"\"\n",
        "        Returns the data loss. Cross-Entropy loss function (Softmax Classifier).\n",
        "        \"\"\"\n",
        "        # Softmax\n",
        "        loss = 0\n",
        "        sum = 0\n",
        "        for i in range(len(predResults)):\n",
        "            sum += math.exp(predResults[i])\n",
        "        correct = np.argmax(trueResults)\n",
        "        loss = (-1)*(math.log((math.exp(predResults[correct]))/sum))\n",
        "        return loss\n",
        "\n",
        "    def ConvOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Convolutional Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        if(len(W.shape) < 4):\n",
        "            f = 1\n",
        "        else:\n",
        "            f = W.shape[0]\n",
        "\n",
        "        for i in range(f):  # Run loop to create f-filters\n",
        "            for k in range(w):  # Convolve around width\n",
        "                for m in range(l):  # Convolve around length\n",
        "                    # for j in range(d):   #Run over entire depth of prevOut volume\n",
        "                    volOutput[i][k][m] += np.sum(np.multiply(W[i][:][:][:], prevOut[:, k*s: k*s + r, m*s: m*s + r])[:, :, :])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def PoolOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Pooling Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer, since there is no Weight matrix for MaxPooling, it is None\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    volOutput[j][k][m] = np.amax(prevOut[j, k*r: (k + 1)*r, m*r: (m + 1)*r])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def FCOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Implements forward pass for the FC layer. Uses a softmax Classifier.\n",
        "        n_nodes - the no.of nodes in the fully connected layer. \n",
        "        \"\"\"\n",
        "        # flatten the input\n",
        "        prevOut = prevOut.flatten()\n",
        "        #len_input_fc = len(prevOut)\n",
        "\n",
        "        totals = np.dot(prevOut, W)  # + self.fc_bias\n",
        "        # Softmax\n",
        "        exp_totals = np.exp(totals)\n",
        "\n",
        "        # Output from the FC layer\n",
        "        self.output_fc = exp_totals / (np.sum(exp_totals, axis=0))\n",
        "        return self.output_fc\n",
        "\n",
        "    def getVolumeOutput(self, n):\n",
        "        \"\"\"\n",
        "        Returns the output of the nth volume of the ConvNet.\n",
        "        \"\"\"\n",
        "        penLayer = len(self.weights) - 1  # The penultimate volume\n",
        "\n",
        "        # h stores the output of the current layer\n",
        "        h = np.array(self.inputImg)\n",
        "\n",
        "        # Loop through the hidden layers\n",
        "        for i in range(min(n, penLayer)):\n",
        "            W = self.weights[i]\n",
        "            s = self.strides[i]\n",
        "            r = self.recfield[i]\n",
        "            d = self.depths[i+1]\n",
        "            w = self.widths[i+1]\n",
        "            l = self.lengths[i+1]\n",
        "            if (self.track[i] == 'c'):\n",
        "                h = self.activFunc(self.ConvOutput(h, W, s, r, d, w, l))\n",
        "            elif (self.track[i] == 'p'):\n",
        "                h = self.PoolOutput(h, W, s, r, d, w, l)\n",
        "            else:\n",
        "                h = self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "        # Return the output\n",
        "        if n <= penLayer:\n",
        "            return h\n",
        "        else:\n",
        "            W = self.weights[n-1]\n",
        "            s = self.strides[n-1]\n",
        "            r = self.recfield[n-1]\n",
        "            d = self.depths[n]\n",
        "            w = self.widths[n]\n",
        "            l = self.lengths[n]\n",
        "            return self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "    def FCGD(self, index, trueResults):  # FC layer Gradient Descent\n",
        "        input_fc = self.getVolumeOutput(index - 1)\n",
        "\n",
        "        # Store the shape of input before flattening (to be used for backpropagation)\n",
        "        input_fc_shape = input_fc.shape\n",
        "\n",
        "        # Flatten the input\n",
        "        input_fc = input_fc.flatten()\n",
        "\n",
        "        # Get the weights and the totals of the fixed layer\n",
        "        W = self.weights[index - 1]\n",
        "        totals = np.dot(input_fc, W)\n",
        "\n",
        "        # Calculating d_L_d_out\n",
        "        correct = np.argmax(trueResults)\n",
        "        d_L_d_out = np.zeros_like(self.output_fc)\n",
        "        d_L_d_out[correct] = -1 / (self.output_fc[correct])\n",
        "\n",
        "        # Calculating d_out_d_t\n",
        "        exp_totals = np.exp(totals)\n",
        "        sum_exp_totals = np.sum(exp_totals)\n",
        "\n",
        "        d_out_d_t = np.zeros_like(self.output_fc)\n",
        "        d_out_d_t = -exp_totals[correct] * (exp_totals / (sum_exp_totals ** 2))\n",
        "        d_out_d_t[correct] = exp_totals[correct] * ((sum_exp_totals - exp_totals[correct])/(sum_exp_totals ** 2))\n",
        "\n",
        "        # Other necessary gradients\n",
        "        d_t_d_w = input_fc\n",
        "        d_t_d_inputs = W\n",
        "        d_L_d_t = d_L_d_out * d_out_d_t\n",
        "\n",
        "        # Gradients of loss wrt Weights of FC layer and Input of FC layers\n",
        "        # d_L_d_t.shape = (n_nodes,1)\n",
        "        # Adding appropriate axes to d_L_d_t and d_t_d_w(same as input_fc) for . product\n",
        "        d_L_d_w = np.dot(d_t_d_w[np.newaxis].T, d_L_d_t[np.newaxis])\n",
        "\n",
        "        # d_L_d_inputs should have the dimensions of input_fc\n",
        "        d_L_d_inputs = np.dot(d_t_d_inputs, d_L_d_t)\n",
        "\n",
        "        # The dimension of d_L_d_inputs is (len_input_fc,), so, changing the shape so it can be given to maxpool's backprop.\n",
        "        d_L_d_inputs_final = d_L_d_inputs.reshape(input_fc_shape)\n",
        "\n",
        "        #W -= self.learning_rate * d_L_d_w\n",
        "        #self.weights[index - 1] = W\n",
        "        dW = -self.learning_rate * d_L_d_w\n",
        "        return (d_L_d_inputs_final, dW)\n",
        "\n",
        "    def PoolGD(self, dLdOut, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates gradients through the MaxPooling layer\n",
        "        dLdOut is the differential of Loss wrt the Output where Output here refers to the output of the MaxPooling layer\n",
        "        This function thus finds dLdI which is the differential of Loss wrt the Input where Input here refers to input to MaxPool layer.\n",
        "        \"\"\"\n",
        "        input_vol = self.getVolumeOutput(index - 1)\n",
        "        s = self.strides[index - 1]\n",
        "        r = self.recfield[index - 1]\n",
        "        d = dLdOut.shape[0]\n",
        "        w = dLdOut.shape[1]\n",
        "        l = dLdOut.shape[2]\n",
        "\n",
        "        # Convert the numbers to int, as the for loops below will report errors if this is not done\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "\n",
        "        # Keep track of the depth and spatial indices of where the maximum element is, in the sub arrays taken for pooling\n",
        "        d_ind = []\n",
        "        spatial_ind = []\n",
        "        # Keep track of which sub array is being taken for max pooling\n",
        "        track_w = []\n",
        "        track_l = []\n",
        "\n",
        "        dLdI = np.zeros((int(self.depths[index - 1]), int(self.lengths[index - 1]), int(self.widths[index - 1])))\n",
        "        replace = dLdOut.flatten()\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    spatial_ind.append(np.where(input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r] == input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r].max()))\n",
        "                    track_l.append(m)\n",
        "                    track_w.append(k)\n",
        "                    d_ind.append(j)\n",
        "\n",
        "        # Initialise correct values in dLdI array\n",
        "        for i in range(len(replace)):\n",
        "            width = spatial_ind[i][0][0]  # Note the (width) spatial index of the maximum element of the sub array\n",
        "            width += track_w[i]*r  # Add the (width) location depending on which sub array was taken for max pooling\n",
        "            length = spatial_ind[i][1][0]  # Note the (length) spatial index of the maximum element of the sub array\n",
        "            length += track_l[i]*r  # Add the (length) location depending on which sub array was taken for max pooling\n",
        "            depth = d_ind[i]  # Note the depth index of the maximum element of the sub array\n",
        "            dLdI[depth][width][length] = replace[i]\n",
        "\n",
        "        return dLdI\n",
        "\n",
        "    # Helper functions for convBackProp()\n",
        "    def convolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the convoluted output convFilter on inputLayer.\n",
        "        Both are two dimensional matrices square matrices.\n",
        "        inputLayer - (n, n)\n",
        "        convFilter - (f, f)\n",
        "        \"\"\"\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Defining the shape of the output matrix\n",
        "        l = (n-f) + 1\n",
        "        output_matrix = np.zeros((l, l))\n",
        "        s = 1\n",
        "\n",
        "        # Convolving\n",
        "        for row in range(l):\n",
        "            for col in range(l):\n",
        "                output_matrix[row][col] = np.sum(np.multiply(inputLayer[row:row+f, col:col+f], convFilter))\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def fullConvolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the full convoluted output of convFilter on inputLayer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Creating padding for the inputLayer matrix\n",
        "        padding = f - 1\n",
        "        new_dim = n + 2*padding\n",
        "\n",
        "        padded_input = np.zeros([new_dim, new_dim])\n",
        "        padded_input[padding:new_dim - padding,padding:new_dim - padding] = inputLayer\n",
        "\n",
        "        # Now convolve padded_input with convFilter\n",
        "        output_matrix = self.convolve(padded_input, convFilter)\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def rotate180(self, matrix):\n",
        "        \"\"\"\n",
        "        Rotates matrix by 180 degrees in the plane.\n",
        "        Takes only two dimensional matrices.\n",
        "        \"\"\"\n",
        "        return np.rot90(matrix, 2)\n",
        "\n",
        "    def ConvGD(self, dLdoutput, index, t):\n",
        "        \"\"\"\n",
        "        Function that backpropagates through a convolutional layer.\n",
        "        index = index of the current layer\n",
        "        dLdoutput = Gradient of the loss function wrt the output of the current layer (channel, row, col)\n",
        "        Returns dLdinput.\n",
        "        t = iterative index\n",
        "        \"\"\"\n",
        "        X = self.getVolumeOutput(index-1)  # Input to the current layer (channel, row, col)\n",
        "        # Weights of the current layer (numFilter, channel, row, col)\n",
        "        W = self.weights[index - 1]\n",
        "        M = self.adam_m[index - 1]\n",
        "        V = self.adam_v[index - 1]\n",
        "\n",
        "        dLdX = np.empty(X.shape)\n",
        "        dLdW = np.empty(W.shape)\n",
        "\n",
        "        dLdout = np.copy(dLdoutput)\n",
        "        dLdout[dLdout < 0] = 0\n",
        "\n",
        "        # Loop over the filters\n",
        "        numFilters = W.shape[0]\n",
        "\n",
        "        for fil_ter in range(numFilters):\n",
        "            filter_output = dLdout[fil_ter]\n",
        "\n",
        "            # Loop over the channels\n",
        "            for channel in range(W.shape[1]):\n",
        "                filter_layer = W[fil_ter][channel]\n",
        "                dWLayer = self.convolve(X[channel], filter_output)\n",
        "                dXLayer = self.rotate180(self.fullConvolve(self.rotate180(filter_layer), filter_output))\n",
        "\n",
        "                # Combine these and return in arrays\n",
        "                dLdW[fil_ter][channel] = dWLayer\n",
        "                dLdX[channel] = dXLayer\n",
        "\n",
        "        # Adam optimization\n",
        "        M = self.beta1 * M + (1 - self.beta1) * dLdW\n",
        "        Mt = M / (1 - self.beta1**t)\n",
        "        V = self.beta2 * V + (1 - self.beta2) * np.square(dLdW)\n",
        "        Vt = V / (1 - self.beta2**t)\n",
        "\n",
        "        dLdW -= self.learning_rate * Mt / (np.sqrt(Vt) + self.eps)  # Note that the weights update is being multiplied with learning rate\n",
        "        #self.weights[index - 1] = W\n",
        "\n",
        "        self.adam_m[index - 1] = M\n",
        "        self.adam_v[index - 1] = V\n",
        "\n",
        "        return (dLdX,dLdW)\n",
        "\n",
        "    def backPropagation(self, input, trueResults, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Updates weights by carrying out backpropagation using mini-batches.\n",
        "        trueResults = the expected output from the neural network.\n",
        "        \"\"\"\n",
        "        # This keeps track of all weight parameter changes, and make average update at end of mini-batch\n",
        "        conv_weights = [] \n",
        "        for i in range(len(self.weights) - 1):\n",
        "            if(self.track[i]=='p'):\n",
        "                conv_weights.append('p')\n",
        "            elif(self.track[i]=='c'):\n",
        "                conv_weights.append(np.zeros(self.weights[i].shape))\n",
        "        fc_weights = np.zeros(self.weights[len(self.weights) - 1].shape)\n",
        "        size = int(len(input)/mini_batch_size)\n",
        "        for batch in range(size):\n",
        "            mini_inp = np.array(input[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            mini_res = np.array(trueResults[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            for i in range(len(mini_inp)):\n",
        "                self.inputImg = np.array(mini_inp[i])\n",
        "                if(len(self.inputImg.shape) < 3):\n",
        "                    a = self.inputImg\n",
        "                    self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                # Called once so that all weights are initialised, just in case if not done before\n",
        "                out = self.getVolumeOutput(len(self.weights))\n",
        "                # Index keeping track of the previous layer\n",
        "                nPrev = len(self.weights)\n",
        "                (doutput,dLdW) = self.FCGD(nPrev, mini_res[i]) # The dLdW has the negative sign and is multiplied by the learning rate before getting passed here\n",
        "                fc_weights += dLdW/len(mini_inp)\n",
        "                nPrev -= 1\n",
        "                \n",
        "                # Loop over the layers\n",
        "                while nPrev - 1 >= 0:\n",
        "                    if(self.track[nPrev - 1] == 'p'):\n",
        "                        dhidden = self.PoolGD(doutput, nPrev)\n",
        "                    else:\n",
        "                        (dhidden, dLdW) = self.ConvGD(doutput, nPrev, self.count)\n",
        "                        conv_weights[nPrev - 1] -= dLdW/len(mini_inp)\n",
        "                        #self.weights[nPrev - 1] -= dLdW / len(mini_inp)\n",
        "                    doutput = dhidden  # Move to the previous layer\n",
        "                    nPrev -= 1\n",
        "            for i in range(len(self.weights) - 1):\n",
        "                if(self.track[i]=='c'):\n",
        "                    self.weights[i] += conv_weights[i]\n",
        "                    conv_weights[i] = np.zeros(conv_weights[i].shape)\n",
        "            self.weights[len(self.weights) - 1] += fc_weights\n",
        "            fc_weights = np.zeros(fc_weights.shape)\n",
        "            print('Training over mini-batch number: ',self.count, 'done.')\n",
        "            self.count = self.count + 1\n",
        "\n",
        "    def evaluate(self, x_train, y_train, x_test, y_test, epochs, mini_batch_size, test_freq):\n",
        "        \"\"\"\n",
        "        Train the neural network. And run test using test data to see progress of the model accuracy.\n",
        "        x_train, x_test = the inout training and testing data respectively.\n",
        "        y_train, y_test = the expected results from the CNN for the training and testing data respectively.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size = the size of mini-batches.\n",
        "        test_freq = Testing results should be printed after 'test_freq' number of epochs.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(x_train, y_train, mini_batch_size)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "            if(i%test_freq==0):\n",
        "                print('Testing score after ',i + 1,' epochs:')\n",
        "                self.accuracy(x_test, y_test)\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "\n",
        "    def train(self, input, Y, epochs, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "        Y = the expected results from the neural network.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size is the size of mini-batches.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(input, Y, mini_batch_size)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "    def accuracy(self, X, Y):\n",
        "        \"\"\"\n",
        "        Function that takes in test data and results and calculates the accuracy of the Network.\n",
        "        \"\"\"\n",
        "        y = []\n",
        "        cor = 0\n",
        "        correct = 0\n",
        "        for i in range(len(X)):\n",
        "            self.inputImg = np.array(X[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                y.append(self.getVolumeOutput(len(self.weights)))\n",
        "        Y = np.array(Y)\n",
        "        y = np.array(y)\n",
        "        if (np.max(y) == 0):\n",
        "            y /= 1.0\n",
        "        else:\n",
        "            y /= np.max(y)\n",
        "        for i in range(len(Y)):\n",
        "            correct = np.argmax(Y[i])\n",
        "            if (np.argmax(y[i]) == correct):\n",
        "                cor += 1\n",
        "        cor /= len(Y)\n",
        "        print('Accuracy = ', cor*100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yHEkrzy41QG"
      },
      "source": [
        "# Code run for Adam + Minibatch Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0454MfAM40nA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09a1f3c-af04-41a4-f202-9f683cd8797f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd    \n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "(x_original, y_original), (x2_original, y2_original) = datasets.mnist.load_data()\n",
        "\n",
        "# See distribution of data for each class in the original testing data of MNIST database\n",
        "count = np.zeros(10)\n",
        "for i in range(len(x_original)):\n",
        "    count[y_original[i]] += 1\n",
        "count /= len(x_original)\n",
        "\n",
        "# How many examples of each do we need to maintain the same distribution of original MNIST database.\n",
        "count *= 2560\n",
        "print('Reshaped class-wise number of examples to maintain same distribution original MNIST data: ',count)\n",
        "\n",
        "# Make new training and testing data\n",
        "x = []\n",
        "y = []\n",
        "x2 = []\n",
        "y2 = []\n",
        "\n",
        "df=pd.DataFrame(y_original, columns=['y_original']) \n",
        "index = [i for i in range(0, len(df.index))]\n",
        "df['index'] = index\n",
        "df.columns=['y_original', 'Index']\n",
        "df.set_index('Index',inplace=True)\n",
        "\n",
        "# Append first n examples of each class\n",
        "# NOTE: Yes it is an easier way if we could use a double for-loop, but we purposely write it in 10 explicit for-loops so that it is easier for the reader to understand.\n",
        "for i in range(253):\n",
        "    index = (df.index[df['y_original'] == 0]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(0)\n",
        "\n",
        "for i in range(288):\n",
        "    index = (df.index[df['y_original'] == 1]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(1)\n",
        "\n",
        "for i in range(254):\n",
        "    index = (df.index[df['y_original'] == 2]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(2)\n",
        "\n",
        "for i in range(262):\n",
        "    index = (df.index[df['y_original'] == 3]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(3)\n",
        "\n",
        "for i in range(249):\n",
        "    index = (df.index[df['y_original'] == 4]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(4)\n",
        "\n",
        "for i in range(231):\n",
        "    index = (df.index[df['y_original'] == 5]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(5)\n",
        "\n",
        "for i in range(252):\n",
        "    index = (df.index[df['y_original'] == 6]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(6)\n",
        "\n",
        "for i in range(267):\n",
        "    index = (df.index[df['y_original'] == 7]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(7)\n",
        "\n",
        "for i in range(250):\n",
        "    index = (df.index[df['y_original'] == 8]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(8)\n",
        "\n",
        "for i in range(254):\n",
        "    index = (df.index[df['y_original'] == 9]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(9)\n",
        "\n",
        "\n",
        "for i in range(260):\n",
        "    x2.append(x2_original[i])\n",
        "    y2.append(y2_original[i])\n",
        "\n",
        "x= np.array(x)\n",
        "y= np.array(y)\n",
        "x2= np.array(x2)\n",
        "y2= np.array(y2)\n",
        "\n",
        "# Normalise data\n",
        "x = ((x/255) - 0.5)\n",
        "x2 = ((x2/255) - 0.5)\n",
        "\n",
        "print('Training and testing data generated, and pre-processed.')\n",
        "print('Training dataset size: 2560 images, testing dataset size: 260 images.')\n",
        "\n",
        "\n",
        "# Create instance of NeuralNetwork\n",
        "#model = convnet_herbo_mbgd()\n",
        "model = convnet_adam_mbgd()\n",
        "#model = convnet_sgd()\n",
        "#model = convnet_mbgd()\n",
        "\n",
        "X = np.random.randn(1,28,28)\n",
        "model.addInput(X) # Input layer\n",
        "model.cvolume(1,5,10) # Add Convolutional volume (stride length, receptive field, filters)\n",
        "model.pmaxvolume(2) # Add Pooling layer (receptive field)\n",
        "model.FCLayer(10)  # Add FC Layer (number of classifiers)\n",
        "# Get final output layer. \n",
        "# ========================================================================================\n",
        "# IMPORTANT: One HAS to run it once before training, so that all variables are initialised.\n",
        "# ========================================================================================\n",
        "print('Test Run Output: ',model.getVolumeOutput(3))\n",
        "\n",
        "#Since we test the CNN with MNIST data, we write the target output in the required format before sent to training/testing.\n",
        "results = np.zeros((len(y),10))\n",
        "results2 = np.zeros((len(y2),10))\n",
        "for i in range(len(y)):\n",
        "    results[i,y[i]] = 1\n",
        "for i in range(len(y2)):\n",
        "    results2[i,y2[i]] = 1\n",
        "\n",
        "# Train model with 2560 images for 10 epochs with a mini-batch size; and then test for 260 images from the MNIST testing dataset\n",
        "model.evaluate(x, results, x2, results2, 10, 128, 1)\n",
        "\n",
        "#model.accuracy(x[2000:4000], results[2000:4000])  # Predict accuracy using test data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaped class-wise number of examples to maintain same distribution original MNIST data:  [252.71466667 287.65866667 254.208      261.58933333 249.25866667\n",
            " 231.296      252.50133333 267.30666667 249.64266667 253.824     ]\n",
            "Training and testing data generated, and pre-processed.\n",
            "Training dataset size: 2560 images, testing dataset size: 260 images.\n",
            "Test Run Output:  [0.09997204 0.09983971 0.10041394 0.10040842 0.10016026 0.09975728\n",
            " 0.10072183 0.10047065 0.09918427 0.09907158]\n",
            "Training over mini-batch number:  1 done.\n",
            "Training over mini-batch number:  2 done.\n",
            "Training over mini-batch number:  3 done.\n",
            "Training over mini-batch number:  4 done.\n",
            "Training over mini-batch number:  5 done.\n",
            "Training over mini-batch number:  6 done.\n",
            "Training over mini-batch number:  7 done.\n",
            "Training over mini-batch number:  8 done.\n",
            "Training over mini-batch number:  9 done.\n",
            "Training over mini-batch number:  10 done.\n",
            "Training over mini-batch number:  11 done.\n",
            "Training over mini-batch number:  12 done.\n",
            "Training over mini-batch number:  13 done.\n",
            "Training over mini-batch number:  14 done.\n",
            "Training over mini-batch number:  15 done.\n",
            "Training over mini-batch number:  16 done.\n",
            "Training over mini-batch number:  17 done.\n",
            "Training over mini-batch number:  18 done.\n",
            "Training over mini-batch number:  19 done.\n",
            "Training over mini-batch number:  20 done.\n",
            "Epoch Number:  1  done.\n",
            "Testing score after  1  epochs:\n",
            "Accuracy =  23.076923076923077 %\n",
            "Training over mini-batch number:  21 done.\n",
            "Training over mini-batch number:  22 done.\n",
            "Training over mini-batch number:  23 done.\n",
            "Training over mini-batch number:  24 done.\n",
            "Training over mini-batch number:  25 done.\n",
            "Training over mini-batch number:  26 done.\n",
            "Training over mini-batch number:  27 done.\n",
            "Training over mini-batch number:  28 done.\n",
            "Training over mini-batch number:  29 done.\n",
            "Training over mini-batch number:  30 done.\n",
            "Training over mini-batch number:  31 done.\n",
            "Training over mini-batch number:  32 done.\n",
            "Training over mini-batch number:  33 done.\n",
            "Training over mini-batch number:  34 done.\n",
            "Training over mini-batch number:  35 done.\n",
            "Training over mini-batch number:  36 done.\n",
            "Training over mini-batch number:  37 done.\n",
            "Training over mini-batch number:  38 done.\n",
            "Training over mini-batch number:  39 done.\n",
            "Training over mini-batch number:  40 done.\n",
            "Epoch Number:  2  done.\n",
            "Testing score after  2  epochs:\n",
            "Accuracy =  28.076923076923077 %\n",
            "Training over mini-batch number:  41 done.\n",
            "Training over mini-batch number:  42 done.\n",
            "Training over mini-batch number:  43 done.\n",
            "Training over mini-batch number:  44 done.\n",
            "Training over mini-batch number:  45 done.\n",
            "Training over mini-batch number:  46 done.\n",
            "Training over mini-batch number:  47 done.\n",
            "Training over mini-batch number:  48 done.\n",
            "Training over mini-batch number:  49 done.\n",
            "Training over mini-batch number:  50 done.\n",
            "Training over mini-batch number:  51 done.\n",
            "Training over mini-batch number:  52 done.\n",
            "Training over mini-batch number:  53 done.\n",
            "Training over mini-batch number:  54 done.\n",
            "Training over mini-batch number:  55 done.\n",
            "Training over mini-batch number:  56 done.\n",
            "Training over mini-batch number:  57 done.\n",
            "Training over mini-batch number:  58 done.\n",
            "Training over mini-batch number:  59 done.\n",
            "Training over mini-batch number:  60 done.\n",
            "Epoch Number:  3  done.\n",
            "Testing score after  3  epochs:\n",
            "Accuracy =  35.38461538461539 %\n",
            "Training over mini-batch number:  61 done.\n",
            "Training over mini-batch number:  62 done.\n",
            "Training over mini-batch number:  63 done.\n",
            "Training over mini-batch number:  64 done.\n",
            "Training over mini-batch number:  65 done.\n",
            "Training over mini-batch number:  66 done.\n",
            "Training over mini-batch number:  67 done.\n",
            "Training over mini-batch number:  68 done.\n",
            "Training over mini-batch number:  69 done.\n",
            "Training over mini-batch number:  70 done.\n",
            "Training over mini-batch number:  71 done.\n",
            "Training over mini-batch number:  72 done.\n",
            "Training over mini-batch number:  73 done.\n",
            "Training over mini-batch number:  74 done.\n",
            "Training over mini-batch number:  75 done.\n",
            "Training over mini-batch number:  76 done.\n",
            "Training over mini-batch number:  77 done.\n",
            "Training over mini-batch number:  78 done.\n",
            "Training over mini-batch number:  79 done.\n",
            "Training over mini-batch number:  80 done.\n",
            "Epoch Number:  4  done.\n",
            "Testing score after  4  epochs:\n",
            "Accuracy =  38.07692307692307 %\n",
            "Training over mini-batch number:  81 done.\n",
            "Training over mini-batch number:  82 done.\n",
            "Training over mini-batch number:  83 done.\n",
            "Training over mini-batch number:  84 done.\n",
            "Training over mini-batch number:  85 done.\n",
            "Training over mini-batch number:  86 done.\n",
            "Training over mini-batch number:  87 done.\n",
            "Training over mini-batch number:  88 done.\n",
            "Training over mini-batch number:  89 done.\n",
            "Training over mini-batch number:  90 done.\n",
            "Training over mini-batch number:  91 done.\n",
            "Training over mini-batch number:  92 done.\n",
            "Training over mini-batch number:  93 done.\n",
            "Training over mini-batch number:  94 done.\n",
            "Training over mini-batch number:  95 done.\n",
            "Training over mini-batch number:  96 done.\n",
            "Training over mini-batch number:  97 done.\n",
            "Training over mini-batch number:  98 done.\n",
            "Training over mini-batch number:  99 done.\n",
            "Training over mini-batch number:  100 done.\n",
            "Epoch Number:  5  done.\n",
            "Testing score after  5  epochs:\n",
            "Accuracy =  41.92307692307693 %\n",
            "Training over mini-batch number:  101 done.\n",
            "Training over mini-batch number:  102 done.\n",
            "Training over mini-batch number:  103 done.\n",
            "Training over mini-batch number:  104 done.\n",
            "Training over mini-batch number:  105 done.\n",
            "Training over mini-batch number:  106 done.\n",
            "Training over mini-batch number:  107 done.\n",
            "Training over mini-batch number:  108 done.\n",
            "Training over mini-batch number:  109 done.\n",
            "Training over mini-batch number:  110 done.\n",
            "Training over mini-batch number:  111 done.\n",
            "Training over mini-batch number:  112 done.\n",
            "Training over mini-batch number:  113 done.\n",
            "Training over mini-batch number:  114 done.\n",
            "Training over mini-batch number:  115 done.\n",
            "Training over mini-batch number:  116 done.\n",
            "Training over mini-batch number:  117 done.\n",
            "Training over mini-batch number:  118 done.\n",
            "Training over mini-batch number:  119 done.\n",
            "Training over mini-batch number:  120 done.\n",
            "Epoch Number:  6  done.\n",
            "Testing score after  6  epochs:\n",
            "Accuracy =  46.15384615384615 %\n",
            "Training over mini-batch number:  121 done.\n",
            "Training over mini-batch number:  122 done.\n",
            "Training over mini-batch number:  123 done.\n",
            "Training over mini-batch number:  124 done.\n",
            "Training over mini-batch number:  125 done.\n",
            "Training over mini-batch number:  126 done.\n",
            "Training over mini-batch number:  127 done.\n",
            "Training over mini-batch number:  128 done.\n",
            "Training over mini-batch number:  129 done.\n",
            "Training over mini-batch number:  130 done.\n",
            "Training over mini-batch number:  131 done.\n",
            "Training over mini-batch number:  132 done.\n",
            "Training over mini-batch number:  133 done.\n",
            "Training over mini-batch number:  134 done.\n",
            "Training over mini-batch number:  135 done.\n",
            "Training over mini-batch number:  136 done.\n",
            "Training over mini-batch number:  137 done.\n",
            "Training over mini-batch number:  138 done.\n",
            "Training over mini-batch number:  139 done.\n",
            "Training over mini-batch number:  140 done.\n",
            "Epoch Number:  7  done.\n",
            "Testing score after  7  epochs:\n",
            "Accuracy =  48.84615384615385 %\n",
            "Training over mini-batch number:  141 done.\n",
            "Training over mini-batch number:  142 done.\n",
            "Training over mini-batch number:  143 done.\n",
            "Training over mini-batch number:  144 done.\n",
            "Training over mini-batch number:  145 done.\n",
            "Training over mini-batch number:  146 done.\n",
            "Training over mini-batch number:  147 done.\n",
            "Training over mini-batch number:  148 done.\n",
            "Training over mini-batch number:  149 done.\n",
            "Training over mini-batch number:  150 done.\n",
            "Training over mini-batch number:  151 done.\n",
            "Training over mini-batch number:  152 done.\n",
            "Training over mini-batch number:  153 done.\n",
            "Training over mini-batch number:  154 done.\n",
            "Training over mini-batch number:  155 done.\n",
            "Training over mini-batch number:  156 done.\n",
            "Training over mini-batch number:  157 done.\n",
            "Training over mini-batch number:  158 done.\n",
            "Training over mini-batch number:  159 done.\n",
            "Training over mini-batch number:  160 done.\n",
            "Epoch Number:  8  done.\n",
            "Testing score after  8  epochs:\n",
            "Accuracy =  50.0 %\n",
            "Training over mini-batch number:  161 done.\n",
            "Training over mini-batch number:  162 done.\n",
            "Training over mini-batch number:  163 done.\n",
            "Training over mini-batch number:  164 done.\n",
            "Training over mini-batch number:  165 done.\n",
            "Training over mini-batch number:  166 done.\n",
            "Training over mini-batch number:  167 done.\n",
            "Training over mini-batch number:  168 done.\n",
            "Training over mini-batch number:  169 done.\n",
            "Training over mini-batch number:  170 done.\n",
            "Training over mini-batch number:  171 done.\n",
            "Training over mini-batch number:  172 done.\n",
            "Training over mini-batch number:  173 done.\n",
            "Training over mini-batch number:  174 done.\n",
            "Training over mini-batch number:  175 done.\n",
            "Training over mini-batch number:  176 done.\n",
            "Training over mini-batch number:  177 done.\n",
            "Training over mini-batch number:  178 done.\n",
            "Training over mini-batch number:  179 done.\n",
            "Training over mini-batch number:  180 done.\n",
            "Epoch Number:  9  done.\n",
            "Testing score after  9  epochs:\n",
            "Accuracy =  51.53846153846153 %\n",
            "Training over mini-batch number:  181 done.\n",
            "Training over mini-batch number:  182 done.\n",
            "Training over mini-batch number:  183 done.\n",
            "Training over mini-batch number:  184 done.\n",
            "Training over mini-batch number:  185 done.\n",
            "Training over mini-batch number:  186 done.\n",
            "Training over mini-batch number:  187 done.\n",
            "Training over mini-batch number:  188 done.\n",
            "Training over mini-batch number:  189 done.\n",
            "Training over mini-batch number:  190 done.\n",
            "Training over mini-batch number:  191 done.\n",
            "Training over mini-batch number:  192 done.\n",
            "Training over mini-batch number:  193 done.\n",
            "Training over mini-batch number:  194 done.\n",
            "Training over mini-batch number:  195 done.\n",
            "Training over mini-batch number:  196 done.\n",
            "Training over mini-batch number:  197 done.\n",
            "Training over mini-batch number:  198 done.\n",
            "Training over mini-batch number:  199 done.\n",
            "Training over mini-batch number:  200 done.\n",
            "Epoch Number:  10  done.\n",
            "Testing score after  10  epochs:\n",
            "Accuracy =  52.69230769230769 %\n",
            "Training Complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cdZEcyqisAO"
      },
      "source": [
        "# SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf9wLuEnirl_"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "class convnet_sgd():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.inputImg = None  # The input data\n",
        "        self.strides = []  # The stride length of each layer for convolution\n",
        "        self.recfield = []  # The receptive field in each layer for convolution\n",
        "        self.lengths = []\n",
        "        self.widths = []\n",
        "        self.depths = []\n",
        "        self.weights = []  # The weights for convolution filters\n",
        "        self.node = 10  # The number of nodes in the output layer\n",
        "        self.track = []  # Keeps track of layer order, i.e Conv./Pooling/FC\n",
        "        self.learning_rate = 0.005\n",
        "        self.fc_weights = []\n",
        "        self.output_fc = []\n",
        "\n",
        "    def addInput(self, inpImage):  # Assign the input image\n",
        "        inpImage = np.array(inpImage)\n",
        "        self.inputImg = inpImage\n",
        "        if(len(inpImage.shape) < 3):\n",
        "            num3 = 1\n",
        "            numrows = inpImage.shape[0]\n",
        "            numcols = inpImage.shape[1]\n",
        "        else:\n",
        "            num3 = inpImage.shape[0]\n",
        "            numrows = inpImage.shape[1]\n",
        "            numcols = inpImage.shape[2]\n",
        "\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "\n",
        "    def cvolume(self, s, r, f):\n",
        "        \"\"\"\n",
        "        Creates a new Conv. volume.\n",
        "        s - stride length for convolving the previous layer to create this new volume\n",
        "        r - receptive field for convolving the previous layer to create this new volume\n",
        "        f - number of filters, or in other words, the depth of this new volume\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Initializing the weights\n",
        "        W = []\n",
        "        #b = np.zeros((1, stre))\n",
        "        for i in range(f):\n",
        "            W.append(np.random.randn(prevd, r, r) / (r*r))\n",
        "\n",
        "        W = np.array(W)\n",
        "\n",
        "        # The dimensions of the layer after convolution with the above weight array\n",
        "        numrows = (prevw - r)/s + 1\n",
        "        numcols = (prevl - r)/s + 1\n",
        "        num3 = f\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(W)\n",
        "        self.strides.append(s)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "        self.track.append('c')\n",
        "\n",
        "    def pmaxvolume(self, r):\n",
        "        \"\"\"\n",
        "        Creates a new max pooling layer.\n",
        "        r - the receptive field around which the max value has to be taken.\n",
        "            E.g - If r = 2, max pooling is done withing 2x2 sub matrices.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(None)\n",
        "        self.strides.append(r)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(prevl/r)\n",
        "        self.widths.append(prevw/r)\n",
        "        self.depths.append(prevd)\n",
        "        self.track.append('p')\n",
        "\n",
        "    def FCLayer(self, n_nodes):\n",
        "        \"\"\"\n",
        "        Creates a fully connected layer\n",
        "        n - the no.of nodes in the output layer.\n",
        "        input_fc - the input to the fully connected layer.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = int(self.depths[-1])\n",
        "        prevw = int(self.widths[-1])\n",
        "        prevl = int(self.lengths[-1])\n",
        "\n",
        "        # flatten the input\n",
        "        input_fc = np.zeros((prevd, prevw, prevl))\n",
        "        input_fc = input_fc.flatten()\n",
        "        len_input_fc = len(input_fc)\n",
        "\n",
        "        # Initialise the weights and biases for the FC layer\n",
        "        self.fc_weights = np.random.randn(len_input_fc, n_nodes) / (len_input_fc)\n",
        "        #self.fc_bias = np.zeros(n_nodes)\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(self.fc_weights)\n",
        "        self.strides.append(0)\n",
        "        self.recfield.append(0)\n",
        "        self.lengths.append(1)\n",
        "        self.widths.append(len_input_fc)\n",
        "        self.depths.append(1)\n",
        "        self.track.append('f')\n",
        "        self.node = n_nodes\n",
        "\n",
        "    def activFunc(self, inputArray):\n",
        "        \"\"\"\n",
        "        The activation function for the neurons in the network.\n",
        "        \"\"\"\n",
        "        # ReLU activation\n",
        "        \n",
        "        return np.maximum(0, inputArray)\n",
        "\n",
        "    def dataLoss(self, predResults, trueResults):\n",
        "        \"\"\"\n",
        "        Returns the data loss. Cross-Entropy loss function (Softmax Classifier).\n",
        "        \"\"\"\n",
        "        # L2 loss\n",
        "        loss = 0\n",
        "        sum = 0\n",
        "        for i in range(len(predResults)):\n",
        "            sum += math.exp(predResults[i])\n",
        "        correct = np.argmax(trueResults)\n",
        "        loss = (-1)*(math.log((math.exp(predResults[correct]))/sum))\n",
        "        return loss\n",
        "\n",
        "    def ConvOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Convolutional Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        if(len(W.shape) < 4):\n",
        "            f = 1\n",
        "        else:\n",
        "            f = W.shape[0]\n",
        "\n",
        "        for i in range(f):  # Run loop to create f-filters\n",
        "            for k in range(w):  # Convolve around width\n",
        "                for m in range(l):  # Convolve around length\n",
        "                    # for j in range(d):   #Run over entire depth of prevOut volume\n",
        "                    volOutput[i][k][m] += np.sum(np.multiply(W[i][:][:][:], prevOut[:, k*s: k*s + r, m*s: m*s + r])[:, :, :])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def PoolOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Pooling Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer, since there is no Weight matrix for MaxPooling, it is None\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    volOutput[j][k][m] = np.amax(prevOut[j, k*r: (k + 1)*r, m*r: (m + 1)*r])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def FCOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Implements forward pass for the FC layer. Uses a softmax Classifier.\n",
        "        n_nodes - the no.of nodes in the fully connected layer. \n",
        "        \"\"\"\n",
        "        # flatten the input\n",
        "        prevOut = prevOut.flatten()\n",
        "        #len_input_fc = len(prevOut)\n",
        "\n",
        "        totals = np.dot(prevOut, W)  # + self.fc_bias\n",
        "        # Softmax\n",
        "        exp_totals = np.exp(totals)\n",
        "\n",
        "        # Output from the FC layer\n",
        "        self.output_fc = exp_totals / (np.sum(exp_totals, axis=0))\n",
        "        \n",
        "        return self.output_fc\n",
        "\n",
        "    def getVolumeOutput(self, n):\n",
        "        \"\"\"\n",
        "        Returns the output of the nth volume of the ConvNet.\n",
        "        \"\"\"\n",
        "        penLayer = len(self.weights) - 1  # The penultimate volume\n",
        "\n",
        "        # h stores the output of the current layer\n",
        "        h = np.array(self.inputImg)\n",
        "\n",
        "        # Loop through the hidden layers\n",
        "        for i in range(min(n, penLayer)):\n",
        "            W = self.weights[i]\n",
        "            s = self.strides[i]\n",
        "            r = self.recfield[i]\n",
        "            d = self.depths[i+1]\n",
        "            w = self.widths[i+1]\n",
        "            l = self.lengths[i+1]\n",
        "            if (self.track[i] == 'c'):\n",
        "                h = self.activFunc(self.ConvOutput(h, W, s, r, d, w, l))\n",
        "            elif (self.track[i] == 'p'):\n",
        "                h = self.PoolOutput(h, W, s, r, d, w, l)\n",
        "            else:\n",
        "                h = self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "        # Return the output\n",
        "        if n <= penLayer:\n",
        "            return h\n",
        "        else:\n",
        "            W = self.weights[n-1]\n",
        "            s = self.strides[n-1]\n",
        "            r = self.recfield[n-1]\n",
        "            d = self.depths[n]\n",
        "            w = self.widths[n]\n",
        "            l = self.lengths[n]\n",
        "            return self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "    def FCGD(self, index, trueResults):  # FC layer Gradient Descent\n",
        "        input_fc = self.getVolumeOutput(index - 1)\n",
        "\n",
        "        # Store the shape of input before flattening (to be used for backpropagation)\n",
        "        input_fc_shape = input_fc.shape\n",
        "\n",
        "        # Flatten the input\n",
        "        input_fc = input_fc.flatten()\n",
        "        \n",
        "        # Get the weights and the totals of the fixed layer\n",
        "        W = self.weights[index - 1]\n",
        "        totals = np.dot(input_fc, W)\n",
        "        \n",
        "        # Calculating d_L_d_out\n",
        "        correct = np.argmax(trueResults)\n",
        "        d_L_d_out = np.zeros_like(self.output_fc)\n",
        "        d_L_d_out[correct] = -1 / (self.output_fc[correct])\n",
        "        \n",
        "        # Calculating d_out_d_t\n",
        "        exp_totals = np.exp(totals)\n",
        "        sum_exp_totals = np.sum(exp_totals)\n",
        "        \n",
        "        d_out_d_t = np.zeros_like(self.output_fc)\n",
        "        d_out_d_t = -exp_totals[correct] * (exp_totals / (sum_exp_totals ** 2))\n",
        "        d_out_d_t[correct] = exp_totals[correct] * ((sum_exp_totals - exp_totals[correct])/(sum_exp_totals ** 2))\n",
        "                \n",
        "        # Other necessary gradients\n",
        "        d_t_d_w = input_fc\n",
        "        d_t_d_inputs = W\n",
        "        d_L_d_t = d_L_d_out * d_out_d_t\n",
        "\n",
        "        # Gradients of loss wrt Weights of FC layer and Input of FC layers\n",
        "        # d_L_d_t.shape = (n_nodes,1)\n",
        "        # Adding appropriate axes to d_L_d_t and d_t_d_w(same as input_fc) for . product\n",
        "        d_L_d_w = np.dot(d_t_d_w[np.newaxis].T, d_L_d_t[np.newaxis])\n",
        "\n",
        "        # d_L_d_inputs should have the dimensions of input_fc\n",
        "        d_L_d_inputs = np.dot(d_t_d_inputs, d_L_d_t)\n",
        "\n",
        "        # The dimension of d_L_d_inputs is (len_input_fc,), so, changing the shape so it can be given to maxpool's backprop.\n",
        "        d_L_d_inputs_final = d_L_d_inputs.reshape(input_fc_shape)\n",
        "\n",
        "        W -= self.learning_rate * d_L_d_w\n",
        "        self.weights[index - 1] = W\n",
        "        \n",
        "        return d_L_d_inputs_final\n",
        "\n",
        "    def PoolGD(self, dLdOut, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates gradients through the MaxPooling layer\n",
        "        dLdOut is the differential of Loss wrt the Output where Output here refers to the output of the MaxPooling layer\n",
        "        This function thus finds dLdI which is the differential of Loss wrt the Input where Input here refers to input to MaxPool layer.\n",
        "        \"\"\"\n",
        "        input_vol = self.getVolumeOutput(index - 1)\n",
        "        s = self.strides[index - 1]\n",
        "        r = self.recfield[index - 1]\n",
        "        d = dLdOut.shape[0]\n",
        "        w = dLdOut.shape[1]\n",
        "        l = dLdOut.shape[2]\n",
        "\n",
        "        # Convert the numbers to int, as the for loops below will report errors if this is not done\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "\n",
        "        # Keep track of the depth and spatial indices of where the maximum element is, in the sub arrays taken for pooling\n",
        "        d_ind = []\n",
        "        spatial_ind = []\n",
        "        # Keep track of which sub array is being taken for max pooling\n",
        "        track_w = []\n",
        "        track_l = []\n",
        "\n",
        "        dLdI = np.zeros((int(self.depths[index - 1]), int(self.lengths[index - 1]), int(self.widths[index - 1])))\n",
        "        replace = dLdOut.flatten()\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    spatial_ind.append(np.where(input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r] == input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r].max()))\n",
        "                    track_l.append(m)\n",
        "                    track_w.append(k)\n",
        "                    d_ind.append(j)\n",
        "\n",
        "        # Initialise correct values in dLdI array\n",
        "        for i in range(len(replace)):\n",
        "            width = spatial_ind[i][0][0]  # Note the (width) spatial index of the maximum element of the sub array\n",
        "            width += track_w[i]*r  # Add the (width) location depending on which sub array was taken for max pooling\n",
        "            length = spatial_ind[i][1][0]  # Note the (length) spatial index of the maximum element of the sub array\n",
        "            length += track_l[i]*r  # Add the (length) location depending on which sub array was taken for max pooling\n",
        "            depth = d_ind[i]  # Note the depth index of the maximum element of the sub array\n",
        "            dLdI[depth][width][length] = replace[i]\n",
        "        \n",
        "        return dLdI\n",
        "\n",
        "    # Helper functions for convBackProp()\n",
        "    def convolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the convoluted output convFilter on inputLayer.\n",
        "        Both are two dimensional matrices square matrices.\n",
        "        inputLayer - (n, n)\n",
        "        convFilter - (f, f)\n",
        "        \"\"\"\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Defining the shape of the output matrix\n",
        "        l = (n-f) + 1\n",
        "        output_matrix = np.zeros((l, l))\n",
        "        s = 1\n",
        "\n",
        "        # Convolving\n",
        "        for row in range(l):\n",
        "            for col in range(l):\n",
        "                output_matrix[row][col] = np.sum(np.multiply(inputLayer[row:row+f, col:col+f], convFilter))\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def fullConvolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the full convoluted output of convFilter on inputLayer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Creating padding for the inputLayer matrix\n",
        "        padding = f - 1\n",
        "        new_dim = n + 2*padding\n",
        "\n",
        "        padded_input = np.zeros([new_dim, new_dim])\n",
        "        padded_input[padding:new_dim - padding,padding:new_dim - padding] = inputLayer\n",
        "\n",
        "        # Now convolve padded_input with convFilter\n",
        "        output_matrix = self.convolve(padded_input, convFilter)\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def rotate180(self, matrix):\n",
        "        \"\"\"\n",
        "        Rotates matrix by 180 degrees in the plane.\n",
        "        Takes only two dimensional matrices.\n",
        "        \"\"\"\n",
        "        return np.rot90(matrix, 2)\n",
        "\n",
        "    def ConvGD(self, dLdoutput, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates through a convolutional layer.\n",
        "        index = index of the current layer\n",
        "        dLdoutput = Gradient of the loss function wrt the output of the current layer (channel, row, col)\n",
        "        Returns dLdinput.\n",
        "        \"\"\"\n",
        "        X = self.getVolumeOutput(index-1)  # Input to the current layer (channel, row, col)\n",
        "        # Weights of the current layer (numFilter, channel, row, col)\n",
        "        W = self.weights[index - 1]\n",
        "\n",
        "        dLdX = np.empty(X.shape)\n",
        "        dLdW = np.empty(W.shape)\n",
        "\n",
        "        dLdout = np.copy(dLdoutput)\n",
        "        dLdout[dLdout < 0] = 0\n",
        "\n",
        "        # Loop over the filters\n",
        "        numFilters = W.shape[0]\n",
        "\n",
        "        for fil_ter in range(numFilters):\n",
        "            filter_output = dLdout[fil_ter]\n",
        "\n",
        "            # Loop over the channels\n",
        "            for channel in range(W.shape[1]):\n",
        "                filter_layer = W[fil_ter][channel]\n",
        "                dWLayer = self.convolve(X[channel], filter_output)\n",
        "                dXLayer = self.rotate180(self.fullConvolve(self.rotate180(filter_layer), filter_output))\n",
        "\n",
        "                # Combine these and return in arrays\n",
        "                dLdW[fil_ter][channel] = dWLayer\n",
        "                dLdX[channel] = dXLayer\n",
        "\n",
        "        W -= self.learning_rate * dLdW\n",
        "        self.weights[index - 1] = W\n",
        "        return dLdX\n",
        "\n",
        "    def backPropagation(self, input, trueResults):\n",
        "        \"\"\"\n",
        "        Updates weights by carrying out backpropagation.\n",
        "        trueResults = the expected output from the neural network.\n",
        "        \"\"\"\n",
        "        for i in range(len(input)):\n",
        "            self.inputImg = np.array(input[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "            # Called once so that all weights are initialised, just in case if not done before\n",
        "            out = self.getVolumeOutput(len(self.weights))\n",
        "            # Index keeping track of the previous layer\n",
        "            nPrev = len(self.weights)\n",
        "            doutput = self.FCGD(nPrev, trueResults[i])\n",
        "            nPrev -= 1\n",
        "\n",
        "            # Loop over the layers\n",
        "            while nPrev - 1 >= 0:\n",
        "                if(self.track[nPrev - 1] == 'p'):\n",
        "                    dhidden = self.PoolGD(doutput, nPrev)\n",
        "                else:\n",
        "                    dhidden = self.ConvGD(doutput, nPrev)\n",
        "                doutput = dhidden  # Move to the previous layer\n",
        "                nPrev -= 1\n",
        "\n",
        "    def train(self, input, Y, epochs):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "        Y = the expected results from the neural network.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(input, Y)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "    def evaluate(self, x_train, y_train, x_test, y_test, epochs, test_freq):\n",
        "        \"\"\"\n",
        "        Train the neural network. And run test using test data to see progress of the model accuracy.\n",
        "        x_train, x_test = the inout training and testing data respectively.\n",
        "        y_train, y_test = the expected results from the CNN for the training and testing data respectively.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        test_freq = Testing results should be printed after 'test_freq' number of epochs.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(x_train, y_train)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "            if(i%test_freq==0):\n",
        "                print('Testing score after ',i + 1,' epochs:')\n",
        "                self.accuracy(x_test, y_test)\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "\n",
        "    def accuracy(self, X, Y):\n",
        "        \"\"\"\n",
        "        Function that takes in test data and results and calculates the accuracy of the Network.\n",
        "        \"\"\"\n",
        "        y = []\n",
        "        cor = 0\n",
        "        correct = 0\n",
        "        for i in range(len(X)):\n",
        "            self.inputImg = np.array(X[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                y.append(self.getVolumeOutput(len(self.weights)))\n",
        "        Y = np.array(Y)\n",
        "        y = np.array(y)\n",
        "        if (np.max(y) == 0):\n",
        "            y /= 1.0\n",
        "        else:\n",
        "            y /= np.max(y)\n",
        "        for i in range(len(Y)):\n",
        "            correct = np.argmax(Y[i])\n",
        "            if (np.argmax(y[i]) == correct):\n",
        "                cor += 1\n",
        "        cor /= len(Y)\n",
        "        print('Accuracy = ', cor*100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDMgYlVrixod"
      },
      "source": [
        "# Code Run for SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LQi7bZjiznD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42522fb8-f246-430b-d3ad-7dd14bc744ad"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd    \n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "(x_original, y_original), (x2_original, y2_original) = datasets.mnist.load_data()\n",
        "\n",
        "# See distribution of data for each class in the original testing data of MNIST database\n",
        "count = np.zeros(10)\n",
        "for i in range(len(x_original)):\n",
        "    count[y_original[i]] += 1\n",
        "count /= len(x_original)\n",
        "\n",
        "# How many examples of each do we need to maintain the same distribution of original MNIST database.\n",
        "count *= 2560\n",
        "print('Reshaped class-wise number of examples to maintain same distribution original MNIST data: ',count)\n",
        "\n",
        "# Make new training and testing data\n",
        "x = []\n",
        "y = []\n",
        "x2 = []\n",
        "y2 = []\n",
        "\n",
        "df=pd.DataFrame(y_original, columns=['y_original']) \n",
        "index = [i for i in range(0, len(df.index))]\n",
        "df['index'] = index\n",
        "df.columns=['y_original', 'Index']\n",
        "df.set_index('Index',inplace=True)\n",
        "\n",
        "# Append first n examples of each class\n",
        "# NOTE: Yes it is an easier way if we could use a double for-loop, but we purposely write it in 10 explicit for-loops so that it is easier for the reader to understand.\n",
        "for i in range(253):\n",
        "    index = (df.index[df['y_original'] == 0]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(0)\n",
        "\n",
        "for i in range(288):\n",
        "    index = (df.index[df['y_original'] == 1]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(1)\n",
        "\n",
        "for i in range(254):\n",
        "    index = (df.index[df['y_original'] == 2]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(2)\n",
        "\n",
        "for i in range(262):\n",
        "    index = (df.index[df['y_original'] == 3]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(3)\n",
        "\n",
        "for i in range(249):\n",
        "    index = (df.index[df['y_original'] == 4]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(4)\n",
        "\n",
        "for i in range(231):\n",
        "    index = (df.index[df['y_original'] == 5]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(5)\n",
        "\n",
        "for i in range(252):\n",
        "    index = (df.index[df['y_original'] == 6]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(6)\n",
        "\n",
        "for i in range(267):\n",
        "    index = (df.index[df['y_original'] == 7]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(7)\n",
        "\n",
        "for i in range(250):\n",
        "    index = (df.index[df['y_original'] == 8]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(8)\n",
        "\n",
        "for i in range(254):\n",
        "    index = (df.index[df['y_original'] == 9]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(9)\n",
        "\n",
        "\n",
        "for i in range(260):\n",
        "    x2.append(x2_original[i])\n",
        "    y2.append(y2_original[i])\n",
        "\n",
        "x= np.array(x)\n",
        "y= np.array(y)\n",
        "x2= np.array(x2)\n",
        "y2= np.array(y2)\n",
        "\n",
        "# Normalise data\n",
        "x = ((x/255) - 0.5)\n",
        "x2 = ((x2/255) - 0.5)\n",
        "\n",
        "print('Training and testing data generated, and pre-processed.')\n",
        "print('Training dataset size: 2560 images, testing dataset size: 260 images.')\n",
        "\n",
        "\n",
        "# Create instance of NeuralNetwork\n",
        "#model = convnet_herbo_mbgd()\n",
        "#model = convnet_adam_mbgd()\n",
        "model = convnet_sgd()\n",
        "#model = convnet_mbgd()\n",
        "\n",
        "X = np.random.randn(1,28,28)\n",
        "model.addInput(X) # Input layer\n",
        "model.cvolume(1,5,10) # Add Convolutional volume (stride length, receptive field, filters)\n",
        "model.pmaxvolume(2) # Add Pooling layer (receptive field)\n",
        "model.FCLayer(10)  # Add FC Layer (number of classifiers)\n",
        "# Get final output layer. \n",
        "# ========================================================================================\n",
        "# IMPORTANT: One HAS to run it once before training, so that all variables are initialised.\n",
        "# ========================================================================================\n",
        "print('Test Run Output: ',model.getVolumeOutput(3))\n",
        "\n",
        "#Since we test the CNN with MNIST data, we write the target output in the required format before sent to training/testing.\n",
        "results = np.zeros((len(y),10))\n",
        "results2 = np.zeros((len(y2),10))\n",
        "for i in range(len(y)):\n",
        "    results[i,y[i]] = 1\n",
        "for i in range(len(y2)):\n",
        "    results2[i,y2[i]] = 1\n",
        "\n",
        "# Train model with 2560 images for 10 epochs with a mini-batch size; and then test for 260 images from the MNIST testing dataset\n",
        "model.evaluate(x, results, x2, results2, 10, 1)\n",
        "\n",
        "#model.accuracy(x[2000:4000], results[2000:4000])  # Predict accuracy using test data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaped class-wise number of examples to maintain same distribution original MNIST data:  [252.71466667 287.65866667 254.208      261.58933333 249.25866667\n",
            " 231.296      252.50133333 267.30666667 249.64266667 253.824     ]\n",
            "Training and testing data generated, and pre-processed.\n",
            "Training dataset size: 2560 images, testing dataset size: 260 images.\n",
            "Test Run Output:  [0.09997204 0.09983971 0.10041394 0.10040842 0.10016026 0.09975728\n",
            " 0.10072183 0.10047065 0.09918427 0.09907158]\n",
            "Epoch Number:  1  done.\n",
            "Testing score after  1  epochs:\n",
            "Accuracy =  32.30769230769231 %\n",
            "Epoch Number:  2  done.\n",
            "Testing score after  2  epochs:\n",
            "Accuracy =  36.53846153846153 %\n",
            "Epoch Number:  3  done.\n",
            "Testing score after  3  epochs:\n",
            "Accuracy =  40.76923076923077 %\n",
            "Epoch Number:  4  done.\n",
            "Testing score after  4  epochs:\n",
            "Accuracy =  42.69230769230769 %\n",
            "Epoch Number:  5  done.\n",
            "Testing score after  5  epochs:\n",
            "Accuracy =  43.84615384615385 %\n",
            "Epoch Number:  6  done.\n",
            "Testing score after  6  epochs:\n",
            "Accuracy =  45.76923076923077 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:265: RuntimeWarning: overflow encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:266: RuntimeWarning: overflow encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Number:  7  done.\n",
            "Testing score after  7  epochs:\n",
            "Accuracy =  47.30769230769231 %\n",
            "Epoch Number:  8  done.\n",
            "Testing score after  8  epochs:\n",
            "Accuracy =  47.30769230769231 %\n",
            "Epoch Number:  9  done.\n",
            "Testing score after  9  epochs:\n",
            "Accuracy =  48.07692307692308 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:199: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:202: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:261: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:265: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:265: RuntimeWarning: invalid value encountered in multiply\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:266: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:402: RuntimeWarning: invalid value encountered in less\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9c42af605b24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m# Train model with 2560 images for 10 epochs with a mini-batch size; and then test for 260 images from the MNIST testing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m#model.accuracy(x[2000:4000], results[2000:4000])  # Predict accuracy using test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e2e5081ed387>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x_train, y_train, x_test, y_test, epochs, test_freq)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# Run backPropagation() 'epochs' number of times.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch Number: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtest_freq\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e2e5081ed387>\u001b[0m in \u001b[0;36mbackPropagation\u001b[0;34m(self, input, trueResults)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mnPrev\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnPrev\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     \u001b[0mdhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPoolGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnPrev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0mdhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnPrev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e2e5081ed387>\u001b[0m in \u001b[0;36mPoolGD\u001b[0;34m(self, dLdOut, index)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Initialise correct values in dLdI array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Note the (width) spatial index of the maximum element of the sub array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m             \u001b[0mwidth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrack_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m  \u001b[0;31m# Add the (width) location depending on which sub array was taken for max pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Note the (length) spatial index of the maximum element of the sub array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8OMh5T-B9-Z"
      },
      "source": [
        "# MBGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIPWIW-hAOzj"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "class convnet_mbgd():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.inputImg = None  # The input data\n",
        "        self.strides = []  # The stride length of each layer for convolution\n",
        "        self.recfield = []  # The receptive field in each layer for convolution\n",
        "        self.lengths = []\n",
        "        self.widths = []\n",
        "        self.depths = []\n",
        "        self.weights = []  # The weights for convolution filters\n",
        "        self.node = 10  # The number of nodes in the output layer\n",
        "        self.track = []  # Keeps track of layer order, i.e Conv./Pooling/FC\n",
        "        self.learning_rate = 0.005\n",
        "        self.fc_weights = []\n",
        "        self.output_fc = []\n",
        "\n",
        "    def addInput(self, inpImage):  # Assign the input image\n",
        "        inpImage = np.array(inpImage)\n",
        "        self.inputImg = inpImage\n",
        "        if(len(inpImage.shape) < 3):\n",
        "            num3 = 1\n",
        "            numrows = inpImage.shape[0]\n",
        "            numcols = inpImage.shape[1]\n",
        "        else:\n",
        "            num3 = inpImage.shape[0]\n",
        "            numrows = inpImage.shape[1]\n",
        "            numcols = inpImage.shape[2]\n",
        "\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "\n",
        "    def cvolume(self, s, r, f):\n",
        "        \"\"\"\n",
        "        Creates a new Conv. volume.\n",
        "        s - stride length for convolving the previous layer to create this new volume\n",
        "        r - receptive field for convolving the previous layer to create this new volume\n",
        "        f - number of filters, or in other words, the depth of this new volume\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Initializing the weights\n",
        "        W = []\n",
        "        #b = np.zeros((1, stre))\n",
        "        for i in range(f):\n",
        "            W.append(np.random.randn(prevd, r, r) / (r*r))\n",
        "\n",
        "        W = np.array(W)\n",
        "\n",
        "        # The dimensions of the layer after convolution with the above weight array\n",
        "        numrows = (prevw - r)/s + 1\n",
        "        numcols = (prevl - r)/s + 1\n",
        "        num3 = f\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(W)\n",
        "        self.strides.append(s)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "        self.track.append('c')\n",
        "\n",
        "    def pmaxvolume(self, r):\n",
        "        \"\"\"\n",
        "        Creates a new max pooling layer.\n",
        "        r - the receptive field around which the max value has to be taken.\n",
        "            E.g - If r = 2, max pooling is done withing 2x2 sub matrices.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(None)\n",
        "        self.strides.append(r)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(prevl/r)\n",
        "        self.widths.append(prevw/r)\n",
        "        self.depths.append(prevd)\n",
        "        self.track.append('p')\n",
        "\n",
        "    def FCLayer(self, n_nodes):\n",
        "        \"\"\"\n",
        "        Creates a fully connected layer\n",
        "        n - the no.of nodes in the output layer.\n",
        "        input_fc - the input to the fully connected layer.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = int(self.depths[-1])\n",
        "        prevw = int(self.widths[-1])\n",
        "        prevl = int(self.lengths[-1])\n",
        "\n",
        "        # flatten the input\n",
        "        input_fc = np.zeros((prevd, prevw, prevl))\n",
        "        input_fc = input_fc.flatten()\n",
        "        len_input_fc = len(input_fc)\n",
        "\n",
        "        # Initialise the weights and biases for the FC layer\n",
        "        self.fc_weights = np.random.randn(len_input_fc, n_nodes) / (len_input_fc)\n",
        "        #self.fc_bias = np.zeros(n_nodes)\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(self.fc_weights)\n",
        "        self.strides.append(0)\n",
        "        self.recfield.append(0)\n",
        "        self.lengths.append(1)\n",
        "        self.widths.append(len_input_fc)\n",
        "        self.depths.append(1)\n",
        "        self.track.append('f')\n",
        "        self.node = n_nodes\n",
        "\n",
        "    def activFunc(self, inputArray):\n",
        "        \"\"\"\n",
        "        The activation function for the neurons in the network.\n",
        "        \"\"\"\n",
        "        # ReLU activation\n",
        "        \n",
        "        return np.maximum(0, inputArray)\n",
        "\n",
        "    def dataLoss(self, predResults, trueResults):\n",
        "        \"\"\"\n",
        "        Returns the data loss. Cross-Entropy loss function (Softmax Classifier).\n",
        "        \"\"\"\n",
        "        # L2 loss\n",
        "        loss = 0\n",
        "        sum = 0\n",
        "        for i in range(len(predResults)):\n",
        "            sum += math.exp(predResults[i])\n",
        "        correct = np.argmax(trueResults)\n",
        "        loss = (-1)*(math.log((math.exp(predResults[correct]))/sum))\n",
        "        return loss\n",
        "\n",
        "    def ConvOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Convolutional Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        if(len(W.shape) < 4):\n",
        "            f = 1\n",
        "        else:\n",
        "            f = W.shape[0]\n",
        "\n",
        "        for i in range(f):  # Run loop to create f-filters\n",
        "            for k in range(w):  # Convolve around width\n",
        "                for m in range(l):  # Convolve around length\n",
        "                    # for j in range(d):   #Run over entire depth of prevOut volume\n",
        "                    volOutput[i][k][m] += np.sum(np.multiply(W[i][:][:][:], prevOut[:, k*s: k*s + r, m*s: m*s + r])[:, :, :])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def PoolOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Pooling Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer, since there is no Weight matrix for MaxPooling, it is None\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    volOutput[j][k][m] = np.amax(prevOut[j, k*r: (k + 1)*r, m*r: (m + 1)*r])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def FCOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Implements forward pass for the FC layer. Uses a softmax Classifier.\n",
        "        n_nodes - the no.of nodes in the fully connected layer. \n",
        "        \"\"\"\n",
        "        # flatten the input\n",
        "        prevOut = prevOut.flatten()\n",
        "        #len_input_fc = len(prevOut)\n",
        "\n",
        "        totals = np.dot(prevOut, W)  # + self.fc_bias\n",
        "        # Softmax\n",
        "        exp_totals = np.exp(totals)\n",
        "\n",
        "        # Output from the FC layer\n",
        "        self.output_fc = exp_totals / (np.sum(exp_totals, axis=0))\n",
        "        \n",
        "        return self.output_fc\n",
        "\n",
        "    def getVolumeOutput(self, n):\n",
        "        \"\"\"\n",
        "        Returns the output of the nth volume of the ConvNet.\n",
        "        \"\"\"\n",
        "        penLayer = len(self.weights) - 1  # The penultimate volume\n",
        "\n",
        "        # h stores the output of the current layer\n",
        "        h = np.array(self.inputImg)\n",
        "\n",
        "        # Loop through the hidden layers\n",
        "        for i in range(min(n, penLayer)):\n",
        "            W = self.weights[i]\n",
        "            s = self.strides[i]\n",
        "            r = self.recfield[i]\n",
        "            d = self.depths[i+1]\n",
        "            w = self.widths[i+1]\n",
        "            l = self.lengths[i+1]\n",
        "            if (self.track[i] == 'c'):\n",
        "                h = self.activFunc(self.ConvOutput(h, W, s, r, d, w, l))\n",
        "            elif (self.track[i] == 'p'):\n",
        "                h = self.PoolOutput(h, W, s, r, d, w, l)\n",
        "            else:\n",
        "                h = self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "        # Return the output\n",
        "        if n <= penLayer:\n",
        "            return h\n",
        "        else:\n",
        "            W = self.weights[n-1]\n",
        "            s = self.strides[n-1]\n",
        "            r = self.recfield[n-1]\n",
        "            d = self.depths[n]\n",
        "            w = self.widths[n]\n",
        "            l = self.lengths[n]\n",
        "            return self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "    def FCGD(self, index, trueResults):  # FC layer Gradient Descent\n",
        "        input_fc = self.getVolumeOutput(index - 1)\n",
        "\n",
        "        # Store the shape of input before flattening (to be used for backpropagation)\n",
        "        input_fc_shape = input_fc.shape\n",
        "\n",
        "        # Flatten the input\n",
        "        input_fc = input_fc.flatten()\n",
        "        \n",
        "        # Get the weights and the totals of the fixed layer\n",
        "        W = self.weights[index - 1]\n",
        "        totals = np.dot(input_fc, W)\n",
        "        \n",
        "        # Calculating d_L_d_out\n",
        "        correct = np.argmax(trueResults)\n",
        "        d_L_d_out = np.zeros_like(self.output_fc)\n",
        "        d_L_d_out[correct] = -1 / (self.output_fc[correct])\n",
        "        \n",
        "        # Calculating d_out_d_t\n",
        "        exp_totals = np.exp(totals)\n",
        "        sum_exp_totals = np.sum(exp_totals)\n",
        "        \n",
        "        d_out_d_t = np.zeros_like(self.output_fc)\n",
        "        d_out_d_t = -exp_totals[correct] * (exp_totals / (sum_exp_totals ** 2))\n",
        "        d_out_d_t[correct] = exp_totals[correct] * ((sum_exp_totals - exp_totals[correct])/(sum_exp_totals ** 2))\n",
        "                \n",
        "        # Other necessary gradients\n",
        "        d_t_d_w = input_fc\n",
        "        d_t_d_inputs = W\n",
        "        d_L_d_t = d_L_d_out * d_out_d_t\n",
        "\n",
        "        # Gradients of loss wrt Weights of FC layer and Input of FC layers\n",
        "        # d_L_d_t.shape = (n_nodes,1)\n",
        "        # Adding appropriate axes to d_L_d_t and d_t_d_w(same as input_fc) for . product\n",
        "        d_L_d_w = np.dot(d_t_d_w[np.newaxis].T, d_L_d_t[np.newaxis])\n",
        "\n",
        "        # d_L_d_inputs should have the dimensions of input_fc\n",
        "        d_L_d_inputs = np.dot(d_t_d_inputs, d_L_d_t)\n",
        "\n",
        "        # The dimension of d_L_d_inputs is (len_input_fc,), so, changing the shape so it can be given to maxpool's backprop.\n",
        "        d_L_d_inputs_final = d_L_d_inputs.reshape(input_fc_shape)\n",
        "\n",
        "        dW = -self.learning_rate * d_L_d_w\n",
        "        #self.weights[index - 1] = W\n",
        "        \n",
        "        return (d_L_d_inputs_final, dW)\n",
        "\n",
        "    def PoolGD(self, dLdOut, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates gradients through the MaxPooling layer\n",
        "        dLdOut is the differential of Loss wrt the Output where Output here refers to the output of the MaxPooling layer\n",
        "        This function thus finds dLdI which is the differential of Loss wrt the Input where Input here refers to input to MaxPool layer.\n",
        "        \"\"\"\n",
        "        input_vol = self.getVolumeOutput(index - 1)\n",
        "        s = self.strides[index - 1]\n",
        "        r = self.recfield[index - 1]\n",
        "        d = dLdOut.shape[0]\n",
        "        w = dLdOut.shape[1]\n",
        "        l = dLdOut.shape[2]\n",
        "\n",
        "        # Convert the numbers to int, as the for loops below will report errors if this is not done\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "\n",
        "        # Keep track of the depth and spatial indices of where the maximum element is, in the sub arrays taken for pooling\n",
        "        d_ind = []\n",
        "        spatial_ind = []\n",
        "        # Keep track of which sub array is being taken for max pooling\n",
        "        track_w = []\n",
        "        track_l = []\n",
        "\n",
        "        dLdI = np.zeros((int(self.depths[index - 1]), int(self.lengths[index - 1]), int(self.widths[index - 1])))\n",
        "        replace = dLdOut.flatten()\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    spatial_ind.append(np.where(input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r] == input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r].max()))\n",
        "                    track_l.append(m)\n",
        "                    track_w.append(k)\n",
        "                    d_ind.append(j)\n",
        "\n",
        "        # Initialise correct values in dLdI array\n",
        "        for i in range(len(replace)):\n",
        "            width = spatial_ind[i][0][0]  # Note the (width) spatial index of the maximum element of the sub array\n",
        "            width += track_w[i]*r  # Add the (width) location depending on which sub array was taken for max pooling\n",
        "            length = spatial_ind[i][1][0]  # Note the (length) spatial index of the maximum element of the sub array\n",
        "            length += track_l[i]*r  # Add the (length) location depending on which sub array was taken for max pooling\n",
        "            depth = d_ind[i]  # Note the depth index of the maximum element of the sub array\n",
        "            dLdI[depth][width][length] = replace[i]\n",
        "        \n",
        "        return dLdI\n",
        "\n",
        "    # Helper functions for convBackProp()\n",
        "    def convolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the convoluted output convFilter on inputLayer.\n",
        "        Both are two dimensional matrices square matrices.\n",
        "        inputLayer - (n, n)\n",
        "        convFilter - (f, f)\n",
        "        \"\"\"\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Defining the shape of the output matrix\n",
        "        l = (n-f) + 1\n",
        "        output_matrix = np.zeros((l, l))\n",
        "        s = 1\n",
        "\n",
        "        # Convolving\n",
        "        for row in range(l):\n",
        "            for col in range(l):\n",
        "                output_matrix[row][col] = np.sum(np.multiply(inputLayer[row:row+f, col:col+f], convFilter))\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def fullConvolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the full convoluted output of convFilter on inputLayer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Creating padding for the inputLayer matrix\n",
        "        padding = f - 1\n",
        "        new_dim = n + 2*padding\n",
        "\n",
        "        padded_input = np.zeros([new_dim, new_dim])\n",
        "        padded_input[padding:new_dim - padding,padding:new_dim - padding] = inputLayer\n",
        "\n",
        "        # Now convolve padded_input with convFilter\n",
        "        output_matrix = self.convolve(padded_input, convFilter)\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def rotate180(self, matrix):\n",
        "        \"\"\"\n",
        "        Rotates matrix by 180 degrees in the plane.\n",
        "        Takes only two dimensional matrices.\n",
        "        \"\"\"\n",
        "        return np.rot90(matrix, 2)\n",
        "\n",
        "    def ConvGD(self, dLdoutput, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates through a convolutional layer.\n",
        "        index = index of the current layer\n",
        "        dLdoutput = Gradient of the loss function wrt the output of the current layer (channel, row, col)\n",
        "        Returns dLdinput.\n",
        "        \"\"\"\n",
        "        X = self.getVolumeOutput(index-1)  # Input to the current layer (channel, row, col)\n",
        "        # Weights of the current layer (numFilter, channel, row, col)\n",
        "        W = self.weights[index - 1]\n",
        "\n",
        "        dLdX = np.empty(X.shape)\n",
        "        dLdW = np.empty(W.shape)\n",
        "\n",
        "        dLdout = np.copy(dLdoutput)\n",
        "        dLdout[dLdout < 0] = 0\n",
        "\n",
        "        # Loop over the filters\n",
        "        numFilters = W.shape[0]\n",
        "\n",
        "        for fil_ter in range(numFilters):\n",
        "            filter_output = dLdout[fil_ter]\n",
        "\n",
        "            # Loop over the channels\n",
        "            for channel in range(W.shape[1]):\n",
        "                filter_layer = W[fil_ter][channel]\n",
        "                dWLayer = self.convolve(X[channel], filter_output)\n",
        "                dXLayer = self.rotate180(self.fullConvolve(self.rotate180(filter_layer), filter_output))\n",
        "\n",
        "                # Combine these and return in arrays\n",
        "                dLdW[fil_ter][channel] = dWLayer\n",
        "                dLdX[channel] = dXLayer\n",
        "\n",
        "        #W -= self.learning_rate * dLdW\n",
        "        #self.weights[index - 1] = W\n",
        "        return (dLdX, -self.learning_rate*dLdW)\n",
        "\n",
        "    def backPropagation(self, input, trueResults, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Updates weights by carrying out backpropagation using mini-batches.\n",
        "        trueResults = the expected output from the neural network.\n",
        "        \"\"\"\n",
        "        # This keeps track of all weight parameter changes, and make average update at end of mini-batch\n",
        "        conv_weights = [] \n",
        "        for i in range(len(self.weights) - 1):\n",
        "            if(self.track[i]=='p'):\n",
        "                conv_weights.append('p')\n",
        "            elif(self.track[i]=='c'):\n",
        "                conv_weights.append(np.zeros(self.weights[i].shape))\n",
        "        fc_weights = np.zeros(self.weights[len(self.weights) - 1].shape)\n",
        "        size = int(len(input)/mini_batch_size)\n",
        "        for batch in range(size):\n",
        "            mini_inp = np.array(input[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            mini_res = np.array(trueResults[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            for i in range(len(mini_inp)):\n",
        "                self.inputImg = np.array(mini_inp[i])\n",
        "                if(len(self.inputImg.shape) < 3):\n",
        "                    a = self.inputImg\n",
        "                    self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                # Called once so that all weights are initialised, just in case if not done before\n",
        "                out = self.getVolumeOutput(len(self.weights))\n",
        "                # Index keeping track of the previous layer\n",
        "                nPrev = len(self.weights)\n",
        "                (doutput,dLdW) = self.FCGD(nPrev, mini_res[i]) # The dLdW has the negative sign and is multiplied by the learning rate before getting passed here\n",
        "                fc_weights += dLdW/len(mini_inp)\n",
        "                nPrev -= 1\n",
        "                \n",
        "                # Loop over the layers\n",
        "                while nPrev - 1 >= 0:\n",
        "                    if(self.track[nPrev - 1] == 'p'):\n",
        "                        dhidden = self.PoolGD(doutput, nPrev)\n",
        "                    else:\n",
        "                        (dhidden, dLdW) = self.ConvGD(doutput, nPrev)\n",
        "                        conv_weights[nPrev - 1] += dLdW/len(mini_inp)\n",
        "                        #self.weights[nPrev - 1] -= dLdW / len(mini_inp)\n",
        "                    doutput = dhidden  # Move to the previous layer\n",
        "                    nPrev -= 1\n",
        "            for i in range(len(self.weights) - 1):\n",
        "                if(self.track[i]=='c'):\n",
        "                    self.weights[i] += conv_weights[i]\n",
        "                    conv_weights[i] = np.zeros(conv_weights[i].shape)\n",
        "            self.weights[len(self.weights) - 1] += fc_weights\n",
        "            fc_weights = np.zeros(fc_weights.shape)\n",
        "            #print('Training over mini-batch number: ',self.count, 'done.')\n",
        "            self.count = self.count + 1\n",
        "\n",
        "    def evaluate(self, x_train, y_train, x_test, y_test, epochs, mini_batch_size, test_freq):\n",
        "        \"\"\"\n",
        "        Train the neural network. And run test using test data to see progress of the model accuracy.\n",
        "        x_train, x_test = the inout training and testing data respectively.\n",
        "        y_train, y_test = the expected results from the CNN for the training and testing data respectively.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size = the size of mini-batches.\n",
        "        test_freq = Testing results should be printed after 'test_freq' number of epochs.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(x_train, y_train, mini_batch_size)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "            if(i%test_freq==0):\n",
        "                print('Testing score after ',i + 1,' epochs:')\n",
        "                self.accuracy(x_test, y_test)\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "\n",
        "    def train(self, input, Y, epochs, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "        Y = the expected results from the neural network.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size is the size of mini-batches.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(input, Y, mini_batch_size)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "    def accuracy(self, X, Y):\n",
        "        \"\"\"\n",
        "        Function that takes in test data and results and calculates the accuracy of the Network.\n",
        "        \"\"\"\n",
        "        y = []\n",
        "        cor = 0\n",
        "        correct = 0\n",
        "        for i in range(len(X)):\n",
        "            self.inputImg = np.array(X[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                y.append(self.getVolumeOutput(len(self.weights)))\n",
        "        Y = np.array(Y)\n",
        "        y = np.array(y)\n",
        "        if (np.max(y) == 0):\n",
        "            y /= 1.0\n",
        "        else:\n",
        "            y /= np.max(y)\n",
        "        for i in range(len(Y)):\n",
        "            correct = np.argmax(Y[i])\n",
        "            if (np.argmax(y[i]) == correct):\n",
        "                cor += 1\n",
        "        cor /= len(Y)\n",
        "        print('Accuracy = ', cor*100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dusxx7ybtFy"
      },
      "source": [
        "# Code Run for MBGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgDepvOfEPua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d57a772e-4947-48f9-8ed6-801c17747b30"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd    \n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "(x_original, y_original), (x2_original, y2_original) = datasets.mnist.load_data()\n",
        "\n",
        "# See distribution of data for each class in the original testing data of MNIST database\n",
        "count = np.zeros(10)\n",
        "for i in range(len(x_original)):\n",
        "    count[y_original[i]] += 1\n",
        "count /= len(x_original)\n",
        "\n",
        "# How many examples of each do we need to maintain the same distribution of original MNIST database.\n",
        "count *= 2560\n",
        "print('Reshaped class-wise number of examples to maintain same distribution original MNIST data: ',count)\n",
        "\n",
        "# Make new training and testing data\n",
        "x = []\n",
        "y = []\n",
        "x2 = []\n",
        "y2 = []\n",
        "\n",
        "df=pd.DataFrame(y_original, columns=['y_original']) \n",
        "index = [i for i in range(0, len(df.index))]\n",
        "df['index'] = index\n",
        "df.columns=['y_original', 'Index']\n",
        "df.set_index('Index',inplace=True)\n",
        "\n",
        "# Append first n examples of each class\n",
        "# NOTE: Yes it is an easier way if we could use a double for-loop, but we purposely write it in 10 explicit for-loops so that it is easier for the reader to understand.\n",
        "for i in range(253):\n",
        "    index = (df.index[df['y_original'] == 0]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(0)\n",
        "\n",
        "for i in range(288):\n",
        "    index = (df.index[df['y_original'] == 1]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(1)\n",
        "\n",
        "for i in range(254):\n",
        "    index = (df.index[df['y_original'] == 2]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(2)\n",
        "\n",
        "for i in range(262):\n",
        "    index = (df.index[df['y_original'] == 3]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(3)\n",
        "\n",
        "for i in range(249):\n",
        "    index = (df.index[df['y_original'] == 4]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(4)\n",
        "\n",
        "for i in range(231):\n",
        "    index = (df.index[df['y_original'] == 5]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(5)\n",
        "\n",
        "for i in range(252):\n",
        "    index = (df.index[df['y_original'] == 6]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(6)\n",
        "\n",
        "for i in range(267):\n",
        "    index = (df.index[df['y_original'] == 7]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(7)\n",
        "\n",
        "for i in range(250):\n",
        "    index = (df.index[df['y_original'] == 8]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(8)\n",
        "\n",
        "for i in range(254):\n",
        "    index = (df.index[df['y_original'] == 9]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(9)\n",
        "\n",
        "\n",
        "for i in range(260):\n",
        "    x2.append(x2_original[i])\n",
        "    y2.append(y2_original[i])\n",
        "\n",
        "x= np.array(x)\n",
        "y= np.array(y)\n",
        "x2= np.array(x2)\n",
        "y2= np.array(y2)\n",
        "\n",
        "# Normalise data\n",
        "x = ((x/255) - 0.5)\n",
        "x2 = ((x2/255) - 0.5)\n",
        "\n",
        "print('Training and testing data generated, and pre-processed.')\n",
        "print('Training dataset size: 2560 images, testing dataset size: 260 images.')\n",
        "\n",
        "\n",
        "# Create instance of NeuralNetwork\n",
        "#model = convnet_herbo_mbgd()\n",
        "#model = convnet_adam_mbgd()\n",
        "#model = convnet_sgd()\n",
        "model = convnet_mbgd()\n",
        "\n",
        "X = np.random.randn(1,28,28)\n",
        "model.addInput(X) # Input layer\n",
        "model.cvolume(1,5,10) # Add Convolutional volume (stride length, receptive field, filters)\n",
        "model.pmaxvolume(2) # Add Pooling layer (receptive field)\n",
        "model.FCLayer(10)  # Add FC Layer (number of classifiers)\n",
        "# Get final output layer. \n",
        "# ========================================================================================\n",
        "# IMPORTANT: One HAS to run it once before training, so that all variables are initialised.\n",
        "# ========================================================================================\n",
        "print('Test Run Output: ',model.getVolumeOutput(3))\n",
        "\n",
        "#Since we test the CNN with MNIST data, we write the target output in the required format before sent to training/testing.\n",
        "results = np.zeros((len(y),10))\n",
        "results2 = np.zeros((len(y2),10))\n",
        "for i in range(len(y)):\n",
        "    results[i,y[i]] = 1\n",
        "for i in range(len(y2)):\n",
        "    results2[i,y2[i]] = 1\n",
        "\n",
        "# Train model with 2560 images for 10 epochs with a mini-batch size; and then test for 260 images from the MNIST testing dataset\n",
        "model.evaluate(x, results, x2, results2, 10, 128, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Reshaped class-wise number of examples to maintain same distribution original MNIST data:  [252.71466667 287.65866667 254.208      261.58933333 249.25866667\n",
            " 231.296      252.50133333 267.30666667 249.64266667 253.824     ]\n",
            "Training and testing data generated, and pre-processed.\n",
            "Training dataset size: 2560 images, testing dataset size: 260 images.\n",
            "Test Run Output:  [0.09997204 0.09983971 0.10041394 0.10040842 0.10016026 0.09975728\n",
            " 0.10072183 0.10047065 0.09918427 0.09907158]\n",
            "Epoch Number:  1  done.\n",
            "Testing score after  1  epochs:\n",
            "Accuracy =  58.07692307692308 %\n",
            "Epoch Number:  2  done.\n",
            "Testing score after  2  epochs:\n",
            "Accuracy =  60.38461538461538 %\n",
            "Epoch Number:  3  done.\n",
            "Testing score after  3  epochs:\n",
            "Accuracy =  60.76923076923077 %\n",
            "Epoch Number:  4  done.\n",
            "Testing score after  4  epochs:\n",
            "Accuracy =  60.38461538461538 %\n",
            "Epoch Number:  5  done.\n",
            "Testing score after  5  epochs:\n",
            "Accuracy =  60.0 %\n",
            "Epoch Number:  6  done.\n",
            "Testing score after  6  epochs:\n",
            "Accuracy =  60.38461538461538 %\n",
            "Epoch Number:  7  done.\n",
            "Testing score after  7  epochs:\n",
            "Accuracy =  60.0 %\n",
            "Epoch Number:  8  done.\n",
            "Testing score after  8  epochs:\n",
            "Accuracy =  60.0 %\n",
            "Epoch Number:  9  done.\n",
            "Testing score after  9  epochs:\n",
            "Accuracy =  60.38461538461538 %\n",
            "Epoch Number:  10  done.\n",
            "Testing score after  10  epochs:\n",
            "Accuracy =  60.38461538461538 %\n",
            "Training Complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZa17gwFBvrI"
      },
      "source": [
        "# Adam SGD\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-as3QzS0BwKm"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Ensuring reproducibsility\n",
        "np.random.seed(0)\n",
        "\n",
        "class convnet_adam_sgd():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.inputImg = None  # The input data\n",
        "        self.strides = []  # The stride length of each layer for convolution\n",
        "        self.recfield = []  # The receptive field in each layer for convolution\n",
        "        self.lengths = []\n",
        "        self.widths = []\n",
        "        self.depths = []\n",
        "        self.weights = []  # The weights for convolution filters\n",
        "        self.node = 10  # The number of nodes in the output layer\n",
        "        self.track = []  # Keeps track of layer order, i.e Conv./Pooling/FC\n",
        "        self.learning_rate = 0.005\n",
        "        self.fc_weights = []\n",
        "        self.output_fc = []\n",
        "\n",
        "        # Adam optimization\n",
        "        self.adam_m = []\n",
        "        self.adam_v = []\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.eps = 1e-4\n",
        "        self.count = 1 # Keeps track of iteration number needed for optimisation\n",
        "\n",
        "    def addInput(self, inpImage):  # Assign the input image\n",
        "        inpImage = np.array(inpImage)\n",
        "        self.inputImg = inpImage\n",
        "        if(len(inpImage.shape) < 3):\n",
        "            num3 = 1\n",
        "            numrows = inpImage.shape[0]\n",
        "            numcols = inpImage.shape[1]\n",
        "        else:\n",
        "            num3 = inpImage.shape[0]\n",
        "            numrows = inpImage.shape[1]\n",
        "            numcols = inpImage.shape[2]\n",
        "\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "\n",
        "    def cvolume(self, s, r, f):\n",
        "        \"\"\"\n",
        "        Creates a new Conv. volume.\n",
        "        s - stride length for convolving the previous layer to create this new volume\n",
        "        r - receptive field for convolving the previous layer to create this new volume\n",
        "        f - number of filters, or in other words, the depth of this new volume\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Initializing the weights\n",
        "        W = []\n",
        "\n",
        "        # For Adam optimization\n",
        "        M = []\n",
        "        V = []\n",
        "\n",
        "        #b = np.zeros((1, stre))\n",
        "        for i in range(f):\n",
        "            W.append(np.random.randn(prevd, r, r) / (r*r))\n",
        "            M.append(np.zeros((prevd, r, r)))\n",
        "            V.append(np.zeros((prevd, r, r)))\n",
        "\n",
        "        W = np.array(W)\n",
        "        M = np.array(M)\n",
        "        V = np.array(V)\n",
        "\n",
        "        # The dimensions of the layer after convolution with the above weight array\n",
        "        numrows = (prevw - r)/s + 1\n",
        "        numcols = (prevl - r)/s + 1\n",
        "        num3 = f\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(W)\n",
        "        self.strides.append(s)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "        self.track.append('c')\n",
        "\n",
        "        self.adam_m.append(M)\n",
        "        self.adam_v.append(V)\n",
        "\n",
        "    def pmaxvolume(self, r):\n",
        "        \"\"\"\n",
        "        Creates a new max pooling layer.\n",
        "        r - the receptive field around which the max value has to be taken.\n",
        "            E.g - If r = 2, max pooling is done withing 2x2 sub matrices.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(None)\n",
        "        self.strides.append(r)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(prevl/r)\n",
        "        self.widths.append(prevw/r)\n",
        "        self.depths.append(prevd)\n",
        "        self.track.append('p')\n",
        "\n",
        "        self.adam_m.append(None)\n",
        "        self.adam_v.append(None)\n",
        "\n",
        "    def FCLayer(self, n_nodes):\n",
        "        \"\"\"\n",
        "        Creates a fully connected layer\n",
        "        n - the no.of nodes in the output layer.\n",
        "        input_fc - the input to the fully connected layer.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = int(self.depths[-1])\n",
        "        prevw = int(self.widths[-1])\n",
        "        prevl = int(self.lengths[-1])\n",
        "\n",
        "        # flatten the input\n",
        "        input_fc = np.zeros((prevd, prevw, prevl))\n",
        "        input_fc = input_fc.flatten()\n",
        "        len_input_fc = len(input_fc)\n",
        "\n",
        "        # Initialise the weights and biases for the FC layer\n",
        "        self.fc_weights = np.random.randn(len_input_fc, n_nodes) / (len_input_fc)\n",
        "        #self.fc_bias = np.zeros(n_nodes)\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(self.fc_weights)\n",
        "        self.strides.append(0)\n",
        "        self.recfield.append(0)\n",
        "        self.lengths.append(1)\n",
        "        self.widths.append(len_input_fc)\n",
        "        self.depths.append(1)\n",
        "        self.track.append('f')\n",
        "        self.node = n_nodes\n",
        "\n",
        "        self.adam_m.append(None)\n",
        "        self.adam_v.append(None)\n",
        "\n",
        "    def activFunc(self, inputArray):\n",
        "        \"\"\"\n",
        "        The activation function for the neurons in the network.\n",
        "        \"\"\"\n",
        "        # ReLU activation\n",
        "        return np.maximum(0, inputArray)\n",
        "\n",
        "    def dataLoss(self, predResults, trueResults):\n",
        "        \"\"\"\n",
        "        Returns the data loss. Cross-Entropy loss function (Softmax Classifier).\n",
        "        \"\"\"\n",
        "        # Softmax\n",
        "        loss = 0\n",
        "        sum = 0\n",
        "        for i in range(len(predResults)):\n",
        "            sum += math.exp(predResults[i])\n",
        "        correct = np.argmax(trueResults)\n",
        "        loss = (-1)*(math.log((math.exp(predResults[correct]))/sum))\n",
        "        return loss\n",
        "\n",
        "    def ConvOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Convolutional Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        if(len(W.shape) < 4):\n",
        "            f = 1\n",
        "        else:\n",
        "            f = W.shape[0]\n",
        "\n",
        "        for i in range(f):  # Run loop to create f-filters\n",
        "            for k in range(w):  # Convolve around width\n",
        "                for m in range(l):  # Convolve around length\n",
        "                    # for j in range(d):   #Run over entire depth of prevOut volume\n",
        "                    volOutput[i][k][m] += np.sum(np.multiply(W[i][:][:][:], prevOut[:, k*s: k*s + r, m*s: m*s + r])[:, :, :])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def PoolOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Pooling Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer, since there is no Weight matrix for MaxPooling, it is None\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    volOutput[j][k][m] = np.amax(prevOut[j, k*r: (k + 1)*r, m*r: (m + 1)*r])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def FCOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Implements forward pass for the FC layer. Uses a softmax Classifier.\n",
        "        n_nodes - the no.of nodes in the fully connected layer. \n",
        "        \"\"\"\n",
        "        # flatten the input\n",
        "        prevOut = prevOut.flatten()\n",
        "        #len_input_fc = len(prevOut)\n",
        "\n",
        "        totals = np.dot(prevOut, W)  # + self.fc_bias\n",
        "        # Softmax\n",
        "        exp_totals = np.exp(totals)\n",
        "\n",
        "        # Output from the FC layer\n",
        "        self.output_fc = exp_totals / (np.sum(exp_totals, axis=0))\n",
        "        return self.output_fc\n",
        "\n",
        "    def getVolumeOutput(self, n):\n",
        "        \"\"\"\n",
        "        Returns the output of the nth volume of the ConvNet.\n",
        "        \"\"\"\n",
        "        penLayer = len(self.weights) - 1  # The penultimate volume\n",
        "\n",
        "        # h stores the output of the current layer\n",
        "        h = np.array(self.inputImg)\n",
        "\n",
        "        # Loop through the hidden layers\n",
        "        for i in range(min(n, penLayer)):\n",
        "            W = self.weights[i]\n",
        "            s = self.strides[i]\n",
        "            r = self.recfield[i]\n",
        "            d = self.depths[i+1]\n",
        "            w = self.widths[i+1]\n",
        "            l = self.lengths[i+1]\n",
        "            if (self.track[i] == 'c'):\n",
        "                h = self.activFunc(self.ConvOutput(h, W, s, r, d, w, l))\n",
        "            elif (self.track[i] == 'p'):\n",
        "                h = self.PoolOutput(h, W, s, r, d, w, l)\n",
        "            else:\n",
        "                h = self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "        # Return the output\n",
        "        if n <= penLayer:\n",
        "            return h\n",
        "        else:\n",
        "            W = self.weights[n-1]\n",
        "            s = self.strides[n-1]\n",
        "            r = self.recfield[n-1]\n",
        "            d = self.depths[n]\n",
        "            w = self.widths[n]\n",
        "            l = self.lengths[n]\n",
        "            return self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "    def FCGD(self, index, trueResults):  # FC layer Gradient Descent\n",
        "        input_fc = self.getVolumeOutput(index - 1)\n",
        "\n",
        "        # Store the shape of input before flattening (to be used for backpropagation)\n",
        "        input_fc_shape = input_fc.shape\n",
        "\n",
        "        # Flatten the input\n",
        "        input_fc = input_fc.flatten()\n",
        "\n",
        "        # Get the weights and the totals of the fixed layer\n",
        "        W = self.weights[index - 1]\n",
        "        totals = np.dot(input_fc, W)\n",
        "\n",
        "        # Calculating d_L_d_out\n",
        "        correct = np.argmax(trueResults)\n",
        "        d_L_d_out = np.zeros_like(self.output_fc)\n",
        "        d_L_d_out[correct] = -1 / (self.output_fc[correct])\n",
        "\n",
        "        # Calculating d_out_d_t\n",
        "        exp_totals = np.exp(totals)\n",
        "        sum_exp_totals = np.sum(exp_totals)\n",
        "\n",
        "        d_out_d_t = np.zeros_like(self.output_fc)\n",
        "        d_out_d_t = -exp_totals[correct] * (exp_totals / (sum_exp_totals ** 2))\n",
        "        d_out_d_t[correct] = exp_totals[correct] * ((sum_exp_totals - exp_totals[correct])/(sum_exp_totals ** 2))\n",
        "\n",
        "        # Other necessary gradients\n",
        "        d_t_d_w = input_fc\n",
        "        d_t_d_inputs = W\n",
        "        d_L_d_t = d_L_d_out * d_out_d_t\n",
        "\n",
        "        # Gradients of loss wrt Weights of FC layer and Input of FC layers\n",
        "        # d_L_d_t.shape = (n_nodes,1)\n",
        "        # Adding appropriate axes to d_L_d_t and d_t_d_w(same as input_fc) for . product\n",
        "        d_L_d_w = np.dot(d_t_d_w[np.newaxis].T, d_L_d_t[np.newaxis])\n",
        "\n",
        "        # d_L_d_inputs should have the dimensions of input_fc\n",
        "        d_L_d_inputs = np.dot(d_t_d_inputs, d_L_d_t)\n",
        "\n",
        "        # The dimension of d_L_d_inputs is (len_input_fc,), so, changing the shape so it can be given to maxpool's backprop.\n",
        "        d_L_d_inputs_final = d_L_d_inputs.reshape(input_fc_shape)\n",
        "\n",
        "        #W -= self.learning_rate * d_L_d_w\n",
        "        #self.weights[index - 1] = W\n",
        "        dW = -self.learning_rate * d_L_d_w\n",
        "        return (d_L_d_inputs_final, dW)\n",
        "\n",
        "    def PoolGD(self, dLdOut, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates gradients through the MaxPooling layer\n",
        "        dLdOut is the differential of Loss wrt the Output where Output here refers to the output of the MaxPooling layer\n",
        "        This function thus finds dLdI which is the differential of Loss wrt the Input where Input here refers to input to MaxPool layer.\n",
        "        \"\"\"\n",
        "        input_vol = self.getVolumeOutput(index - 1)\n",
        "        s = self.strides[index - 1]\n",
        "        r = self.recfield[index - 1]\n",
        "        d = dLdOut.shape[0]\n",
        "        w = dLdOut.shape[1]\n",
        "        l = dLdOut.shape[2]\n",
        "\n",
        "        # Convert the numbers to int, as the for loops below will report errors if this is not done\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "\n",
        "        # Keep track of the depth and spatial indices of where the maximum element is, in the sub arrays taken for pooling\n",
        "        d_ind = []\n",
        "        spatial_ind = []\n",
        "        # Keep track of which sub array is being taken for max pooling\n",
        "        track_w = []\n",
        "        track_l = []\n",
        "\n",
        "        dLdI = np.zeros((int(self.depths[index - 1]), int(self.lengths[index - 1]), int(self.widths[index - 1])))\n",
        "        replace = dLdOut.flatten()\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    spatial_ind.append(np.where(input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r] == input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r].max()))\n",
        "                    track_l.append(m)\n",
        "                    track_w.append(k)\n",
        "                    d_ind.append(j)\n",
        "\n",
        "        # Initialise correct values in dLdI array\n",
        "        for i in range(len(replace)):\n",
        "            width = spatial_ind[i][0][0]  # Note the (width) spatial index of the maximum element of the sub array\n",
        "            width += track_w[i]*r  # Add the (width) location depending on which sub array was taken for max pooling\n",
        "            length = spatial_ind[i][1][0]  # Note the (length) spatial index of the maximum element of the sub array\n",
        "            length += track_l[i]*r  # Add the (length) location depending on which sub array was taken for max pooling\n",
        "            depth = d_ind[i]  # Note the depth index of the maximum element of the sub array\n",
        "            dLdI[depth][width][length] = replace[i]\n",
        "\n",
        "        return dLdI\n",
        "\n",
        "    # Helper functions for convBackProp()\n",
        "    def convolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the convoluted output convFilter on inputLayer.\n",
        "        Both are two dimensional matrices square matrices.\n",
        "        inputLayer - (n, n)\n",
        "        convFilter - (f, f)\n",
        "        \"\"\"\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Defining the shape of the output matrix\n",
        "        l = (n-f) + 1\n",
        "        output_matrix = np.zeros((l, l))\n",
        "        s = 1\n",
        "\n",
        "        # Convolving\n",
        "        for row in range(l):\n",
        "            for col in range(l):\n",
        "                output_matrix[row][col] = np.sum(np.multiply(inputLayer[row:row+f, col:col+f], convFilter))\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def fullConvolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the full convoluted output of convFilter on inputLayer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Creating padding for the inputLayer matrix\n",
        "        padding = f - 1\n",
        "        new_dim = n + 2*padding\n",
        "\n",
        "        padded_input = np.zeros([new_dim, new_dim])\n",
        "        padded_input[padding:new_dim - padding,padding:new_dim - padding] = inputLayer\n",
        "\n",
        "        # Now convolve padded_input with convFilter\n",
        "        output_matrix = self.convolve(padded_input, convFilter)\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def rotate180(self, matrix):\n",
        "        \"\"\"\n",
        "        Rotates matrix by 180 degrees in the plane.\n",
        "        Takes only two dimensional matrices.\n",
        "        \"\"\"\n",
        "        return np.rot90(matrix, 2)\n",
        "\n",
        "    def ConvGD(self, dLdoutput, index, t):\n",
        "        \"\"\"\n",
        "        Function that backpropagates through a convolutional layer.\n",
        "        index = index of the current layer\n",
        "        dLdoutput = Gradient of the loss function wrt the output of the current layer (channel, row, col)\n",
        "        Returns dLdinput.\n",
        "        t = iterative index\n",
        "        \"\"\"\n",
        "        X = self.getVolumeOutput(index-1)  # Input to the current layer (channel, row, col)\n",
        "        # Weights of the current layer (numFilter, channel, row, col)\n",
        "        W = self.weights[index - 1]\n",
        "        M = self.adam_m[index - 1]\n",
        "        V = self.adam_v[index - 1]\n",
        "\n",
        "        dLdX = np.empty(X.shape)\n",
        "        dLdW = np.empty(W.shape)\n",
        "\n",
        "        dLdout = np.copy(dLdoutput)\n",
        "        dLdout[dLdout < 0] = 0\n",
        "\n",
        "        # Loop over the filters\n",
        "        numFilters = W.shape[0]\n",
        "\n",
        "        for fil_ter in range(numFilters):\n",
        "            filter_output = dLdout[fil_ter]\n",
        "\n",
        "            # Loop over the channels\n",
        "            for channel in range(W.shape[1]):\n",
        "                filter_layer = W[fil_ter][channel]\n",
        "                dWLayer = self.convolve(X[channel], filter_output)\n",
        "                dXLayer = self.rotate180(self.fullConvolve(self.rotate180(filter_layer), filter_output))\n",
        "\n",
        "                # Combine these and return in arrays\n",
        "                dLdW[fil_ter][channel] = dWLayer\n",
        "                dLdX[channel] = dXLayer\n",
        "\n",
        "        # Adam optimization\n",
        "        M = self.beta1 * M + (1 - self.beta1) * dLdW\n",
        "        Mt = M / (1 - self.beta1**t)\n",
        "        V = self.beta2 * V + (1 - self.beta2) * np.square(dLdW)\n",
        "        Vt = V / (1 - self.beta2**t)\n",
        "\n",
        "        dLdW -= self.learning_rate * Mt / (np.sqrt(Vt) + self.eps)  # Note that the weights update is being multiplied with learning rate\n",
        "        #self.weights[index - 1] = W\n",
        "\n",
        "        self.adam_m[index - 1] = M\n",
        "        self.adam_v[index - 1] = V\n",
        "\n",
        "        return (dLdX,dLdW)\n",
        "\n",
        "    def backPropagation(self, input, trueResults, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Updates weights by carrying out backpropagation using mini-batches.\n",
        "        trueResults = the expected output from the neural network.\n",
        "        \"\"\"\n",
        "        # This keeps track of all weight parameter changes, and make average update at end of mini-batch\n",
        "        conv_weights = [] \n",
        "        for i in range(len(self.weights) - 1):\n",
        "            if(self.track[i]=='p'):\n",
        "                conv_weights.append('p')\n",
        "            elif(self.track[i]=='c'):\n",
        "                conv_weights.append(np.zeros(self.weights[i].shape))\n",
        "        fc_weights = np.zeros(self.weights[len(self.weights) - 1].shape)\n",
        "        size = int(len(input)/mini_batch_size)\n",
        "        for batch in range(size):\n",
        "            mini_inp = np.array(input[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            mini_res = np.array(trueResults[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            for i in range(len(mini_inp)):\n",
        "                self.inputImg = np.array(mini_inp[i])\n",
        "                if(len(self.inputImg.shape) < 3):\n",
        "                    a = self.inputImg\n",
        "                    self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                # Called once so that all weights are initialised, just in case if not done before\n",
        "                out = self.getVolumeOutput(len(self.weights))\n",
        "                # Index keeping track of the previous layer\n",
        "                nPrev = len(self.weights)\n",
        "                (doutput,dLdW) = self.FCGD(nPrev, mini_res[i]) # The dLdW has the negative sign and is multiplied by the learning rate before getting passed here\n",
        "                fc_weights += dLdW/len(mini_inp)\n",
        "                nPrev -= 1\n",
        "                \n",
        "                # Loop over the layers\n",
        "                while nPrev - 1 >= 0:\n",
        "                    if(self.track[nPrev - 1] == 'p'):\n",
        "                        dhidden = self.PoolGD(doutput, nPrev)\n",
        "                    else:\n",
        "                        (dhidden, dLdW) = self.ConvGD(doutput, nPrev, self.count)\n",
        "                        conv_weights[nPrev - 1] -= dLdW/len(mini_inp)\n",
        "                        #self.weights[nPrev - 1] -= dLdW / len(mini_inp)\n",
        "                    doutput = dhidden  # Move to the previous layer\n",
        "                    nPrev -= 1\n",
        "            for i in range(len(self.weights) - 1):\n",
        "                if(self.track[i]=='c'):\n",
        "                    self.weights[i] += conv_weights[i]\n",
        "                    conv_weights[i] = np.zeros(conv_weights[i].shape)\n",
        "            self.weights[len(self.weights) - 1] += fc_weights\n",
        "            fc_weights = np.zeros(fc_weights.shape)\n",
        "            #print('Training over mini-batch number: ',self.count, 'done.')\n",
        "            self.count = self.count + 1\n",
        "\n",
        "    def evaluate(self, x_train, y_train, x_test, y_test, epochs, mini_batch_size, test_freq):\n",
        "        \"\"\"\n",
        "        Train the neural network. And run test using test data to see progress of the model accuracy.\n",
        "        x_train, x_test = the inout training and testing data respectively.\n",
        "        y_train, y_test = the expected results from the CNN for the training and testing data respectively.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size = the size of mini-batches.\n",
        "        test_freq = Testing results should be printed after 'test_freq' number of epochs.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(x_train, y_train, mini_batch_size)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "            if(i%test_freq==0):\n",
        "                print('Testing score after ',i + 1,' epochs:')\n",
        "                self.accuracy(x_test, y_test)\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "\n",
        "    def train(self, input, Y, epochs, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "        Y = the expected results from the neural network.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size is the size of mini-batches.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(input, Y, mini_batch_size)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "    def accuracy(self, X, Y):\n",
        "        \"\"\"\n",
        "        Function that takes in test data and results and calculates the accuracy of the Network.\n",
        "        \"\"\"\n",
        "        y = []\n",
        "        cor = 0\n",
        "        correct = 0\n",
        "        for i in range(len(X)):\n",
        "            self.inputImg = np.array(X[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                y.append(self.getVolumeOutput(len(self.weights)))\n",
        "        Y = np.array(Y)\n",
        "        y = np.array(y)\n",
        "        if (np.max(y) == 0):\n",
        "            y /= 1.0\n",
        "        else:\n",
        "            y /= np.max(y)\n",
        "        for i in range(len(Y)):\n",
        "            correct = np.argmax(Y[i])\n",
        "            if (np.argmax(y[i]) == correct):\n",
        "                cor += 1\n",
        "        cor /= len(Y)\n",
        "        print('Accuracy = ', cor*100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25TXqxG_Bwwu"
      },
      "source": [
        "# Code Run for Adam + SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ0fRKeSBxSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee985b3-6f72-4e18-8d75-b44530cab95a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd    \n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "(x_original, y_original), (x2_original, y2_original) = datasets.mnist.load_data()\n",
        "\n",
        "# See distribution of data for each class in the original testing data of MNIST database\n",
        "count = np.zeros(10)\n",
        "for i in range(len(x_original)):\n",
        "    count[y_original[i]] += 1\n",
        "count /= len(x_original)\n",
        "\n",
        "# How many examples of each do we need to maintain the same distribution of original MNIST database.\n",
        "count *= 2560\n",
        "print('Reshaped class-wise number of examples to maintain same distribution original MNIST data: ',count)\n",
        "\n",
        "# Make new training and testing data\n",
        "x = []\n",
        "y = []\n",
        "x2 = []\n",
        "y2 = []\n",
        "\n",
        "df=pd.DataFrame(y_original, columns=['y_original']) \n",
        "index = [i for i in range(0, len(df.index))]\n",
        "df['index'] = index\n",
        "df.columns=['y_original', 'Index']\n",
        "df.set_index('Index',inplace=True)\n",
        "\n",
        "# Append first n examples of each class\n",
        "# NOTE: Yes it is an easier way if we could use a double for-loop, but we purposely write it in 10 explicit for-loops so that it is easier for the reader to understand.\n",
        "for i in range(253):\n",
        "    index = (df.index[df['y_original'] == 0]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(0)\n",
        "\n",
        "for i in range(288):\n",
        "    index = (df.index[df['y_original'] == 1]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(1)\n",
        "\n",
        "for i in range(254):\n",
        "    index = (df.index[df['y_original'] == 2]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(2)\n",
        "\n",
        "for i in range(262):\n",
        "    index = (df.index[df['y_original'] == 3]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(3)\n",
        "\n",
        "for i in range(249):\n",
        "    index = (df.index[df['y_original'] == 4]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(4)\n",
        "\n",
        "for i in range(231):\n",
        "    index = (df.index[df['y_original'] == 5]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(5)\n",
        "\n",
        "for i in range(252):\n",
        "    index = (df.index[df['y_original'] == 6]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(6)\n",
        "\n",
        "for i in range(267):\n",
        "    index = (df.index[df['y_original'] == 7]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(7)\n",
        "\n",
        "for i in range(250):\n",
        "    index = (df.index[df['y_original'] == 8]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(8)\n",
        "\n",
        "for i in range(254):\n",
        "    index = (df.index[df['y_original'] == 9]).tolist()[i]\n",
        "    x.append(x_original[index])\n",
        "    y.append(9)\n",
        "\n",
        "\n",
        "for i in range(260):\n",
        "    x2.append(x2_original[i])\n",
        "    y2.append(y2_original[i])\n",
        "\n",
        "x= np.array(x)\n",
        "y= np.array(y)\n",
        "x2= np.array(x2)\n",
        "y2= np.array(y2)\n",
        "\n",
        "# Normalise data\n",
        "x = ((x/255) - 0.5)\n",
        "x2 = ((x2/255) - 0.5)\n",
        "\n",
        "print('Training and testing data generated, and pre-processed.')\n",
        "print('Training dataset size: 2560 images, testing dataset size: 260 images.')\n",
        "\n",
        "\n",
        "# Create instance of NeuralNetwork\n",
        "#model = convnet_herbo_mbgd()\n",
        "#model = convnet_adam_mbgd()\n",
        "#model = convnet_sgd()\n",
        "#model = convnet_mbgd()\n",
        "model = convnet_adam_sgd()\n",
        "\n",
        "X = np.random.randn(1,28,28)\n",
        "model.addInput(X) # Input layer\n",
        "model.cvolume(1,5,10) # Add Convolutional volume (stride length, receptive field, filters)\n",
        "model.pmaxvolume(2) # Add Pooling layer (receptive field)\n",
        "model.FCLayer(10)  # Add FC Layer (number of classifiers)\n",
        "# Get final output layer. \n",
        "# ========================================================================================\n",
        "# IMPORTANT: One HAS to run it once before training, so that all variables are initialised.\n",
        "# ========================================================================================\n",
        "print('Test Run Output: ',model.getVolumeOutput(3))\n",
        "\n",
        "#Since we test the CNN with MNIST data, we write the target output in the required format before sent to training/testing.\n",
        "results = np.zeros((len(y),10))\n",
        "results2 = np.zeros((len(y2),10))\n",
        "for i in range(len(y)):\n",
        "    results[i,y[i]] = 1\n",
        "for i in range(len(y2)):\n",
        "    results2[i,y2[i]] = 1\n",
        "\n",
        "# Train model with 2560 images for 10 epochs with a mini-batch size; and then test for 260 images from the MNIST testing dataset\n",
        "model.evaluate(x, results, x2, results2, 10, 1, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Reshaped class-wise number of examples to maintain same distribution original MNIST data:  [252.71466667 287.65866667 254.208      261.58933333 249.25866667\n",
            " 231.296      252.50133333 267.30666667 249.64266667 253.824     ]\n",
            "Training and testing data generated, and pre-processed.\n",
            "Training dataset size: 2560 images, testing dataset size: 260 images.\n",
            "Test Run Output:  [0.09997204 0.09983971 0.10041394 0.10040842 0.10016026 0.09975728\n",
            " 0.10072183 0.10047065 0.09918427 0.09907158]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:289: RuntimeWarning: overflow encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:290: RuntimeWarning: overflow encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Number:  1  done.\n",
            "Testing score after  1  epochs:\n",
            "Accuracy =  10.76923076923077 %\n",
            "Epoch Number:  2  done.\n",
            "Testing score after  2  epochs:\n",
            "Accuracy =  7.307692307692308 %\n",
            "Epoch Number:  3  done.\n",
            "Testing score after  3  epochs:\n",
            "Accuracy =  10.76923076923077 %\n",
            "Epoch Number:  4  done.\n",
            "Testing score after  4  epochs:\n",
            "Accuracy =  10.76923076923077 %\n",
            "Epoch Number:  5  done.\n",
            "Testing score after  5  epochs:\n",
            "Accuracy =  10.76923076923077 %\n",
            "Epoch Number:  6  done.\n",
            "Testing score after  6  epochs:\n",
            "Accuracy =  10.76923076923077 %\n",
            "Epoch Number:  7  done.\n",
            "Testing score after  7  epochs:\n",
            "Accuracy =  10.76923076923077 %\n",
            "Epoch Number:  8  done.\n",
            "Testing score after  8  epochs:\n",
            "Accuracy =  10.76923076923077 %\n",
            "Epoch Number:  9  done.\n",
            "Testing score after  9  epochs:\n",
            "Accuracy =  10.76923076923077 %\n",
            "Epoch Number:  10  done.\n",
            "Testing score after  10  epochs:\n",
            "Accuracy =  10.76923076923077 %\n",
            "Training Complete.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}