{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_via_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBZffMLgi1oK"
      },
      "source": [
        "# Neural Network Code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE0cLpGu50Vt"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Ensuring reproducibility in parameter initialisation\n",
        "np.random.seed(0)\n",
        "\n",
        "class NeuralNetwork():\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\t# Stores...\n",
        "\t\tself.input = None # the input data\n",
        "\t\tself.nodes = [] # the number of nodes in each layer\n",
        "\t\tself.weights = [] # the weights and biases (as tuples)\n",
        "\t\tself.regLossParam = 1e-3 # Regularization strength\n",
        "\n",
        "\n",
        "\tdef addInput(self, inputArray):\n",
        "\t\t\"\"\"\n",
        "\t\tSet the input data.\n",
        "\t\t\"\"\"\n",
        "\t\tself.input = inputArray\n",
        "\t\tself.nodes.append(len(inputArray[0]))\n",
        "\n",
        "\n",
        "\tdef layer(self, n):\n",
        "\t\t\"\"\"\n",
        "\t\tCreates a new layer.\n",
        "\t\tn - the number of nodes in the layer.\n",
        "\t\t\"\"\"\n",
        "\t\t# Number of nodes in previous layer\n",
        "\t\tnPrev = self.nodes[-1]\n",
        "\n",
        "\t\t# Initializing the weights and biases\n",
        "\t\tW = np.random.randn(nPrev, n) * math.sqrt(2.0/nPrev) # Recommended initialization method\n",
        "\t\tb = np.zeros((1, n))\n",
        "\n",
        "\t\t# Store them\n",
        "\t\tself.nodes.append(n)\n",
        "\t\tself.weights.append((W, b))\n",
        "\n",
        "\n",
        "\tdef activFunc(self, inputArray):\n",
        "\t\t\"\"\"\n",
        "\t\tThe activation function for the neurons in the network.\n",
        "\t\t\"\"\"\n",
        "\t\t# ReLU activation\n",
        "\t\treturn np.maximum(0, inputArray)\n",
        "\n",
        "\n",
        "\tdef hiddenLayerOutput(self, prevOut, W, b):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the output of a hidden layer.\n",
        "\t\tprevOut - Output from the previous layer\n",
        "\t\tW, b = Weight and bias of this layer\n",
        "\t\t\"\"\"\n",
        "\t\tlayerOutput = np.dot(prevOut, W) + b\n",
        "\t\treturn self.activFunc(layerOutput)\n",
        "\n",
        "\n",
        "\tdef finalOutput(self, prevOut, W, b):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the output of the final layer.\n",
        "\t\tSimilar to hiddenLayerOutput(), but without \n",
        "\t\tthe activation function.\n",
        "\t\t\"\"\"\n",
        "\t\tfinal_output = np.dot(prevOut, W) + b\n",
        "\t\treturn final_output\n",
        "\n",
        "\n",
        "\tdef getLayerOutput(self, n):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the output of the nth layer of the neural network.\n",
        "\t\tn = 0 is the input layer.\n",
        "\t\tn = len(self.weights) is the output layer.\n",
        "\t\t\"\"\"\n",
        "\t\tpenLayer = len(self.weights) - 1 # The penultimate layer\n",
        "\n",
        "\t\t# h stores the output of the current layer\n",
        "\t\th = self.input\n",
        "\n",
        "\t\t# Loop through the hidden layers\n",
        "\t\tfor i in range(min(n, penLayer)):\n",
        "\t\t\t(W, b) = self.weights[i]\n",
        "\t\t\th = self.hiddenLayerOutput(h, W, b)\n",
        "\n",
        "\t\t# Return the output\n",
        "\t\tif n <= penLayer:\n",
        "\t\t\treturn h\n",
        "\t\telse:\n",
        "\t\t\t(W, b) = self.weights[n-1]\n",
        "\t\t\treturn self.finalOutput(h, W, b)\n",
        "\n",
        "\n",
        "\tdef dataLoss(self, predResults, trueResults):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the data loss.\n",
        "\t\t\"\"\"\n",
        "\t\t# L2 loss\n",
        "\t\tloss = np.square(trueResults - predResults)\n",
        "\t\treturn loss/len(trueResults)\n",
        "\n",
        "\n",
        "\tdef regLoss(self):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the regularization loss.\n",
        "\t\t\"\"\"\n",
        "\t\tif self.regLossParam == 0:\n",
        "\t\t\treturn 0\n",
        "\t\telse:\n",
        "\t\t\tsquaredTotal = 0\n",
        "\t\t\tfor (W, _) in self.weights:\n",
        "\t\t\t\tsquaredTotal += np.sum(np.square(W))\n",
        "\n",
        "\t\t\tloss = 0.5 * self.regLossParam * squaredTotal\n",
        "\t\t\treturn loss\n",
        "\n",
        "\n",
        "\tdef backPropagation(self, trueResults):\n",
        "\t\t\"\"\"\n",
        "\t\tUpdates weights by carrying out backpropagation.\n",
        "\t\ttrueResults = the expected output from the neural network.\n",
        "\t\t\"\"\"\n",
        "\t\tpredResults = self.getLayerOutput(len(self.weights)) # The output from the neural network\n",
        "\n",
        "\t\t# Parameters\n",
        "\t\th = 0.001 * np.ones(predResults.shape) # For numerical calculation of the derivative\n",
        "\t\tlearningRate = 1e-5\n",
        "\n",
        "\t\t# The derivative of the loss function with respect to the output:\n",
        "\t\tdoutput = (self.dataLoss(predResults + h, trueResults) - self.dataLoss(predResults - h, trueResults))/(2*h)\n",
        "\n",
        "\t\tnPrev = len(self.weights) # Index keeping track of the previous layer\n",
        "\n",
        "\t\t# Loop over the layers\n",
        "\t\twhile nPrev - 1 >= 0:\n",
        "\n",
        "\t\t\t# If the current layer is not the output layer:\n",
        "\t\t\tif nPrev != len(self.weights):\n",
        "\t\t\t\t# Backprop into hidden layer\n",
        "\t\t\t\tdhidden = np.dot(doutput, W.T)\n",
        "\t\t\t\t# Backprop the ReLU non-linearity\n",
        "\t\t\t\tdhidden[prevLayer <= 0] = 0\n",
        "\t\t\telse:\n",
        "\t\t\t\tdhidden = doutput\n",
        "\n",
        "\t\t\tnPrev += -1\n",
        "\t\t\tprevLayer = self.getLayerOutput(nPrev) # The output of the previous layer\n",
        "\n",
        "\t\t\t# Find the gradients of the weights and biases\n",
        "\t\t\t(W, b) = self.weights[nPrev]\n",
        "\t\t\tdW = np.dot(prevLayer.T, dhidden)\n",
        "\t\t\tdb = np.sum(dhidden, axis=0, keepdims=True)\n",
        "\n",
        "\t\t\tdW += self.regLossParam * W # Regularization gradient\n",
        "\n",
        "\t\t\t# Update the weights and biases\n",
        "\t\t\tW += -learningRate * dW\n",
        "\t\t\tb += -learningRate * db\n",
        "\t\t\tself.weights[nPrev] = (W, b)\n",
        "\n",
        "\t\t\tdoutput = dhidden # Move to the previous layer\n",
        "\n",
        "\n",
        "\tdef train(self, Y, epochs):\n",
        "\t\t\"\"\"\n",
        "\t\tTrain the neural network.\n",
        "\t\tY = the expected results from the neural network.\n",
        "\t\tepochs = the number of times the neural network should 'learn'.\n",
        "\t\t\"\"\"\n",
        "\t\t# Run backPropagation() 'epochs' number of times.\n",
        "\t\tfor i in range(epochs):\n",
        "\t\t\tself.backPropagation(Y)\n",
        "\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\t\"\"\"\n",
        "\t\tMake predictions.\n",
        "\t\tX = input data for the neural network to predict.\n",
        "\t\t\"\"\"\n",
        "\t\tself.input = X\n",
        "\t\treturn self.getLayerOutput(len(self.weights))\n",
        "  \n",
        "\n",
        "    def accuracy(self, ypred, ytrue):\n",
        "        \"\"\"\n",
        "        Function that takes in test data and results and calculates the accuracy of the Network.\n",
        "        \"\"\"\n",
        "        cor = 0\n",
        "        correct = 0\n",
        "        ypred = np.array(ypred)\n",
        "        ytrue = np.array(ytrue)\n",
        "        for i in range(len(ytrue)):\n",
        "            correct = np.argmax(ytrue[i])\n",
        "            if (np.argmax(ypred[i]) == correct):\n",
        "                cor += 1\n",
        "        cor /= len(ytrue)\n",
        "        print('Accuracy = ', cor*100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt4295EVFcJl",
        "outputId": "3859cb5a-19ef-4f5f-ab28-c95138b8d26e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "from network import NeuralNetwork\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# (x, y) - Training dataset, (x2, y2) - Testing dataset\n",
        "# We do not use the testing dataset because we would like to use a test-train split in the testing dataset for a common \n",
        "# random_state so that we can control of reproducibility of results.\n",
        "(x, y), (x2, y2) = datasets.mnist.load_data()\n",
        "\n",
        "# Making sure that the values are float so that we can get decimal points after division\n",
        "x = x.astype('float32')\n",
        "\n",
        "# Normalise data\n",
        "x = ((x/255) - 0.5)\n",
        "\n",
        "# Flatten data since we are using a neural network, and not a CNN \n",
        "xf = []\n",
        "for i in range(len(x)):\n",
        "    xf.append(x[i].flatten())\n",
        "xf=np.array(xf)\n",
        "\n",
        "# Train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(xf, y, train_size=0.80, test_size=0.20, random_state=69)\n",
        "\n",
        "# Since we test the NN with MNIST data, we write the target output in the required format before sent to training/testing\n",
        "results_train = np.zeros((len(y_train),10))  \n",
        "for i in range(len(y_train)):\n",
        "    results_train[i,y_train[i]] = 1\n",
        "results_test = np.zeros((len(y_test),10))  \n",
        "for i in range(len(y_test)):\n",
        "    results_train[i,y_test[i]] = 1\n",
        "\n",
        "# Create instance of NeuralNetwork\n",
        "model = NeuralNetwork()\n",
        "\n",
        "model.addInput(xf) # Input layer\n",
        "model.layer(1000) # Hidden layer 1\n",
        "model.layer(1000) # Hidden layer 2\n",
        "model.layer(10) # Output layer\n",
        "\n",
        "# Train the model (y - true labels, epochs)\n",
        "model.train(y_test, 10)\n",
        "\n",
        "# Get predictions for test data\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "model.accuracy(predictions, y_test)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-VAzGMf5Bf2"
      },
      "source": [
        "======================================================================================\n",
        "# IGNORE CODE BELOW THIS, THE CODES BELOW ARE FOR CNN\n",
        "======================================================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cdZEcyqisAO"
      },
      "source": [
        "# SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf9wLuEnirl_"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "class ConvNet():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.inputImg = None  # The input data\n",
        "        self.strides = []  # The stride length of each layer for convolution\n",
        "        self.recfield = []  # The receptive field in each layer for convolution\n",
        "        self.lengths = []\n",
        "        self.widths = []\n",
        "        self.depths = []\n",
        "        self.weights = []  # The weights for convolution filters\n",
        "        self.node = 10  # The number of nodes in the output layer\n",
        "        self.track = []  # Keeps track of layer order, i.e Conv./Pooling/FC\n",
        "        self.learning_rate = 0.005\n",
        "        self.fc_weights = []\n",
        "        self.output_fc = []\n",
        "\n",
        "    def addInput(self, inpImage):  # Assign the input image\n",
        "        inpImage = np.array(inpImage)\n",
        "        self.inputImg = inpImage\n",
        "        if(len(inpImage.shape) < 3):\n",
        "            num3 = 1\n",
        "            numrows = inpImage.shape[0]\n",
        "            numcols = inpImage.shape[1]\n",
        "        else:\n",
        "            num3 = inpImage.shape[0]\n",
        "            numrows = inpImage.shape[1]\n",
        "            numcols = inpImage.shape[2]\n",
        "\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "\n",
        "    def cvolume(self, s, r, f):\n",
        "        \"\"\"\n",
        "        Creates a new Conv. volume.\n",
        "        s - stride length for convolving the previous layer to create this new volume\n",
        "        r - receptive field for convolving the previous layer to create this new volume\n",
        "        f - number of filters, or in other words, the depth of this new volume\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Initializing the weights\n",
        "        W = []\n",
        "        #b = np.zeros((1, stre))\n",
        "        for i in range(f):\n",
        "            W.append(np.random.randn(prevd, r, r) / (r*r))\n",
        "\n",
        "        W = np.array(W)\n",
        "\n",
        "        # The dimensions of the layer after convolution with the above weight array\n",
        "        numrows = (prevw - r)/s + 1\n",
        "        numcols = (prevl - r)/s + 1\n",
        "        num3 = f\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(W)\n",
        "        self.strides.append(s)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "        self.track.append('c')\n",
        "\n",
        "    def pmaxvolume(self, r):\n",
        "        \"\"\"\n",
        "        Creates a new max pooling layer.\n",
        "        r - the receptive field around which the max value has to be taken.\n",
        "            E.g - If r = 2, max pooling is done withing 2x2 sub matrices.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(None)\n",
        "        self.strides.append(r)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(prevl/r)\n",
        "        self.widths.append(prevw/r)\n",
        "        self.depths.append(prevd)\n",
        "        self.track.append('p')\n",
        "\n",
        "    def FCLayer(self, n_nodes):\n",
        "        \"\"\"\n",
        "        Creates a fully connected layer\n",
        "        n - the no.of nodes in the output layer.\n",
        "        input_fc - the input to the fully connected layer.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = int(self.depths[-1])\n",
        "        prevw = int(self.widths[-1])\n",
        "        prevl = int(self.lengths[-1])\n",
        "\n",
        "        # flatten the input\n",
        "        input_fc = np.zeros((prevd, prevw, prevl))\n",
        "        input_fc = input_fc.flatten()\n",
        "        len_input_fc = len(input_fc)\n",
        "\n",
        "        # Initialise the weights and biases for the FC layer\n",
        "        self.fc_weights = np.random.randn(len_input_fc, n_nodes) / (len_input_fc)\n",
        "        #self.fc_bias = np.zeros(n_nodes)\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(self.fc_weights)\n",
        "        self.strides.append(0)\n",
        "        self.recfield.append(0)\n",
        "        self.lengths.append(1)\n",
        "        self.widths.append(len_input_fc)\n",
        "        self.depths.append(1)\n",
        "        self.track.append('f')\n",
        "        self.node = n_nodes\n",
        "\n",
        "    def activFunc(self, inputArray):\n",
        "        \"\"\"\n",
        "        The activation function for the neurons in the network.\n",
        "        \"\"\"\n",
        "        # ReLU activation\n",
        "        \n",
        "        return np.maximum(0, inputArray)\n",
        "\n",
        "    def dataLoss(self, predResults, trueResults):\n",
        "        \"\"\"\n",
        "        Returns the data loss. Cross-Entropy loss function (Softmax Classifier).\n",
        "        \"\"\"\n",
        "        # L2 loss\n",
        "        loss = 0\n",
        "        sum = 0\n",
        "        for i in range(len(predResults)):\n",
        "            sum += math.exp(predResults[i])\n",
        "        correct = np.argmax(trueResults)\n",
        "        loss = (-1)*(math.log((math.exp(predResults[correct]))/sum))\n",
        "        return loss\n",
        "\n",
        "    def ConvOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Convolutional Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        if(len(W.shape) < 4):\n",
        "            f = 1\n",
        "        else:\n",
        "            f = W.shape[0]\n",
        "\n",
        "        for i in range(f):  # Run loop to create f-filters\n",
        "            for k in range(w):  # Convolve around width\n",
        "                for m in range(l):  # Convolve around length\n",
        "                    # for j in range(d):   #Run over entire depth of prevOut volume\n",
        "                    volOutput[i][k][m] += np.sum(np.multiply(W[i][:][:][:], prevOut[:, k*s: k*s + r, m*s: m*s + r])[:, :, :])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def PoolOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Pooling Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer, since there is no Weight matrix for MaxPooling, it is None\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    volOutput[j][k][m] = np.amax(prevOut[j, k*r: (k + 1)*r, m*r: (m + 1)*r])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def FCOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Implements forward pass for the FC layer. Uses a softmax Classifier.\n",
        "        n_nodes - the no.of nodes in the fully connected layer. \n",
        "        \"\"\"\n",
        "        # flatten the input\n",
        "        prevOut = prevOut.flatten()\n",
        "        #len_input_fc = len(prevOut)\n",
        "\n",
        "        totals = np.dot(prevOut, W)  # + self.fc_bias\n",
        "        # Softmax\n",
        "        exp_totals = np.exp(totals)\n",
        "\n",
        "        # Output from the FC layer\n",
        "        self.output_fc = exp_totals / (np.sum(exp_totals, axis=0))\n",
        "        \n",
        "        return self.output_fc\n",
        "\n",
        "    def getVolumeOutput(self, n):\n",
        "        \"\"\"\n",
        "        Returns the output of the nth volume of the ConvNet.\n",
        "        \"\"\"\n",
        "        penLayer = len(self.weights) - 1  # The penultimate volume\n",
        "\n",
        "        # h stores the output of the current layer\n",
        "        h = np.array(self.inputImg)\n",
        "\n",
        "        # Loop through the hidden layers\n",
        "        for i in range(min(n, penLayer)):\n",
        "            W = self.weights[i]\n",
        "            s = self.strides[i]\n",
        "            r = self.recfield[i]\n",
        "            d = self.depths[i+1]\n",
        "            w = self.widths[i+1]\n",
        "            l = self.lengths[i+1]\n",
        "            if (self.track[i] == 'c'):\n",
        "                h = self.activFunc(self.ConvOutput(h, W, s, r, d, w, l))\n",
        "            elif (self.track[i] == 'p'):\n",
        "                h = self.PoolOutput(h, W, s, r, d, w, l)\n",
        "            else:\n",
        "                h = self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "        # Return the output\n",
        "        if n <= penLayer:\n",
        "            return h\n",
        "        else:\n",
        "            W = self.weights[n-1]\n",
        "            s = self.strides[n-1]\n",
        "            r = self.recfield[n-1]\n",
        "            d = self.depths[n]\n",
        "            w = self.widths[n]\n",
        "            l = self.lengths[n]\n",
        "            return self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "    def FCGD(self, index, trueResults):  # FC layer Gradient Descent\n",
        "        input_fc = self.getVolumeOutput(index - 1)\n",
        "\n",
        "        # Store the shape of input before flattening (to be used for backpropagation)\n",
        "        input_fc_shape = input_fc.shape\n",
        "\n",
        "        # Flatten the input\n",
        "        input_fc = input_fc.flatten()\n",
        "        \n",
        "        # Get the weights and the totals of the fixed layer\n",
        "        W = self.weights[index - 1]\n",
        "        totals = np.dot(input_fc, W)\n",
        "        \n",
        "        # Calculating d_L_d_out\n",
        "        correct = np.argmax(trueResults)\n",
        "        d_L_d_out = np.zeros_like(self.output_fc)\n",
        "        d_L_d_out[correct] = -1 / (self.output_fc[correct])\n",
        "        \n",
        "        # Calculating d_out_d_t\n",
        "        exp_totals = np.exp(totals)\n",
        "        sum_exp_totals = np.sum(exp_totals)\n",
        "        \n",
        "        d_out_d_t = np.zeros_like(self.output_fc)\n",
        "        d_out_d_t = -exp_totals[correct] * (exp_totals / (sum_exp_totals ** 2))\n",
        "        d_out_d_t[correct] = exp_totals[correct] * ((sum_exp_totals - exp_totals[correct])/(sum_exp_totals ** 2))\n",
        "                \n",
        "        # Other necessary gradients\n",
        "        d_t_d_w = input_fc\n",
        "        d_t_d_inputs = W\n",
        "        d_L_d_t = d_L_d_out * d_out_d_t\n",
        "\n",
        "        # Gradients of loss wrt Weights of FC layer and Input of FC layers\n",
        "        # d_L_d_t.shape = (n_nodes,1)\n",
        "        # Adding appropriate axes to d_L_d_t and d_t_d_w(same as input_fc) for . product\n",
        "        d_L_d_w = np.dot(d_t_d_w[np.newaxis].T, d_L_d_t[np.newaxis])\n",
        "\n",
        "        # d_L_d_inputs should have the dimensions of input_fc\n",
        "        d_L_d_inputs = np.dot(d_t_d_inputs, d_L_d_t)\n",
        "\n",
        "        # The dimension of d_L_d_inputs is (len_input_fc,), so, changing the shape so it can be given to maxpool's backprop.\n",
        "        d_L_d_inputs_final = d_L_d_inputs.reshape(input_fc_shape)\n",
        "\n",
        "        W -= self.learning_rate * d_L_d_w\n",
        "        self.weights[index - 1] = W\n",
        "        \n",
        "        return d_L_d_inputs_final\n",
        "\n",
        "    def PoolGD(self, dLdOut, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates gradients through the MaxPooling layer\n",
        "        dLdOut is the differential of Loss wrt the Output where Output here refers to the output of the MaxPooling layer\n",
        "        This function thus finds dLdI which is the differential of Loss wrt the Input where Input here refers to input to MaxPool layer.\n",
        "        \"\"\"\n",
        "        input_vol = self.getVolumeOutput(index - 1)\n",
        "        s = self.strides[index - 1]\n",
        "        r = self.recfield[index - 1]\n",
        "        d = dLdOut.shape[0]\n",
        "        w = dLdOut.shape[1]\n",
        "        l = dLdOut.shape[2]\n",
        "\n",
        "        # Convert the numbers to int, as the for loops below will report errors if this is not done\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "\n",
        "        # Keep track of the depth and spatial indices of where the maximum element is, in the sub arrays taken for pooling\n",
        "        d_ind = []\n",
        "        spatial_ind = []\n",
        "        # Keep track of which sub array is being taken for max pooling\n",
        "        track_w = []\n",
        "        track_l = []\n",
        "\n",
        "        dLdI = np.zeros((int(self.depths[index - 1]), int(self.lengths[index - 1]), int(self.widths[index - 1])))\n",
        "        replace = dLdOut.flatten()\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    spatial_ind.append(np.where(input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r] == input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r].max()))\n",
        "                    track_l.append(m)\n",
        "                    track_w.append(k)\n",
        "                    d_ind.append(j)\n",
        "\n",
        "        # Initialise correct values in dLdI array\n",
        "        for i in range(len(replace)):\n",
        "            width = spatial_ind[i][0][0]  # Note the (width) spatial index of the maximum element of the sub array\n",
        "            width += track_w[i]*r  # Add the (width) location depending on which sub array was taken for max pooling\n",
        "            length = spatial_ind[i][1][0]  # Note the (length) spatial index of the maximum element of the sub array\n",
        "            length += track_l[i]*r  # Add the (length) location depending on which sub array was taken for max pooling\n",
        "            depth = d_ind[i]  # Note the depth index of the maximum element of the sub array\n",
        "            dLdI[depth][width][length] = replace[i]\n",
        "        \n",
        "        return dLdI\n",
        "\n",
        "    # Helper functions for convBackProp()\n",
        "    def convolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the convoluted output convFilter on inputLayer.\n",
        "        Both are two dimensional matrices square matrices.\n",
        "        inputLayer - (n, n)\n",
        "        convFilter - (f, f)\n",
        "        \"\"\"\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Defining the shape of the output matrix\n",
        "        l = (n-f) + 1\n",
        "        output_matrix = np.zeros((l, l))\n",
        "        s = 1\n",
        "\n",
        "        # Convolving\n",
        "        for row in range(l):\n",
        "            for col in range(l):\n",
        "                output_matrix[row][col] = np.sum(np.multiply(inputLayer[row:row+f, col:col+f], convFilter))\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def fullConvolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the full convoluted output of convFilter on inputLayer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Creating padding for the inputLayer matrix\n",
        "        padding = f - 1\n",
        "        new_dim = n + 2*padding\n",
        "\n",
        "        padded_input = np.zeros([new_dim, new_dim])\n",
        "        padded_input[padding:new_dim - padding,padding:new_dim - padding] = inputLayer\n",
        "\n",
        "        # Now convolve padded_input with convFilter\n",
        "        output_matrix = self.convolve(padded_input, convFilter)\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def rotate180(self, matrix):\n",
        "        \"\"\"\n",
        "        Rotates matrix by 180 degrees in the plane.\n",
        "        Takes only two dimensional matrices.\n",
        "        \"\"\"\n",
        "        return np.rot90(matrix, 2)\n",
        "\n",
        "    def ConvGD(self, dLdoutput, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates through a convolutional layer.\n",
        "        index = index of the current layer\n",
        "        dLdoutput = Gradient of the loss function wrt the output of the current layer (channel, row, col)\n",
        "        Returns dLdinput.\n",
        "        \"\"\"\n",
        "        X = self.getVolumeOutput(index-1)  # Input to the current layer (channel, row, col)\n",
        "        # Weights of the current layer (numFilter, channel, row, col)\n",
        "        W = self.weights[index - 1]\n",
        "\n",
        "        dLdX = np.empty(X.shape)\n",
        "        dLdW = np.empty(W.shape)\n",
        "\n",
        "        dLdout = np.copy(dLdoutput)\n",
        "        dLdout[dLdout < 0] = 0\n",
        "\n",
        "        # Loop over the filters\n",
        "        numFilters = W.shape[0]\n",
        "\n",
        "        for fil_ter in range(numFilters):\n",
        "            filter_output = dLdout[fil_ter]\n",
        "\n",
        "            # Loop over the channels\n",
        "            for channel in range(W.shape[1]):\n",
        "                filter_layer = W[fil_ter][channel]\n",
        "                dWLayer = self.convolve(X[channel], filter_output)\n",
        "                dXLayer = self.rotate180(self.fullConvolve(self.rotate180(filter_layer), filter_output))\n",
        "\n",
        "                # Combine these and return in arrays\n",
        "                dLdW[fil_ter][channel] = dWLayer\n",
        "                dLdX[channel] = dXLayer\n",
        "\n",
        "        W -= self.learning_rate * dLdW\n",
        "        self.weights[index - 1] = W\n",
        "        return dLdX\n",
        "\n",
        "    def backPropagation(self, input, trueResults):\n",
        "        \"\"\"\n",
        "        Updates weights by carrying out backpropagation.\n",
        "        trueResults = the expected output from the neural network.\n",
        "        \"\"\"\n",
        "        for i in range(len(input)):\n",
        "            self.inputImg = np.array(input[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "            # Called once so that all weights are initialised, just in case if not done before\n",
        "            out = self.getVolumeOutput(len(self.weights))\n",
        "            # Index keeping track of the previous layer\n",
        "            nPrev = len(self.weights)\n",
        "            doutput = self.FCGD(nPrev, trueResults[i])\n",
        "            nPrev -= 1\n",
        "\n",
        "            # Loop over the layers\n",
        "            while nPrev - 1 >= 0:\n",
        "                if(self.track[nPrev - 1] == 'p'):\n",
        "                    dhidden = self.PoolGD(doutput, nPrev)\n",
        "                else:\n",
        "                    dhidden = self.ConvGD(doutput, nPrev)\n",
        "                doutput = dhidden  # Move to the previous layer\n",
        "                nPrev -= 1\n",
        "\n",
        "    def train(self, input, Y, epochs):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "        Y = the expected results from the neural network.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(input, Y)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "    def accuracy(self, X, Y):\n",
        "        \"\"\"\n",
        "        Function that takes in test data and results and calculates the accuracy of the Network.\n",
        "        \"\"\"\n",
        "        y = []\n",
        "        cor = 0\n",
        "        correct = 0\n",
        "        for i in range(len(X)):\n",
        "            self.inputImg = np.array(X[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                y.append(self.getVolumeOutput(len(self.weights)))\n",
        "        Y = np.array(Y)\n",
        "        y = np.array(y)\n",
        "        if (np.max(y) == 0):\n",
        "            y /= 1.0\n",
        "        else:\n",
        "            y /= np.max(y)\n",
        "        for i in range(len(Y)):\n",
        "            correct = np.argmax(Y[i])\n",
        "            if (np.argmax(y[i]) == correct):\n",
        "                cor += 1\n",
        "        cor /= len(Y)\n",
        "        print('Accuracy = ', cor*100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDMgYlVrixod"
      },
      "source": [
        "# Code Run for SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LQi7bZjiznD",
        "outputId": "e048a379-ec6e-4ab6-d2ae-2d9a93aac5d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(x, y), (x2, y2) = datasets.mnist.load_data()\n",
        "# Normalise data\n",
        "x = ((x/255) - 0.5)\n",
        "x2 = ((x2/255) - 0.5)\n",
        "\n",
        "# Create instance of NeuralNetwork\n",
        "model = ConvNet()\n",
        "X = np.random.randn(1,28,28)\n",
        "model.addInput(X) # Input layer\n",
        "model.cvolume(1,3,10) # Add Convolutional volume (stride length, receptive field, filters)\n",
        "model.pmaxvolume(2) # Add Pooling layer (receptive field)\n",
        "model.FCLayer(10)  # Add FC Layer (number of classifiers)\n",
        "# Get final output layer. It is advised to run it once before training, so that all variables are initialised.\n",
        "print('Test Run Output: ',model.getVolumeOutput(3))\n",
        "\n",
        "#Since we test the CNN with MNIST data, we write the target output in the required format before sent to training/testing.\n",
        "results = np.zeros((len(y),10))  \n",
        "for i in range(len(y)):\n",
        "    results[i,y[i]] = 1\n",
        "\n",
        "model.train(x[0:2000], results[0:2000], 5) # Train model with 2000 images for 5 epochs\n",
        "model.accuracy(x[2000:4000], results[2000:4000])  # Predict accuracy using test data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Run Output:  [0.0994069  0.09899433 0.10095154 0.09940226 0.10014923 0.10128744\n",
            " 0.10030085 0.10103834 0.09895257 0.09951654]\n",
            "Epoch Number:  1  done.\n",
            "Epoch Number:  2  done.\n",
            "Epoch Number:  3  done.\n",
            "Epoch Number:  4  done.\n",
            "Epoch Number:  5  done.\n",
            "Training Complete.\n",
            "Accuracy =  80.45 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELeOGPeZ64oz"
      },
      "source": [
        "# Adam Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbv_1VG2Eha3"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Ensuring reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "class ConvNet():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.inputImg = None  # The input data\n",
        "        self.strides = []  # The stride length of each layer for convolution\n",
        "        self.recfield = []  # The receptive field in each layer for convolution\n",
        "        self.lengths = []\n",
        "        self.widths = []\n",
        "        self.depths = []\n",
        "        self.weights = []  # The weights for convolution filters\n",
        "        self.node = 10  # The number of nodes in the output layer\n",
        "        self.track = []  # Keeps track of layer order, i.e Conv./Pooling/FC\n",
        "        self.learning_rate = 0.005\n",
        "        self.fc_weights = []\n",
        "        self.output_fc = []\n",
        "\n",
        "        # Adam optimization\n",
        "        self.adam_m = []\n",
        "        self.adam_v = []\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.eps = 1e-4\n",
        "        self.count = 1 # Keeps track of iteration number needed for optimisation\n",
        "\n",
        "    def addInput(self, inpImage):  # Assign the input image\n",
        "        inpImage = np.array(inpImage)\n",
        "        self.inputImg = inpImage\n",
        "        if(len(inpImage.shape) < 3):\n",
        "            num3 = 1\n",
        "            numrows = inpImage.shape[0]\n",
        "            numcols = inpImage.shape[1]\n",
        "        else:\n",
        "            num3 = inpImage.shape[0]\n",
        "            numrows = inpImage.shape[1]\n",
        "            numcols = inpImage.shape[2]\n",
        "\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "\n",
        "    def cvolume(self, s, r, f):\n",
        "        \"\"\"\n",
        "        Creates a new Conv. volume.\n",
        "        s - stride length for convolving the previous layer to create this new volume\n",
        "        r - receptive field for convolving the previous layer to create this new volume\n",
        "        f - number of filters, or in other words, the depth of this new volume\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Initializing the weights\n",
        "        W = []\n",
        "\n",
        "        # For Adam optimization\n",
        "        M = []\n",
        "        V = []\n",
        "\n",
        "        #b = np.zeros((1, stre))\n",
        "        for i in range(f):\n",
        "            W.append(np.random.randn(prevd, r, r) / (r*r))\n",
        "            M.append(np.zeros((prevd, r, r)))\n",
        "            V.append(np.zeros((prevd, r, r)))\n",
        "\n",
        "        W = np.array(W)\n",
        "        M = np.array(M)\n",
        "        V = np.array(V)\n",
        "\n",
        "        # The dimensions of the layer after convolution with the above weight array\n",
        "        numrows = (prevw - r)/s + 1\n",
        "        numcols = (prevl - r)/s + 1\n",
        "        num3 = f\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(W)\n",
        "        self.strides.append(s)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(numcols)\n",
        "        self.widths.append(numrows)\n",
        "        self.depths.append(num3)\n",
        "        self.track.append('c')\n",
        "\n",
        "        self.adam_m.append(M)\n",
        "        self.adam_v.append(V)\n",
        "\n",
        "    def pmaxvolume(self, r):\n",
        "        \"\"\"\n",
        "        Creates a new max pooling layer.\n",
        "        r - the receptive field around which the max value has to be taken.\n",
        "            E.g - If r = 2, max pooling is done withing 2x2 sub matrices.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = self.depths[-1]\n",
        "        prevw = self.widths[-1]\n",
        "        prevl = self.lengths[-1]\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(None)\n",
        "        self.strides.append(r)\n",
        "        self.recfield.append(r)\n",
        "        self.lengths.append(prevl/r)\n",
        "        self.widths.append(prevw/r)\n",
        "        self.depths.append(prevd)\n",
        "        self.track.append('p')\n",
        "\n",
        "        self.adam_m.append(None)\n",
        "        self.adam_v.append(None)\n",
        "\n",
        "    def FCLayer(self, n_nodes):\n",
        "        \"\"\"\n",
        "        Creates a fully connected layer\n",
        "        n - the no.of nodes in the output layer.\n",
        "        input_fc - the input to the fully connected layer.\n",
        "        \"\"\"\n",
        "        # Depth, width and length of previous layer\n",
        "        prevd = int(self.depths[-1])\n",
        "        prevw = int(self.widths[-1])\n",
        "        prevl = int(self.lengths[-1])\n",
        "\n",
        "        # flatten the input\n",
        "        input_fc = np.zeros((prevd, prevw, prevl))\n",
        "        input_fc = input_fc.flatten()\n",
        "        len_input_fc = len(input_fc)\n",
        "\n",
        "        # Initialise the weights and biases for the FC layer\n",
        "        self.fc_weights = np.random.randn(len_input_fc, n_nodes) / (len_input_fc)\n",
        "        #self.fc_bias = np.zeros(n_nodes)\n",
        "\n",
        "        # Store them\n",
        "        self.weights.append(self.fc_weights)\n",
        "        self.strides.append(0)\n",
        "        self.recfield.append(0)\n",
        "        self.lengths.append(1)\n",
        "        self.widths.append(len_input_fc)\n",
        "        self.depths.append(1)\n",
        "        self.track.append('f')\n",
        "        self.node = n_nodes\n",
        "\n",
        "        self.adam_m.append(None)\n",
        "        self.adam_v.append(None)\n",
        "\n",
        "    def activFunc(self, inputArray):\n",
        "        \"\"\"\n",
        "        The activation function for the neurons in the network.\n",
        "        \"\"\"\n",
        "        # ReLU activation\n",
        "        return np.maximum(0, inputArray)\n",
        "\n",
        "    def dataLoss(self, predResults, trueResults):\n",
        "        \"\"\"\n",
        "        Returns the data loss. Cross-Entropy loss function (Softmax Classifier).\n",
        "        \"\"\"\n",
        "        # Softmax\n",
        "        loss = 0\n",
        "        sum = 0\n",
        "        for i in range(len(predResults)):\n",
        "            sum += math.exp(predResults[i])\n",
        "        correct = np.argmax(trueResults)\n",
        "        loss = (-1)*(math.log((math.exp(predResults[correct]))/sum))\n",
        "        return loss\n",
        "\n",
        "    def ConvOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Convolutional Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        if(len(W.shape) < 4):\n",
        "            f = 1\n",
        "        else:\n",
        "            f = W.shape[0]\n",
        "\n",
        "        for i in range(f):  # Run loop to create f-filters\n",
        "            for k in range(w):  # Convolve around width\n",
        "                for m in range(l):  # Convolve around length\n",
        "                    # for j in range(d):   #Run over entire depth of prevOut volume\n",
        "                    volOutput[i][k][m] += np.sum(np.multiply(W[i][:][:][:], prevOut[:, k*s: k*s + r, m*s: m*s + r])[:, :, :])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def PoolOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Returns the output of the Pooling Layer.\n",
        "        prevOut - Output from the previous layer\n",
        "        W = Weight of this layer, since there is no Weight matrix for MaxPooling, it is None\n",
        "        \"\"\"\n",
        "        prevOut = np.array(prevOut)\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "        volOutput = np.zeros((d, w, l))\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    volOutput[j][k][m] = np.amax(prevOut[j, k*r: (k + 1)*r, m*r: (m + 1)*r])\n",
        "\n",
        "        volOutput = np.array(volOutput)\n",
        "        return volOutput\n",
        "\n",
        "    def FCOutput(self, prevOut, W, s, r, d, w, l):\n",
        "        \"\"\"\n",
        "        Implements forward pass for the FC layer. Uses a softmax Classifier.\n",
        "        n_nodes - the no.of nodes in the fully connected layer. \n",
        "        \"\"\"\n",
        "        # flatten the input\n",
        "        prevOut = prevOut.flatten()\n",
        "        #len_input_fc = len(prevOut)\n",
        "\n",
        "        totals = np.dot(prevOut, W)  # + self.fc_bias\n",
        "        # Softmax\n",
        "        exp_totals = np.exp(totals)\n",
        "\n",
        "        # Output from the FC layer\n",
        "        self.output_fc = exp_totals / (np.sum(exp_totals, axis=0))\n",
        "        return self.output_fc\n",
        "\n",
        "    def getVolumeOutput(self, n):\n",
        "        \"\"\"\n",
        "        Returns the output of the nth volume of the ConvNet.\n",
        "        \"\"\"\n",
        "        penLayer = len(self.weights) - 1  # The penultimate volume\n",
        "\n",
        "        # h stores the output of the current layer\n",
        "        h = np.array(self.inputImg)\n",
        "\n",
        "        # Loop through the hidden layers\n",
        "        for i in range(min(n, penLayer)):\n",
        "            W = self.weights[i]\n",
        "            s = self.strides[i]\n",
        "            r = self.recfield[i]\n",
        "            d = self.depths[i+1]\n",
        "            w = self.widths[i+1]\n",
        "            l = self.lengths[i+1]\n",
        "            if (self.track[i] == 'c'):\n",
        "                h = self.activFunc(self.ConvOutput(h, W, s, r, d, w, l))\n",
        "            elif (self.track[i] == 'p'):\n",
        "                h = self.PoolOutput(h, W, s, r, d, w, l)\n",
        "            else:\n",
        "                h = self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "        # Return the output\n",
        "        if n <= penLayer:\n",
        "            return h\n",
        "        else:\n",
        "            W = self.weights[n-1]\n",
        "            s = self.strides[n-1]\n",
        "            r = self.recfield[n-1]\n",
        "            d = self.depths[n]\n",
        "            w = self.widths[n]\n",
        "            l = self.lengths[n]\n",
        "            return self.FCOutput(h, W, s, r, d, w, l)\n",
        "\n",
        "    def FCGD(self, index, trueResults):  # FC layer Gradient Descent\n",
        "        input_fc = self.getVolumeOutput(index - 1)\n",
        "\n",
        "        # Store the shape of input before flattening (to be used for backpropagation)\n",
        "        input_fc_shape = input_fc.shape\n",
        "\n",
        "        # Flatten the input\n",
        "        input_fc = input_fc.flatten()\n",
        "\n",
        "        # Get the weights and the totals of the fixed layer\n",
        "        W = self.weights[index - 1]\n",
        "        totals = np.dot(input_fc, W)\n",
        "\n",
        "        # Calculating d_L_d_out\n",
        "        correct = np.argmax(trueResults)\n",
        "        d_L_d_out = np.zeros_like(self.output_fc)\n",
        "        d_L_d_out[correct] = -1 / (self.output_fc[correct])\n",
        "\n",
        "        # Calculating d_out_d_t\n",
        "        exp_totals = np.exp(totals)\n",
        "        sum_exp_totals = np.sum(exp_totals)\n",
        "\n",
        "        d_out_d_t = np.zeros_like(self.output_fc)\n",
        "        d_out_d_t = -exp_totals[correct] * (exp_totals / (sum_exp_totals ** 2))\n",
        "        d_out_d_t[correct] = exp_totals[correct] * ((sum_exp_totals - exp_totals[correct])/(sum_exp_totals ** 2))\n",
        "\n",
        "        # Other necessary gradients\n",
        "        d_t_d_w = input_fc\n",
        "        d_t_d_inputs = W\n",
        "        d_L_d_t = d_L_d_out * d_out_d_t\n",
        "\n",
        "        # Gradients of loss wrt Weights of FC layer and Input of FC layers\n",
        "        # d_L_d_t.shape = (n_nodes,1)\n",
        "        # Adding appropriate axes to d_L_d_t and d_t_d_w(same as input_fc) for . product\n",
        "        d_L_d_w = np.dot(d_t_d_w[np.newaxis].T, d_L_d_t[np.newaxis])\n",
        "\n",
        "        # d_L_d_inputs should have the dimensions of input_fc\n",
        "        d_L_d_inputs = np.dot(d_t_d_inputs, d_L_d_t)\n",
        "\n",
        "        # The dimension of d_L_d_inputs is (len_input_fc,), so, changing the shape so it can be given to maxpool's backprop.\n",
        "        d_L_d_inputs_final = d_L_d_inputs.reshape(input_fc_shape)\n",
        "\n",
        "        W -= self.learning_rate * d_L_d_w\n",
        "        self.weights[index - 1] = W\n",
        "\n",
        "        return d_L_d_inputs_final\n",
        "\n",
        "    def PoolGD(self, dLdOut, index):\n",
        "        \"\"\"\n",
        "        Function that backpropagates gradients through the MaxPooling layer\n",
        "        dLdOut is the differential of Loss wrt the Output where Output here refers to the output of the MaxPooling layer\n",
        "        This function thus finds dLdI which is the differential of Loss wrt the Input where Input here refers to input to MaxPool layer.\n",
        "        \"\"\"\n",
        "        input_vol = self.getVolumeOutput(index - 1)\n",
        "        s = self.strides[index - 1]\n",
        "        r = self.recfield[index - 1]\n",
        "        d = dLdOut.shape[0]\n",
        "        w = dLdOut.shape[1]\n",
        "        l = dLdOut.shape[2]\n",
        "\n",
        "        # Convert the numbers to int, as the for loops below will report errors if this is not done\n",
        "        d = int(d)\n",
        "        w = int(w)\n",
        "        l = int(l)\n",
        "\n",
        "        # Keep track of the depth and spatial indices of where the maximum element is, in the sub arrays taken for pooling\n",
        "        d_ind = []\n",
        "        spatial_ind = []\n",
        "        # Keep track of which sub array is being taken for max pooling\n",
        "        track_w = []\n",
        "        track_l = []\n",
        "\n",
        "        dLdI = np.zeros((int(self.depths[index - 1]), int(self.lengths[index - 1]), int(self.widths[index - 1])))\n",
        "        replace = dLdOut.flatten()\n",
        "        for j in range(d):\n",
        "            for k in range(w):\n",
        "                for m in range(l):\n",
        "                    spatial_ind.append(np.where(input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r] == input_vol[j, k*r: (k + 1)*r, m*r: (m + 1)*r].max()))\n",
        "                    track_l.append(m)\n",
        "                    track_w.append(k)\n",
        "                    d_ind.append(j)\n",
        "\n",
        "        # Initialise correct values in dLdI array\n",
        "        for i in range(len(replace)):\n",
        "            width = spatial_ind[i][0][0]  # Note the (width) spatial index of the maximum element of the sub array\n",
        "            width += track_w[i]*r  # Add the (width) location depending on which sub array was taken for max pooling\n",
        "            length = spatial_ind[i][1][0]  # Note the (length) spatial index of the maximum element of the sub array\n",
        "            length += track_l[i]*r  # Add the (length) location depending on which sub array was taken for max pooling\n",
        "            depth = d_ind[i]  # Note the depth index of the maximum element of the sub array\n",
        "            dLdI[depth][width][length] = replace[i]\n",
        "\n",
        "        return dLdI\n",
        "\n",
        "    # Helper functions for convBackProp()\n",
        "    def convolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the convoluted output convFilter on inputLayer.\n",
        "        Both are two dimensional matrices square matrices.\n",
        "        inputLayer - (n, n)\n",
        "        convFilter - (f, f)\n",
        "        \"\"\"\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Defining the shape of the output matrix\n",
        "        l = (n-f) + 1\n",
        "        output_matrix = np.zeros((l, l))\n",
        "        s = 1\n",
        "\n",
        "        # Convolving\n",
        "        for row in range(l):\n",
        "            for col in range(l):\n",
        "                output_matrix[row][col] = np.sum(np.multiply(inputLayer[row:row+f, col:col+f], convFilter))\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def fullConvolve(self, inputLayer, convFilter):\n",
        "        \"\"\"\n",
        "        Returns the full convoluted output of convFilter on inputLayer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dimensions of the input matrices\n",
        "        n = inputLayer.shape[0]\n",
        "        f = convFilter.shape[0]\n",
        "\n",
        "        # Creating padding for the inputLayer matrix\n",
        "        padding = f - 1\n",
        "        new_dim = n + 2*padding\n",
        "\n",
        "        padded_input = np.zeros([new_dim, new_dim])\n",
        "        padded_input[padding:new_dim - padding,padding:new_dim - padding] = inputLayer\n",
        "\n",
        "        # Now convolve padded_input with convFilter\n",
        "        output_matrix = self.convolve(padded_input, convFilter)\n",
        "\n",
        "        return output_matrix\n",
        "\n",
        "    def rotate180(self, matrix):\n",
        "        \"\"\"\n",
        "        Rotates matrix by 180 degrees in the plane.\n",
        "        Takes only two dimensional matrices.\n",
        "        \"\"\"\n",
        "        return np.rot90(matrix, 2)\n",
        "\n",
        "    def ConvGD(self, dLdoutput, index, t):\n",
        "        \"\"\"\n",
        "        Function that backpropagates through a convolutional layer.\n",
        "        index = index of the current layer\n",
        "        dLdoutput = Gradient of the loss function wrt the output of the current layer (channel, row, col)\n",
        "        Returns dLdinput.\n",
        "        t = iterative index\n",
        "        \"\"\"\n",
        "        X = self.getVolumeOutput(index-1)  # Input to the current layer (channel, row, col)\n",
        "        # Weights of the current layer (numFilter, channel, row, col)\n",
        "        W = self.weights[index - 1]\n",
        "        M = self.adam_m[index - 1]\n",
        "        V = self.adam_v[index - 1]\n",
        "\n",
        "        dLdX = np.empty(X.shape)\n",
        "        dLdW = np.empty(W.shape)\n",
        "\n",
        "        dLdout = np.copy(dLdoutput)\n",
        "        dLdout[dLdout < 0] = 0\n",
        "\n",
        "        # Loop over the filters\n",
        "        numFilters = W.shape[0]\n",
        "\n",
        "        for fil_ter in range(numFilters):\n",
        "            filter_output = dLdout[fil_ter]\n",
        "\n",
        "            # Loop over the channels\n",
        "            for channel in range(W.shape[1]):\n",
        "                filter_layer = W[fil_ter][channel]\n",
        "                dWLayer = self.convolve(X[channel], filter_output)\n",
        "                dXLayer = self.rotate180(self.fullConvolve(self.rotate180(filter_layer), filter_output))\n",
        "\n",
        "                # Combine these and return in arrays\n",
        "                dLdW[fil_ter][channel] = dWLayer\n",
        "                dLdX[channel] = dXLayer\n",
        "\n",
        "        # Adam optimization\n",
        "        M = self.beta1 * M + (1 - self.beta1) * dLdW\n",
        "        Mt = M / (1 - self.beta1**t)\n",
        "        V = self.beta2 * V + (1 - self.beta2) * np.square(dLdW)\n",
        "        Vt = V / (1 - self.beta2**t)\n",
        "\n",
        "        dLdW -= self.learning_rate * Mt / (np.sqrt(Vt) + self.eps)  # Note that the weights update is being multiplied with learning rate\n",
        "        #self.weights[index - 1] = W\n",
        "\n",
        "        self.adam_m[index - 1] = M\n",
        "        self.adam_v[index - 1] = V\n",
        "\n",
        "        return (dLdX,dLdW)\n",
        "\n",
        "    def backPropagation(self, input, trueResults, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Updates weights by carrying out backpropagation using mini-batches.\n",
        "        trueResults = the expected output from the neural network.\n",
        "        \"\"\"\n",
        "        size = int(len(input)/mini_batch_size)\n",
        "        for batch in range(size):\n",
        "            mini_inp = np.array(input[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            mini_res = np.array(trueResults[batch*mini_batch_size: (batch + 1)*mini_batch_size])\n",
        "            for i in range(len(mini_inp)):\n",
        "                self.inputImg = np.array(mini_inp[i])\n",
        "                if(len(self.inputImg.shape) < 3):\n",
        "                    a = self.inputImg\n",
        "                    self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                # Called once so that all weights are initialised, just in case if not done before\n",
        "                out = self.getVolumeOutput(len(self.weights))\n",
        "                # Index keeping track of the previous layer\n",
        "                nPrev = len(self.weights)\n",
        "                doutput = self.FCGD(nPrev, mini_res[i])\n",
        "                nPrev -= 1\n",
        "                \n",
        "                # Loop over the layers\n",
        "                while nPrev - 1 >= 0:\n",
        "                    if(self.track[nPrev - 1] == 'p'):\n",
        "                        dhidden = self.PoolGD(doutput, nPrev)\n",
        "                    else:\n",
        "                        (dhidden, dLdW) = self.ConvGD(doutput, nPrev, self.count)\n",
        "                        self.weights[nPrev - 1] -= dLdW / len(mini_inp)\n",
        "                    doutput = dhidden  # Move to the previous layer\n",
        "                    nPrev -= 1\n",
        "            print('count for adam: ',self.count)\n",
        "            self.count = self.count + 1\n",
        "\n",
        "\n",
        "    def train(self, input, Y, epochs, mini_batch_size):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "        Y = the expected results from the neural network.\n",
        "        epochs = the number of times the neural network should 'learn'.\n",
        "        mini-batch-size is the size of mini-batches.\n",
        "        \"\"\"\n",
        "        # Run backPropagation() 'epochs' number of times.\n",
        "        self.count = 1\n",
        "        for i in range(epochs):\n",
        "            self.backPropagation(input, Y, mini_batch_size)\n",
        "            print('Epoch Number: ', i + 1, ' done.')\n",
        "        print(\"Training Complete.\")\n",
        "\n",
        "    def accuracy(self, X, Y):\n",
        "        \"\"\"\n",
        "        Function that takes in test data and results and calculates the accuracy of the Network.\n",
        "        \"\"\"\n",
        "        y = []\n",
        "        cor = 0\n",
        "        correct = 0\n",
        "        for i in range(len(X)):\n",
        "            self.inputImg = np.array(X[i])\n",
        "            if(len(self.inputImg.shape) < 3):\n",
        "                a = self.inputImg\n",
        "                self.inputImg = a.reshape(1, a.shape[0], a.shape[1])\n",
        "                y.append(self.getVolumeOutput(len(self.weights)))\n",
        "        Y = np.array(Y)\n",
        "        y = np.array(y)\n",
        "        if (np.max(y) == 0):\n",
        "            y /= 1.0\n",
        "        else:\n",
        "            y /= np.max(y)\n",
        "        for i in range(len(Y)):\n",
        "            correct = np.argmax(Y[i])\n",
        "            if (np.argmax(y[i]) == correct):\n",
        "                cor += 1\n",
        "        cor /= len(Y)\n",
        "        print('Accuracy = ', cor*100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeNDunC2NJOV"
      },
      "source": [
        "# Code Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xdA8TUe7G6U",
        "outputId": "aae540c4-42ac-4930-eb6a-1dbce4706364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(x, y), (x2, y2) = datasets.mnist.load_data()\n",
        "# Normalise data\n",
        "x = ((x/255) - 0.5)\n",
        "x2 = ((x2/255) - 0.5)\n",
        "\n",
        "# Create instance of NeuralNetwork\n",
        "model = ConvNet()\n",
        "X = np.random.randn(1,28,28)\n",
        "model.addInput(X) # Input layer\n",
        "model.cvolume(1,3,10) # Add Convolutional volume (stride length, receptive field, filters)\n",
        "model.pmaxvolume(2) # Add Pooling layer (receptive field)\n",
        "model.FCLayer(10)  # Add FC Layer (number of classifiers)\n",
        "# Get final output layer. It is advised to run it once before training, so that all variables are initialised.\n",
        "print('Test Run Output: ',model.getVolumeOutput(3))\n",
        "\n",
        "#Since we test the CNN with MNIST data, we write the target output in the required format before sent to training/testing.\n",
        "results = np.zeros((len(y),10))  \n",
        "for i in range(len(y)):\n",
        "    results[i,y[i]] = 1\n",
        "\n",
        "model.train(x[0:2000], results[0:2000], 5, 100) # Train model with 2000 images, 5 epochs and 10 mini-batch size of 100\n",
        "model.accuracy(x[2000:4000], results[2000:4000])  # Predict accuracy using test data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Run Output:  [0.09973764 0.09803868 0.09997482 0.09920703 0.10080966 0.10060631\n",
            " 0.0996896  0.10163932 0.10028933 0.10000762]\n",
            "count for adam:  1\n",
            "count for adam:  2\n",
            "count for adam:  3\n",
            "count for adam:  4\n",
            "count for adam:  5\n",
            "count for adam:  6\n",
            "count for adam:  7\n",
            "count for adam:  8\n",
            "count for adam:  9\n",
            "count for adam:  10\n",
            "count for adam:  11\n",
            "count for adam:  12\n",
            "count for adam:  13\n",
            "count for adam:  14\n",
            "count for adam:  15\n",
            "count for adam:  16\n",
            "count for adam:  17\n",
            "count for adam:  18\n",
            "count for adam:  19\n",
            "count for adam:  20\n",
            "Epoch Number:  1  done.\n",
            "count for adam:  21\n",
            "count for adam:  22\n",
            "count for adam:  23\n",
            "count for adam:  24\n",
            "count for adam:  25\n",
            "count for adam:  26\n",
            "count for adam:  27\n",
            "count for adam:  28\n",
            "count for adam:  29\n",
            "count for adam:  30\n",
            "count for adam:  31\n",
            "count for adam:  32\n",
            "count for adam:  33\n",
            "count for adam:  34\n",
            "count for adam:  35\n",
            "count for adam:  36\n",
            "count for adam:  37\n",
            "count for adam:  38\n",
            "count for adam:  39\n",
            "count for adam:  40\n",
            "Epoch Number:  2  done.\n",
            "count for adam:  41\n",
            "count for adam:  42\n",
            "count for adam:  43\n",
            "count for adam:  44\n",
            "count for adam:  45\n",
            "count for adam:  46\n",
            "count for adam:  47\n",
            "count for adam:  48\n",
            "count for adam:  49\n",
            "count for adam:  50\n",
            "count for adam:  51\n",
            "count for adam:  52\n",
            "count for adam:  53\n",
            "count for adam:  54\n",
            "count for adam:  55\n",
            "count for adam:  56\n",
            "count for adam:  57\n",
            "count for adam:  58\n",
            "count for adam:  59\n",
            "count for adam:  60\n",
            "Epoch Number:  3  done.\n",
            "count for adam:  61\n",
            "count for adam:  62\n",
            "count for adam:  63\n",
            "count for adam:  64\n",
            "count for adam:  65\n",
            "count for adam:  66\n",
            "count for adam:  67\n",
            "count for adam:  68\n",
            "count for adam:  69\n",
            "count for adam:  70\n",
            "count for adam:  71\n",
            "count for adam:  72\n",
            "count for adam:  73\n",
            "count for adam:  74\n",
            "count for adam:  75\n",
            "count for adam:  76\n",
            "count for adam:  77\n",
            "count for adam:  78\n",
            "count for adam:  79\n",
            "count for adam:  80\n",
            "Epoch Number:  4  done.\n",
            "count for adam:  81\n",
            "count for adam:  82\n",
            "count for adam:  83\n",
            "count for adam:  84\n",
            "count for adam:  85\n",
            "count for adam:  86\n",
            "count for adam:  87\n",
            "count for adam:  88\n",
            "count for adam:  89\n",
            "count for adam:  90\n",
            "count for adam:  91\n",
            "count for adam:  92\n",
            "count for adam:  93\n",
            "count for adam:  94\n",
            "count for adam:  95\n",
            "count for adam:  96\n",
            "count for adam:  97\n",
            "count for adam:  98\n",
            "count for adam:  99\n",
            "count for adam:  100\n",
            "Epoch Number:  5  done.\n",
            "Training Complete.\n",
            "Accuracy =  80.65 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbeAg2WdN4NJ"
      },
      "source": [
        "Change accuracy function\n",
        "\n",
        "Check weights (plot them)\n",
        "\n",
        "Set control as Keras"
      ]
    }
  ]
}