<!DOCTYPE html>
<html lang="en">
  <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta name="description" content="">
      <meta name="author" content="Danush Shekar and Harisankar K R">
      <title>CS460 Project</title>
      <link rel = "icon" href =  "Images/heavy-ball" type = "image/x-icon"> 
      <link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap" rel="stylesheet">
      <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300&display=swap" rel="stylesheet">
      <script type="text/javascript" src="http://codecogs.unisa.ac.za/latexit.js"></script>
      <link rel="stylesheet" href="CSS/normalize.css">
      <link rel="stylesheet" href="CSS/style.css">
    </head>
	<body>
        <nav>
            <div class="topic">
                <h1>Introducing Higher Moment Terms in Heavy-Ball Optimisation</h1>
            </div>
        </nav>
        <div class="bg1">
            <div class="content-area-top">
                <div class="wrapper">
                    <h2>
                        Why do we need optimisation methods?
                    </h2>
                    <p>
                        Machine learning and Artificial Intelligence have become so popular that one can see its applications in almost every field one can think of. How to make these algorithms much more efficient and effective are crucial questions we would like to address. To give an idea of how complex even the simplest of problems can be, let us look at some numbers. The number of parameters in simple and good neural networks tackling the MNIST Database using a Convolutional Neural Network (CNN) are in millions. Even typical Neural Networks (NNs) tackling even simpler problems would have its number of parameters in orders of tens of thousands. The gradient descent explanations one usually encounters online come with a parabola or a contour representing the loss function with respect to 1 and 2 parameters respectively. We see that, as the complexity of the problem increases, the number of calculations required in terms of finding gradients at each point also increases. Instead of looking at how the curve is at each point blind, there are a variety of gradient descent and optimisation techniques we can use to tackle such problems. Keep in mind that when we use the term optimisation, we do not refer to optimisations like parameter initialisation (like weights in NNs and CNNs), but we refer to optimisation methods in gradient descent algorithms. Learning rate is an important hyper-parameter in every algorithm, setting it too low would mean it would take too long for the algorithm to converge to a solution, and setting it too high would result in fluctuations and diverging solutions. Optimisation methods are used to find a good solution, fast, along with a good probability to converge. Data is used more effectively and such algorithms save us a significant amount of time and computational resources. The animation shown below shows the difference between some popular optimisation methods and how fast (and accurately) they converge to the minima compared to the others.
                    </p>
                    <figure>
                        <img src="Images/comparision.gif" alt="Optimisation" style="width:500px"> 
                        <figcaption>
                            Fig.1 - SGD Optimisation in Loss Contour (Source: <a href="https://ruder.io/optimizing-gradient-descent/" target="blank">An overview of gradient descent optimization algorithms</a>)
                        </figcaption>
                    </figure>
                    <p>
                        As explained in CITE 2, besides the learning rate, how to avoid the objective function being trapped in infinite numbers of the local minimum is a common challenge. The slope of a saddle point is positive in one direction and negative in another direction, and gradient values in all directions are zero. It is an important problem to escape from these points and algorithms like Nesterov Accelerated Gradient Descent (NAG) come into picture. Although higher order methods are more suited, but will introduce them later on.
                    </p>
                    <p>
                        Now that we have looked at why studies in optimisation is important, let us get ourselves introduced to current optimisation methods before stating our problem.
                    </p>
                </div>
            </div>
        </div>
        <div class="bg2">
            <div class="content-area">
                <div class="wrapper">
                    <h2>
                        Current Optimisation Methods
                    </h2>
                    <p>
                        Popular optimization methods can be divided into three categories: first-order optimization methods (represented by the widely used stochastic gradient methods); high-order optimization methods; and heuristic derivative-free optimization methods.
                    </p>
                    <p>
                        Just like we discussed in class, there can be situations where we reach a saddle point in the loss function, there are high-order optimisation algorithms to overcome this problem which involves complex calculations (like inverse of large matrices). As explained in CITE 2, compared to first-order optimization methods, high-order methods converge at a faster speed in which the curvature information makes the search direction more effective. High-order optimizations attract widespread attention but face more challenges. The difficulty in high-order methods lies in the computational process which costly and thus rarely used for solving large machine learning problems. Although the convergence of the algorithm can be guaranteed, the computational process is costly and thus rarely used for solving large machine learning problems. For example, computing and storing the full Hessian matrix takes O(n^2) memory, which is infeasible for high-dimensional functions such as the loss functions of neural networks CITE 3. We will not get into high-order optimisation, but what we look to study is the effect of adding additional higher "moment" terms to the heavy-ball optimisation. We use "higher moment terms" to avoid confusion with the already-in-use "high-order methods". We will also be diverting from the main problem if one gets into derivative-free optimization methods also. So, let us look into first-order optimisation methods and where does our project come into picture among these methods.
                    </p>
                    <p>
                        First Order methods involve methods like Gradient Descent (Normal, stochastic, batch/mini-batch), Nesterov Accelerated Gradient Descent, AdaGrad, and so on. Gradient Descent is eplained in more detail in the Additional Resources section. More detailed explanations of most first (and high) order methods can be read at CITE 4 AND 5. In NAG, the update is given by:
                    </p>
                    <pre lang="latex">
                        x^{k+1}=x^{k} - a \nabla L\left(x^{k}+b\left(x^{k}-x^{k-1}\right)\right)+b\left(x^{k}-x^{k-1}\right)
                    </pre>
                    <p>
                        where x is the paramter, k is the iteration number, L is the loss function, a is the learning rate and b is another hyperparameter.
                    </p>
                    <p>
                        The method we look to focus on is the Polyak's Heavy-Ball Optimisation Method, whose parameter updates is given by:
                    </p>
                    <pre lang="latex">
                        x^{k+1}=x^{k}-a \nabla L\left(x^{k}\right)+b\left(x^{k}-x^{k-1}\right)
                    </pre>
                    <p>
                        If observed carefully one can see that NAG and HBO are not governed by the same equations, but yet there are references that call either of the algorithms as "Momentum" optimisation. To avoid such confusion, we will refer to them with their original names.
                    </p>
                 </div>
            </div>
        </div>
        <div class="bg1">
            <div class="content-area">
                <div class="wrapper">
                    <h2>
                        Problem Statement and Objecives
                    </h2>
                    <p>
                        We see that there are literature that delve into high-order methods which involve taking higher order derivatinves of the loss function to get more information of the curvature. But when one looks at the parameter-update equation of the HBO, one can realise that there could have been additional higher order terms of the parameter differences (momentum terms). For instance, we had <pre2 lang="latex">b\left(x^{k}-x^{k-1}\right)</pre2>, but why not <pre2 lang="latex">b\left((x^{k})^3-(x^{k-1})^3\right)</pre2>, and terms with 5th power, and so on (Why are we not considering even powers?). So this thought got us searching for the algorithms that does this, but according to CITE 2, up until 2019 there have been no literature on the same. So our project would inovolve looking into such higher moment terms and look at how they affect the training process in terms of rate of convergence, probability of convergence, etc.
                    </p>
                    <p>
                        We look forward to studying the effects of including a set of terms to the HBO:
                    </p>
                    <pre lang="latex">
                        b\left((x^{k})^n-(x^{k-1})^n\right)
                    </pre>
                    <p>
                        where n is an odd integer. If time permits, we would also want to look at addition of terms like:
                    </p>          
                    <pre lang="latex">
                        b\left(\left(x^{k}-x^{k-1}\right)\right)^n
                    </pre>
                    <p>
                        where n is an odd integer. One also has to keep in mind that studying the effect of addition of the above terms would involve deeper ablative tests to truly show that the positive/negative results obtained are due to addition of these terms only. For example, intuitively one can realise that learning rates that work for HBO might not necessarily work for addition of 3rd moment terms, because every iteration would go faster when going down the slope, and towards the minima can off-shoot and cause fluctuations.
                    </p>
                    <p>
                        We look forward to make this project an exhaustive study by focussing on one or more benchmark problems (like MNIST, to compare with existing optimisation methods), include ablative tests on hyper-parameters (like learning rate, a, b) and parameters, and thus compare how our proposed change fares with existing methods. At the end, we hope to get a GIF like Fig. 1 to provide for a more visual end result of our project.
                    </p>
                </div>
            </div>
        </div>
        <div class="bg2">
            <div class="content-area">
                <div class="wrapper">
                    <h2>
                        Scope, Feasibility and Work Plan
                    </h2>
                    <p>
                        We will take up the benchmark problems, people use to fare their optimisation methods against others, but keeping in mind the computational resources we have access to, we might have restrict our extent of testing to selected problems. Other than this, all tools needed to run the experiments are present and accessible (all that is needed are Python libraries and Google Colab access). As per the data
                    </p>
                 </div>
            </div>
        </div>
        <div class="bg1">
            <div class="content-area">
                <div class="wrapper">
                    <h2>
                        Is the problem worth looking at?
                    </h2>
                    <p>
                        To be really honest, we do not really know what the outcome of this project would be, but we are hopeful for some positive results, if not, then we would know one direction where we need not search. That said, we need to take a step back and ask ourselves why are we doing this? Why do we need to look for different methods? Yes, these methods would help make our programs faster, saving hours and even days of time and a lot more computational resources. But that truly is secondary, what we hope to gain from this project is to be able to answer deeper questions like how do these algorithms work? Why some work and some don't? We hope to learn and work with numerous algorithms, test them, play with them and thus at the end of it, also share our experience and knowledge we learnt with the class.
                    </p>
                </div>
            </div>
        </div>
        <div class="details">
            <div class="wrapper">
                <div class="heading">
                    <h2>
                        Guided by:
                    </h2>
                </div>  <!--.heading div-->
                <div class="photo">
                    <a href="https://www.niser.ac.in/users/smishra" target="blank"><img src="Images/mishra.jpg"></a>
                    <a href="https://www.niser.ac.in/users/smishra" target="blank"><p>Subhankar Mishra</p></a>
                </div><!--.photo div-->
            </div><!--.wrapper div-->
        </div><!--.details div-->

        <div class="bg4">
            <div class="details">
                <div class="wrapper">
                    <div class="heading">
                        <h2>
                            Contributors:
                        </h2>
                    </div>  
                    <div class="photo">
                        <img src="Images/Danush.jpeg">
                        <p>Danush Shekar</p>
                    </div>
                    <div class="photo">
                        <img src="Images/Hari.jpg">
                        <p>Harisankar K R</p>
                    </div><!--.widget div-->
                </div><!--.wrapper div-->
            </div><!--.details div-->
        </div>
    </body>
</html>
